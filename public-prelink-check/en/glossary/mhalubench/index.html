<!DOCTYPE html>
<html dir="ltr" lang="en">
<head>
<meta content="strict-origin-when-cross-origin" name="referrer"/>
<meta charset="utf-8"/>
<meta content="minimum-scale=1, width=device-width, initial-scale=1.0, shrink-to-fit=no" name="viewport"/>
<title>MHaluBench | SmartWeb</title>
<link href="https://main.d1jtfhinlastnr.amplifyapp.com/en/glossary/mhalubench/" rel="canonical"/>
<link href="/images/faivicon.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="/images/faivicon.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="/images/faivicon.png" rel="apple-touch-icon" sizes="180x180"/>
<link href="/images/faivicon.png" rel="shortcut icon"/>
<link href="https://main.d1jtfhinlastnr.amplifyapp.com/glossary/mhalubench/" hreflang="en" rel="alternate"/>
<link href="https://main.d1jtfhinlastnr.amplifyapp.com/ja/glossary/mhalubench/" hreflang="ja" rel="alternate"/>
<link href="https://main.d1jtfhinlastnr.amplifyapp.com/glossary/mhalubench/" hreflang="x-default" rel="alternate"/>
<meta content="A benchmark tool that detects false or made-up information in AI systems that work with both images and text, by breaking down responses into individual claims and checking each one for accuracy." name="description"/>
<meta content="MHaluBench, hallucination detection, multimodal LLMs, AI benchmark, I2T T2I" name="keywords"/>
<meta content="website" property="og:type"/>
<meta content="https://main.d1jtfhinlastnr.amplifyapp.com/en/glossary/mhalubench/" property="og:url"/>
<meta content="MHaluBench | SmartWeb" property="og:title"/>
<meta content="A benchmark tool that detects false or made-up information in AI systems that work with both images and text, by breaking down responses into individual claims and checking each one for accuracy." property="og:description"/>
<meta content="" property="og:image"/>
<meta content="1200" property="og:image:width"/>
<meta content="630" property="og:image:height"/>
<meta content="summary_large_image" name="twitter:card"/>
<meta content="https://main.d1jtfhinlastnr.amplifyapp.com/en/glossary/mhalubench/" name="twitter:url"/>
<meta content="MHaluBench | SmartWeb" name="twitter:title"/>
<meta content="A benchmark tool that detects false or made-up information in AI systems that work with both images and text, by breaking down responses into individual claims and checking each one for accuracy." name="twitter:description"/>
<meta content="" name="twitter:image"/>
<style>
  :root {
     
    --color-primary: #1a73e8;
    --color-primary-light: #4285f4;
    --color-primary-dark: #1557b0;

    --color-secondary: #34a853;
    --color-accent: #fbbc05;

    --color-text: #202124;
    --color-background: #ffffff;

     
    --gradient-primary: linear-gradient(to right, var(--color-primary), var(--color-primary-light));
  }

   
  .bg-gradient-primary {
    background-image: var(--gradient-primary);
  }

  .text-gradient,
  .text-gradient-primary {
    background-image: var(--gradient-primary);
    background-clip: text;
    -webkit-background-clip: text;
    color: transparent;
    -webkit-text-fill-color: transparent;
  }
</style>
<link as="font" crossorigin="anonymous" href="/fonts/inter/Inter-VariableFont_opsz,wght.woff2" rel="preload" type="font/woff2"/>
<link href="https://fonts.googleapis.com" rel="preconnect"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css2?family=Noto+Sans+JP:wght@400;500;700&amp;family=Noto+Serif+JP:wght@400;500;600;700&amp;display=swap" rel="stylesheet"/>
<link crossorigin="anonymous" href="/css/main.css?v=20260111190821" rel="stylesheet"/>
<link crossorigin="anonymous" href="/css/custom-code-blockquote.css?v=20260111190821" rel="stylesheet"/>
<script defer="" src="/js/main.js?v=20260111190821"></script>
</head>
<body class="antialiased bg-white">
<header class="bg-white">
<nav aria-label="Global" class="mx-auto flex max-w-7xl items-center justify-between gap-x-6 p-6 lg:px-8">
<div class="flex lg:flex-1">
<a class="-m-1.5 p-1.5" href="/en/">
<span class="sr-only">SmartWeb</span>
<picture class="lazy-picture" data-maxwidth="200">
<source data-original-src="/images/smartweb-logo.png" data-srcset="/images/smartweb-logo.png 466w" sizes="200px" type="image/png">
<img alt="SmartWeb Logo" class="lazy-image h-6 sm:h-10 md:h-12 w-auto" data-original-src="/images/smartweb-logo.png" data-src="/images/smartweb-logo.png" decoding="async" loading="lazy"/>
</source></picture>
</a>
</div>
<div class="hidden lg:flex lg:gap-x-12"><a class="text-sm/6 font-semibold text-gray-900" href="/en/">Home</a><a class="text-sm/6 font-semibold text-gray-900" href="/en/blog/">Blog</a><a class="text-sm/6 font-semibold text-gray-900" href="/en/glossary/">Glossary</a><a class="text-sm/6 font-semibold text-gray-900" href="https://www.intwk.co.jp/about/">Company</a></div>
<div class="flex flex-1 items-center justify-end gap-x-6">
<a class="hidden text-sm/6 font-semibold text-gray-900 lg:block" href="https://support.smartweb.jp/">Support</a>
<a class="rounded-md bg-indigo-600 px-3 py-2 text-sm font-semibold text-white shadow-xs hover:bg-indigo-500 focus-visible:outline-2 focus-visible:outline-offset-2 focus-visible:outline-indigo-600" href="/en/blog/">Get Started</a>
</div>
<div class="flex lg:hidden">
<button aria-controls="mobile-menu-1768126101320965000" aria-expanded="false" class="-m-2.5 inline-flex items-center justify-center rounded-xl p-2.5 text-gray-700" type="button">
<span class="sr-only">Open main menu</span>
<svg aria-hidden="true" class="size-6" data-slot="icon" fill="none" stroke="currentColor" stroke-width="1.5" viewbox="0 0 24 24">
<path d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5" stroke-linecap="round" stroke-linejoin="round"></path>
</svg>
</button>
</div>
</nav>
<div aria-modal="true" class="lg:hidden hidden relative z-50" id="mobile-menu-1768126101320965000" role="dialog">
<div class="fixed inset-0 z-50 bg-black bg-opacity-25"></div>
<div class="fixed inset-y-0 right-0 z-50 w-full overflow-y-auto bg-white px-6 py-6 sm:max-w-sm sm:ring-1 sm:ring-gray-900/10 transform transition-transform duration-300 ease-in-out translate-x-full">
<div class="flex items-center gap-x-6">
<a class="-m-1.5 p-1.5" href="/en/">
<span class="sr-only">SmartWeb</span>
<picture class="lazy-picture" data-maxwidth="3000">
<source data-original-src="/images/smartweb-logo.png" data-srcset="/images/smartweb-logo.png 466w" sizes="(max-width: 466px) 466px, 3000px" type="image/png">
<img alt="SmartWeb Logo" class="lazy-image h-6 sm:h-10 md:h-12 w-auto" data-original-src="/images/smartweb-logo.png" data-src="/images/smartweb-logo.png" decoding="async" loading="lazy"/>
</source></picture>
</a>
<a class="ml-auto rounded-md bg-indigo-600 px-3 py-2 text-sm font-semibold text-white shadow-xs hover:bg-indigo-500 focus-visible:outline-2 focus-visible:outline-offset-2 focus-visible:outline-indigo-600" href="/en/blog/">Get Started</a>
<button class="-m-2.5 rounded-xl p-2.5 text-gray-700 close-mobile-menu" type="button">
<span class="sr-only">Close menu</span>
<svg aria-hidden="true" class="size-6" data-slot="icon" fill="none" stroke="currentColor" stroke-width="1.5" viewbox="0 0 24 24">
<path d="M6 18 18 6M6 6l12 12" stroke-linecap="round" stroke-linejoin="round"></path>
</svg>
</button>
</div>
<div class="mt-6 flow-root">
<div class="-my-6 divide-y divide-gray-500/10">
<div class="space-y-2 py-6"><a class="-mx-3 block rounded-xl px-3 py-2 text-base/7 font-semibold text-gray-900 hover:bg-gray-50" href="/en/">Home</a><a class="-mx-3 block rounded-xl px-3 py-2 text-base/7 font-semibold text-gray-900 hover:bg-gray-50" href="/en/blog/">Blog</a><a class="-mx-3 block rounded-xl px-3 py-2 text-base/7 font-semibold text-gray-900 hover:bg-gray-50" href="/en/glossary/">Glossary</a><a class="-mx-3 block rounded-xl px-3 py-2 text-base/7 font-semibold text-gray-900 hover:bg-gray-50" href="https://www.intwk.co.jp/about/">Company</a></div>
<div class="py-6">
<a class="-mx-3 block rounded-lg px-3 py-2.5 text-base/7 font-semibold text-gray-900 hover:bg-gray-50" href="https://support.smartweb.jp/">Support</a>
</div>
</div>
</div>
</div>
</div>
</header>
<script>
  document.addEventListener('DOMContentLoaded', function() {
    const mobileMenuButton = document.querySelector('[aria-controls="mobile-menu-1768126101320965000"]');
    const mobileMenu = document.getElementById('mobile-menu-1768126101320965000');
    const closeButtons = document.querySelectorAll('.close-mobile-menu');
    const mobileMenuContent = mobileMenu.querySelector('.fixed.inset-y-0');
    
    if (mobileMenuButton && mobileMenu) {
      mobileMenuButton.addEventListener('click', function() {
        const expanded = mobileMenuButton.getAttribute('aria-expanded') === 'true';
        
        if (expanded) {
          closeMobileMenu();
        } else {
          openMobileMenu();
        }
      });
      
      
      closeButtons.forEach(button => {
        button.addEventListener('click', closeMobileMenu);
      });
      
      
      mobileMenu.addEventListener('click', function(event) {
        if (event.target === mobileMenu) {
          closeMobileMenu();
        }
      });
      
      
      mobileMenuContent.addEventListener('click', function(event) {
        event.stopPropagation();
      });
      
      function openMobileMenu() {
        mobileMenuButton.setAttribute('aria-expanded', 'true');
        mobileMenu.classList.remove('hidden');
        
        
        mobileMenuContent.offsetHeight;
        
        mobileMenuContent.classList.remove('translate-x-full');
        document.body.classList.add('overflow-hidden');
      }
      
      function closeMobileMenu() {
        mobileMenuButton.setAttribute('aria-expanded', 'false');
        mobileMenuContent.classList.add('translate-x-full');
        
        
        setTimeout(() => {
          mobileMenu.classList.add('hidden');
          document.body.classList.remove('overflow-hidden');
        }, 300);
      }
    }
  });
</script>
<main class="w-full">
<article class="mx-auto max-w-5xl px-4 sm:px-6 lg:px-8">
<header class="py-12 sm:py-16">
<div class="mx-auto max-w-4xl">
<div class="mb-8">
<nav class="text-sm hidden sm:block">
<ol class="flex items-center space-x-2 text-gray-400 dark:text-gray-500">
<li class="flex items-center">
<a class="hover:text-gray-900 dark:hover:text-gray-300 transition-colors flex items-center" href="/en/">
<img alt="Home" class="h-4 w-4 opacity-60" src="/images/home-icon.png"/>
</a>
</li>
<li><span class="mx-2 text-gray-300 dark:text-gray-600">/</span></li>
<li>
<a class="hover:text-gray-900 dark:hover:text-gray-300 transition-colors" href="/en/glossary/">
                Glossary
              </a>
</li>
<li><span class="mx-2 text-gray-300 dark:text-gray-600">/</span></li>
<li class="text-gray-600 dark:text-gray-400 truncate max-w-xs">MHaluBench</li>
</ol>
</nav>
</div>
<div class="mb-6">
<span class="inline-flex items-center text-xs font-medium tracking-wider uppercase text-gray-500 dark:text-gray-400 border-b border-gray-300 dark:border-gray-600 pb-1">
<svg class="mr-2 h-3.5 w-3.5" fill="none" stroke="currentColor" viewbox="0 0 24 24">
<path d="M7 7h.01M7 3h5c.512 0 1.024.195 1.414.586l7 7a2 2 0 010 2.828l-7 7a2 2 0 01-2.828 0l-7-7A1.994 1.994 0 013 12V7a4 4 0 014-4z" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"></path>
</svg>
            Multimodal AI
          </span>
</div>
<h1 class="text-2xl font-bold tracking-tight text-gray-900 dark:text-white sm:text-3xl md:text-4xl leading-tight mb-2">
        MHaluBench
      </h1>
<div class="mb-6"></div>
<p class="text-base sm:text-lg leading-relaxed text-gray-600 dark:text-gray-300 font-light max-w-3xl">
          A benchmark tool that detects false or made-up information in AI systems that work with both images and text, by breaking down responses into individual claims and checking each one for accuracy.
        </p>
<div class="mt-6 mb-4 border-t border-gray-100 dark:border-gray-800"></div>
<div class="flex flex-col sm:flex-row sm:items-center sm:justify-between gap-4">
<div class="flex flex-wrap gap-2">
<span class="inline-flex items-center px-2.5 py-1 text-xs font-medium text-gray-600 dark:text-gray-400 bg-transparent border border-gray-200 dark:border-gray-700 rounded hover:border-gray-400 dark:hover:border-gray-500 transition-colors">
                MHaluBench
              </span>
<span class="inline-flex items-center px-2.5 py-1 text-xs font-medium text-gray-600 dark:text-gray-400 bg-transparent border border-gray-200 dark:border-gray-700 rounded hover:border-gray-400 dark:hover:border-gray-500 transition-colors">
                hallucination detection
              </span>
<span class="inline-flex items-center px-2.5 py-1 text-xs font-medium text-gray-600 dark:text-gray-400 bg-transparent border border-gray-200 dark:border-gray-700 rounded hover:border-gray-400 dark:hover:border-gray-500 transition-colors">
                multimodal LLMs
              </span>
<span class="inline-flex items-center px-2.5 py-1 text-xs font-medium text-gray-600 dark:text-gray-400 bg-transparent border border-gray-200 dark:border-gray-700 rounded hover:border-gray-400 dark:hover:border-gray-500 transition-colors">
                AI benchmark
              </span>
<span class="inline-flex items-center px-2.5 py-1 text-xs font-medium text-gray-600 dark:text-gray-400 bg-transparent border border-gray-200 dark:border-gray-700 rounded hover:border-gray-400 dark:hover:border-gray-500 transition-colors">
                I2T T2I
              </span>
</div>
<div class="text-sm text-gray-500 dark:text-gray-400 flex flex-col items-end gap-y-1 text-right">
<span class="inline-flex items-center justify-end">
<svg class="mr-1.5 h-4 w-4" fill="none" stroke="currentColor" viewbox="0 0 24 24">
<path d="M8 7V5a3 3 0 013-3h2a3 3 0 013 3v2m4 0h-16a2 2 0 00-2 2v9a2 2 0 002 2h16a2 2 0 002-2V9a2 2 0 00-2-2z" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"></path>
</svg>
      
        Created: December 18, 2025
      
    </span>
</div>
</div>
</div>
</header>
<div class="prose prose-base sm:prose-lg dark:prose-invert mx-auto max-w-4xl py-6 sm:py-8">
<h2 id="what-is-mhalubench">What is MHaluBench?</h2>
<p>MHaluBench is a comprehensive benchmark for evaluating and detecting hallucinations in Multimodal <a data-lb="1" href="/blog/how-to-use-large-language-models-effectively/" title="Learn practical applications of large language models like ChatGPT, explore different LLM platforms, understand how these models work under the hood, and discover how to leverage them effectively in your daily work and life.">Large Language Models</a> (MLLMs) at fine-grained, claim-level granularity. It provides standardized evaluation across both Image-to-Text (I2T) and Text-to-Image (T2I) tasks, enabling precise assessment of model reliability and accuracy in multimodal contexts.</p>
<p>Unlike previous benchmarks that operate at response or sentence level, MHaluBench decomposes model outputs into atomic factual claims, annotates each claim for <a data-lb="1" href="/en/glossary/hallucination/" title="Hallucination glossary entry">hallucination</a> type and category, and provides ground truth for meta-evaluation of detection systems. This granular approach enables targeted diagnosis of specific failure modes in <a data-lb="1" href="/en/glossary/multimodal-ai/" title="Multimodal AI glossary entry">multimodal AI</a> systems.</p>
<p>The benchmark addresses critical challenges in deploying MLLMs for production use cases where accuracy and truthfulness are paramount—including medical imaging, autonomous systems, <a data-lb="1" href="/en/glossary/content-moderation/" title="Content Moderation glossary entry">content moderation</a>, and assistive technologies. By providing standardized evaluation methodology and high-quality annotations, MHaluBench accelerates research and development of more reliable multimodal <a data-lb="1" href="/en/glossary/artificial-intelligence/" title="Artificial Intelligence (AI) glossary entry">AI systems</a>.</p>
<h2 id="understanding-hallucinations-in-multimodal-ai">Understanding Hallucinations in Multimodal AI</h2>
<p>Hallucination in MLLMs occurs when generated output content is syntactically plausible but semantically unfaithful to provided inputs or contradicts established world knowledge. This phenomenon manifests across visual, textual, and cross-modal outputs, presenting unique challenges compared to text-only language models.</p>
<h3 id="hallucination-taxonomy">Hallucination Taxonomy</h3>
<p><strong>Faithfulness Hallucinations:</strong> Output contradicts direct input context. Example: Describing objects or attributes not present in input image, such as claiming “a dog is running” when only a cat appears in the image.</p>
<p><strong>Factuality Hallucinations:</strong> Output conflicts with established external knowledge despite plausible appearance. Example: Asserting “Eiffel Tower is located in London” when generating captions or answering visual questions.</p>
<p><strong>Modality-Conflicting Hallucinations:</strong> Direct contradictions between modalities in either input or output. Example: Text description states “red car” while associated image shows blue car, or T2I model generates image contradicting input text prompt.</p>
<p><strong>Fact-Conflicting Hallucinations:</strong> Output appears reasonable but violates world knowledge or common sense. Example: Claiming person in historical photo is using smartphone, or dating building construction to impossible time period.</p>
<h3 id="granularity-levels">Granularity Levels</h3>
<p><strong>Object-Level:</strong> Incorrect identification, omission, or fabrication of entities. Example: Detecting person where none exists, missing primary subject, or misidentifying object category.</p>
<p><strong>Attribute-Level:</strong> Incorrect properties assigned to correctly identified objects. Example: Wrong colors, incorrect sizes, misattributed materials, or inaccurate spatial relationships.</p>
<p><strong>Scene-Level:</strong> Misrepresentation of overall context, relationships, or events. Example: Describing indoor scene as outdoor, misidentifying activity or event, incorrect setting characterization.</p>
<p><strong>Scene-Text:</strong> Errors in recognizing or generating text within images. Example: Misreading signage, fabricating text content, or generating incorrect written language in T2I outputs.</p>
<h2 id="benchmark-structure">Benchmark Structure</h2>
<h3 id="dataset-composition">Dataset Composition</h3>
<p>MHaluBench comprises 620 carefully curated instances spanning three task categories, each annotated at both segment and claim levels for comprehensive evaluation.</p>
<p><strong>Image Captioning (IC) - 200 samples:</strong></p>
<ul>
<li>Source: MS-COCO 2014 Validation dataset</li>
<li>Outputs generated from: mPLUG, LLaVA, MiniGPT-4</li>
<li>Annotation focus: Faithfulness to visual content, object/attribute accuracy</li>
</ul>
<p><strong>Visual Question Answering (VQA) - 200 samples:</strong></p>
<ul>
<li>Source: TextVQA test set</li>
<li>Tasks: Scene text recognition, visual reasoning, attribute identification</li>
<li>Annotation focus: Answer accuracy, scene-text hallucinations, reasoning correctness</li>
</ul>
<p><strong>Text-to-Image Generation (T2I) - 220 samples:</strong></p>
<ul>
<li>Source <a data-lb="1" href="/en/glossary/prompts/" title="Prompts glossary entry">prompts</a>: DrawBench, T2I-CompBench</li>
<li>Models evaluated: DALL-E 2, DALL-E 3, Stable Diffusion variants</li>
<li>Annotation focus: Prompt adherence, attribute fidelity, composition accuracy</li>
</ul>
<p>Total coverage: 620 instances with dual-level annotation (segment and claim), providing 2,847 annotated claims across all categories.</p>
<h3 id="annotation-methodology">Annotation Methodology</h3>
<p><strong>Claim Extraction:</strong></p>
<ul>
<li>Automated extraction using GPT-4V and <a data-lb="1" href="/en/glossary/gemini/" title="Google's AI system that understands text, images, audio, and video together to answer questions and complete tasks.">Gemini</a></li>
<li>Manual verification and refinement by annotators</li>
<li>Atomic factual assertions isolated from complex sentences</li>
<li>Maintained semantic completeness while ensuring independence</li>
</ul>
<p><strong>Labeling Process:</strong></p>
<ul>
<li>Three graduate-level annotators independently label each claim</li>
<li>Binary classification: hallucinatory vs. non-hallucinatory</li>
<li>Category assignment: Object, Attribute, Scene-Text, Fact</li>
<li>Disagreements resolved by majority vote</li>
<li>Inter-annotator agreement: Fleiss’s Kappa κ = 0.822 (strong agreement)</li>
</ul>
<p><strong>Propagation Rules:</strong></p>
<ul>
<li>Segment marked hallucinatory if any contained claim is hallucinatory</li>
<li>Response marked hallucinatory if any segment is hallucinatory</li>
<li>Preserves fine-grained analysis while enabling coarse-grained evaluation</li>
</ul>
<h3 id="data-schema">Data Schema</h3>
<p>Each benchmark entry includes:</p>
<p><strong>Identifiers:</strong> Unique ID, task type designation, source dataset reference</p>
<p><strong>Inputs:</strong> Original image (I2T tasks) or text prompt (T2I tasks)</p>
<p><strong>Outputs:</strong> Generated text (I2T) or synthesized image (T2I)</p>
<p><strong>Segments:</strong> Logical text divisions (sentences or clauses) with hallucination labels</p>
<p><strong>Claims:</strong> Atomic factual assertions with detailed annotations including claim text, category classification, hallucination status, supporting rationale</p>
<p><strong>Metadata:</strong> Model information, generation parameters, annotation timestamps</p>
<h2 id="unihd-detection-framework">UNIHD Detection Framework</h2>
<p>Unified <a data-lb="1" href="/en/glossary/hallucination-detection/" title="Hallucination Detection glossary entry">Hallucination Detection</a> (UNIHD) represents state-of-the-art approach for automated hallucination detection, providing end-to-end pipeline from claim extraction to verification.</p>
<h3 id="four-stage-pipeline">Four-Stage Pipeline</h3>
<p><strong>Stage 1 - Essential Claim Extraction:</strong></p>
<ul>
<li>Decompose complex outputs into atomic factual claims</li>
<li>Ensure claims are independently verifiable</li>
<li>Maintain semantic relationships for context-dependent claims</li>
<li>Filter non-factual content (opinions, questions, imperatives)</li>
</ul>
<p><strong>Stage 2 - Autonomous Tool Selection:</strong></p>
<ul>
<li>Analyze each claim for verification requirements</li>
<li>Formulate targeted queries for validation</li>
<li>Select appropriate verification tools from toolkit:
<ul>
<li>Object detectors (Grounding DINO, YOLO)</li>
<li>Attribute classifiers (color, size, material)</li>
<li>OCR systems (scene text recognition)</li>
<li>Knowledge bases (factual verification)</li>
</ul>
</li>
</ul>
<p><strong>Stage 3 - Parallel Tool Execution:</strong></p>
<ul>
<li>Deploy selected tools concurrently for efficiency</li>
<li>Retrieve verification evidence from multiple sources</li>
<li>Handle tool failures and uncertain results gracefully</li>
<li>Aggregate results maintaining provenance</li>
</ul>
<p><strong>Stage 4 - Hallucination Verification:</strong></p>
<ul>
<li>Compare claims against verification evidence</li>
<li>Generate human-readable rationales for decisions</li>
<li>Assign confidence scores to detections</li>
<li>Produce final hallucinatory/non-hallucinatory labels</li>
</ul>
<h3 id="detection-approaches">Detection Approaches</h3>
<p><strong>Black-Box Methods:</strong> Evaluate models using only input-output pairs without internal access. Examples: FaithScore, GAVIE, HaELM. Advantages: Model-agnostic, deployable without architecture knowledge. Limitations: Cannot leverage internal states, limited explainability.</p>
<p><strong>White-Box Methods:</strong> Exploit model internals (attention weights, hidden states, token probabilities). Examples: DHCP, OPERA, ContextualLens. Advantages: Direct access to uncertainty signals, detailed interpretability. Limitations: Model-specific implementation, requires architectural knowledge.</p>
<p><strong>Tool-Augmented Methods:</strong> Leverage external verification tools for evidence-based detection. Examples: UNIHD, FactChecker, CutPaste &amp; Find. Advantages: Grounded in external evidence, extensible with new tools. Limitations: Dependent on tool accuracy, potential scalability challenges.</p>
<p><strong>Hybrid Approaches:</strong> Combine multiple detection paradigms for robust performance across scenarios and failure modes.</p>
<h2 id="benchmark-positioning">Benchmark Positioning</h2>
<h3 id="comparison-with-related-benchmarks">Comparison with Related Benchmarks</h3>
<table>
<thead>
<tr>
<th>Benchmark</th>
<th>Modalities</th>
<th>Tasks</th>
<th>Granularity</th>
<th>Categories</th>
<th>Annotation</th>
<th>Unique Features</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>HaluEval</strong></td>
<td>Text</td>
<td>QA, Summarization</td>
<td>Response</td>
<td>Factuality</td>
<td>Response-level</td>
<td>Large-scale text focus</td>
</tr>
<tr>
<td><strong>POPE</strong></td>
<td>Image+Text</td>
<td>Captioning</td>
<td>Response</td>
<td>Faithfulness</td>
<td>Response-level</td>
<td>Visual object presence</td>
</tr>
<tr>
<td><strong>HalluCode</strong></td>
<td>Code</td>
<td>Code generation</td>
<td>Token</td>
<td>Mapping, Naming, Logic</td>
<td>Token-level</td>
<td>Execution-based validation</td>
</tr>
<tr>
<td><strong>CodeHalu</strong></td>
<td>Code</td>
<td>Code generation</td>
<td>Span</td>
<td>Resource, Logic</td>
<td>Span-level</td>
<td>Comprehensive code analysis</td>
</tr>
<tr>
<td><strong>Collu-Bench</strong></td>
<td>Code</td>
<td>Gen &amp; Repair</td>
<td>Token</td>
<td>Multiple</td>
<td>Token-level</td>
<td>Multi-LLM comparison</td>
</tr>
<tr>
<td><strong>MHaluBench</strong></td>
<td>Image+Text</td>
<td>I2T, T2I</td>
<td>Claim</td>
<td>Object, Attribute, Scene, Fact</td>
<td>Claim+Segment</td>
<td>Unified multimodal coverage</td>
</tr>
</tbody>
</table>
<p><strong>Distinctive Advantages:</strong></p>
<ul>
<li>Only benchmark spanning both I2T and T2I tasks comprehensively</li>
<li>Finest granularity with claim-level annotation enabling targeted analysis</li>
<li>Explicit taxonomy distinguishing modality vs. fact conflicts</li>
<li>Designed specifically for meta-evaluation of detection systems</li>
<li>Balanced coverage across hallucination categories and task types</li>
</ul>
<h2 id="practical-applications">Practical Applications</h2>
<h3 id="model-development">Model Development</h3>
<p><strong>Targeted Improvement:</strong> Identify specific failure modes (e.g., attribute hallucinations in medical imaging) enabling focused model refinement.</p>
<p><strong>Ablation Studies:</strong> Evaluate impact of architectural changes or training procedures on specific hallucination categories.</p>
<p><strong>Comparison Analysis:</strong> Benchmark multiple model variants or architectures on standardized hallucination metrics.</p>
<h3 id="detection-system-evaluation">Detection System Evaluation</h3>
<p><strong>Meta-Evaluation:</strong> Assess detection systems against ground truth, measuring <a data-lb="1" href="/en/glossary/precision/" title="Precision glossary entry">precision</a>, recall, F1 across categories.</p>
<p><strong>Robustness Testing:</strong> Evaluate detector performance across diverse scenarios, domains, and hallucination types.</p>
<p><strong>Tool Development:</strong> Guide development of specialized detection tools for specific hallucination categories.</p>
<h3 id="production-deployment">Production Deployment</h3>
<p><strong>Quality Assurance:</strong> Establish thresholds and <a data-lb="1" href="/en/glossary/monitoring/" title="Monitoring glossary entry">monitoring</a> for acceptable hallucination rates in production systems.</p>
<p><strong>User Trust:</strong> Provide evidence-based reliability metrics informing deployment decisions and user expectations.</p>
<p><strong>Risk Mitigation:</strong> Identify high-risk scenarios requiring human oversight or additional verification.</p>
<h2 id="illustrative-examples">Illustrative Examples</h2>
<h3 id="image-to-text-hallucination">Image-to-Text Hallucination</h3>
<p><strong>Input:</strong> Soccer match photograph showing athlete in blue uniform on right side</p>
<p><strong>Model Output:</strong> “The athlete on the right side, wearing the red uniform, belongs to Club América.”</p>
<p><strong>Extracted Claims:</strong></p>
<ol>
<li>“Athlete on right wears red uniform” - HALLUCINATORY (Attribute-level, Modality-conflicting: Image shows blue uniform)</li>
<li>“Athlete belongs to Club América” - REQUIRES FACT-CHECK (Fact-level: Needs external verification of team membership)</li>
</ol>
<p><strong>Detection Process:</strong> Object detector confirms athlete presence and location, attribute classifier identifies blue uniform contradicting claim 1, <a data-lb="1" href="/en/glossary/knowledge-base/" title="Knowledge Base glossary entry">knowledge base</a> query verifies team information for claim 2.</p>
<h3 id="text-to-image-hallucination">Text-to-Image Hallucination</h3>
<p><strong>Input Prompt:</strong> “A yellow school bus parked in front of the Eiffel Tower in Paris”</p>
<p><strong>Generated Image:</strong> Red bus in front of unidentified landmark</p>
<p><strong>Extracted Claims:</strong></p>
<ol>
<li>“Image contains yellow school bus” - HALLUCINATORY (Object/Attribute-level, Modality-conflicting: Image shows red bus)</li>
<li>“Bus positioned in front of Eiffel Tower” - HALLUCINATORY (Fact-level, Modality-conflicting: Landmark not identifiable as Eiffel Tower)</li>
</ol>
<p><strong>Detection Process:</strong> Object detection identifies bus but wrong color, landmark recognition fails to confirm Eiffel Tower, attribute verification contradicts yellow color claim.</p>
<h2 id="limitations-and-future-directions">Limitations and Future Directions</h2>
<p><strong>Scale Constraints:</strong> Current 620 instances smaller than large-scale text benchmarks. Expansion planned to thousands of instances across more domains and modalities.</p>
<p><strong>Annotation Costs:</strong> Human annotation resource-intensive limiting rapid scaling. Future work explores semi-automatic annotation with human verification.</p>
<p><strong>Modality Coverage:</strong> Currently limited to image-text pairs. Extension to video, audio, 3D, and sensor data modalities under consideration.</p>
<p><strong>Tool Dependency:</strong> Detection performance bounded by external tool accuracy. Improving tool reliability and developing specialized verification systems ongoing.</p>
<p><strong>Dynamic Evaluation:</strong> Static benchmark may not reflect real-world deployment challenges. Development of dynamic evaluation protocols for live systems needed.</p>
<p><strong>Cultural and Linguistic Diversity:</strong> Current focus on English and common image domains. Expansion to multilingual settings and diverse cultural contexts planned.</p>
<p><strong>Mitigation Integration:</strong> Benchmark focuses on detection; integration with correction and mitigation systems represents future direction.</p>
<h2 id="implementation-resources">Implementation Resources</h2>
<p><strong>Dataset Access:</strong> Available on HuggingFace Datasets platform with comprehensive documentation, evaluation scripts, and baseline results.</p>
<p><strong>Evaluation Tools:</strong> Python toolkit for computing detection metrics, analyzing error patterns, and generating detailed reports.</p>
<p><strong>Baseline Implementations:</strong> Reference implementations of UNIHD and other detection approaches for reproduction and extension.</p>
<p><strong>Community Contributions:</strong> Open invitation for submission of detection systems to public leaderboard, expanding benchmark coverage.</p>
<h2 id="references">References</h2>
<ul>
<li><a href="https://aclanthology.org/2024.acl-long.178/" rel="nofollow noopener noreferrer" target="_blank">Unified Hallucination Detection (ACL 2024)</a></li>
<li><a href="https://arxiv.org/abs/2507.19024" rel="nofollow noopener noreferrer" target="_blank">Multimodal Hallucination Survey (arXiv 2024)</a></li>
<li><a href="https://huggingface.co/datasets/OpenKG/MHaluBench" rel="nofollow noopener noreferrer" target="_blank">MHaluBench Dataset (HuggingFace)</a></li>
<li><a href="https://cocodataset.org/#home" rel="nofollow noopener noreferrer" target="_blank">MS-COCO Dataset</a></li>
<li><a href="https://textvqa.org/" rel="nofollow noopener noreferrer" target="_blank">TextVQA Dataset</a></li>
<li><a href="https://arxiv.org/abs/2206.01714" rel="nofollow noopener noreferrer" target="_blank">DrawBench Benchmark</a></li>
<li><a href="https://arxiv.org/abs/2307.10021" rel="nofollow noopener noreferrer" target="_blank">T2I-CompBench</a></li>
<li><a href="https://arxiv.org/abs/2306.05645" rel="nofollow noopener noreferrer" target="_blank">HaluEval Paper</a></li>
<li><a href="https://arxiv.org/abs/2402.09901" rel="nofollow noopener noreferrer" target="_blank">HalluCode Benchmark</a></li>
<li><a href="https://arxiv.org/abs/2402.09901" rel="nofollow noopener noreferrer" target="_blank">CodeHalu Benchmark</a></li>
<li><a href="https://arxiv.org/abs/2405.14451" rel="nofollow noopener noreferrer" target="_blank">Collu-Bench</a></li>
<li><a href="https://github.com/X-PLUG/mPLUG" rel="nofollow noopener noreferrer" target="_blank">mPLUG Framework</a></li>
<li><a href="https://llava-vl.github.io/" rel="nofollow noopener noreferrer" target="_blank">LLaVA Model</a></li>
<li><a href="https://minigpt-4.github.io/" rel="nofollow noopener noreferrer" target="_blank">MiniGPT-4</a></li>
<li><a href="https://openai.com/research/dall-e" rel="nofollow noopener noreferrer" target="_blank">DALL-E 2 (OpenAI)</a></li>
<li><a href="https://openai.com/blog/dall-e-3" rel="nofollow noopener noreferrer" target="_blank">DALL-E 3 (OpenAI)</a></li>
<li><a href="https://github.com/IDEA-Research/GroundingDINO" rel="nofollow noopener noreferrer" target="_blank">Grounding DINO</a></li>
<li><a href="https://platform.openai.com/docs/guides/vision" rel="nofollow noopener noreferrer" target="_blank">GPT-4V Vision</a></li>
<li><a href="https://deepmind.google/technologies/gemini/" rel="nofollow noopener noreferrer" target="_blank">Gemini Multimodal</a></li>
<li><a href="https://www.emergentmind.com/topics/mmhal-bench" rel="nofollow noopener noreferrer" target="_blank">MMHal-Bench</a></li>
</ul>
</div>
<div class="mt-16 sm:mt-20 border-t border-gray-200 dark:border-gray-800 pt-12 sm:pt-16">
<h2 class="text-2xl sm:text-3xl font-bold tracking-tight text-gray-900 dark:text-white mb-8 sm:mb-10">
        
          Related Terms
        
      </h2>
<div class="grid gap-6 sm:gap-8 grid-cols-1 sm:grid-cols-2 lg:grid-cols-3">
<article class="group relative flex flex-col overflow-hidden border border-gray-200 dark:border-gray-700 bg-white dark:bg-gray-900 hover:border-gray-400 dark:hover:border-gray-500 transition-all duration-300">
<div class="flex flex-1 flex-col p-5 sm:p-6">
<h3 class="text-base sm:text-lg font-semibold text-gray-900 dark:text-white line-clamp-2 mb-3">
<a class="hover:text-gray-600 dark:hover:text-gray-300 transition-colors" href="https://main.d1jtfhinlastnr.amplifyapp.com/en/glossary/hallucination-detection/">
                    Hallucination Detection
                  </a>
</h3>
<p class="text-sm text-gray-600 dark:text-gray-400 flex-1 line-clamp-3">
                    Technology that automatically identifies when AI systems generate false or made-up information, help...
                  </p>
<div class="mt-4 pt-4 border-t border-gray-100 dark:border-gray-800">
<a class="inline-flex items-center text-sm font-medium text-gray-900 dark:text-gray-100 group-hover:translate-x-1 transition-transform" href="https://main.d1jtfhinlastnr.amplifyapp.com/en/glossary/hallucination-detection/">
                    View details
                    <svg class="ml-1.5 h-4 w-4" fill="none" stroke="currentColor" viewbox="0 0 24 24">
<path d="M9 5l7 7-7 7" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"></path>
</svg>
</a>
</div>
</div>
</article>
</div>
</div>
<div class="mt-12 sm:mt-16 py-8 border-t border-gray-200 dark:border-gray-800">
<a class="inline-flex items-center text-sm font-medium text-gray-600 dark:text-gray-400 hover:text-gray-900 dark:hover:text-gray-100 transition-colors" href="/en/glossary/">
<svg class="mr-2 h-5 w-5" fill="none" stroke="currentColor" viewbox="0 0 24 24">
<path d="M10 19l-7-7m0 0l7-7m-7 7h18" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"></path>
</svg>
      
        Back to Glossary
      
    </a>
</div>
</article>
</main>
<footer style="background-color: #000000;">
<div id="cta-curves-container" style="
    position: relative;
    background: #000000;
    padding: 4rem 2rem;
    overflow: hidden;
  ">
<svg id="cta-curves-svg" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; z-index: 1; opacity: 0.7;" xmlns="http://www.w3.org/2000/svg">
<defs>
<lineargradient id="curveGrad1" x1="0%" x2="100%" y1="0%" y2="0%">
<stop offset="0%" style="stop-color:rgba(99, 102, 241, 0); stop-opacity:0"></stop>
<stop offset="50%" style="stop-color:rgba(99, 102, 241, 0.9); stop-opacity:1"></stop>
<stop offset="100%" style="stop-color:rgba(99, 102, 241, 0); stop-opacity:0"></stop>
</lineargradient>
<lineargradient id="curveGrad2" x1="0%" x2="100%" y1="0%" y2="0%">
<stop offset="0%" style="stop-color:rgba(139, 92, 246, 0); stop-opacity:0"></stop>
<stop offset="50%" style="stop-color:rgba(139, 92, 246, 0.8); stop-opacity:1"></stop>
<stop offset="100%" style="stop-color:rgba(139, 92, 246, 0); stop-opacity:0"></stop>
</lineargradient>
<lineargradient id="curveGrad3" x1="0%" x2="100%" y1="0%" y2="0%">
<stop offset="0%" style="stop-color:rgba(59, 130, 246, 0); stop-opacity:0"></stop>
<stop offset="50%" style="stop-color:rgba(59, 130, 246, 0.7); stop-opacity:1"></stop>
<stop offset="100%" style="stop-color:rgba(59, 130, 246, 0); stop-opacity:0"></stop>
</lineargradient>
</defs>
<path class="curve" data-speed="0.8" fill="none" stroke="url(#curveGrad1)" stroke-width="2"></path>
<path class="curve" data-speed="1.2" fill="none" opacity="0.8" stroke="url(#curveGrad2)" stroke-width="2.5"></path>
<path class="curve" data-speed="0.6" fill="none" opacity="0.6" stroke="url(#curveGrad3)" stroke-width="1.5"></path>
<path class="curve" data-speed="1.5" fill="none" opacity="0.7" stroke="url(#curveGrad1)" stroke-width="2"></path>
<path class="curve" data-speed="0.9" fill="none" opacity="0.5" stroke="url(#curveGrad2)" stroke-width="3"></path>
<path class="curve" data-speed="1.3" fill="none" opacity="0.9" stroke="url(#curveGrad3)" stroke-width="1.8"></path>
<path class="curve" data-speed="0.7" fill="none" opacity="0.6" stroke="url(#curveGrad1)" stroke-width="2.2"></path>
<path class="curve" data-speed="1.1" fill="none" opacity="0.8" stroke="url(#curveGrad2)" stroke-width="2"></path>
<path class="curve" data-speed="1.4" fill="none" opacity="0.5" stroke="url(#curveGrad3)" stroke-width="2.5"></path>
<path class="curve" data-speed="0.85" fill="none" opacity="0.7" stroke="url(#curveGrad1)" stroke-width="1.5"></path>
<path class="curve" data-speed="1.0" fill="none" opacity="0.6" stroke="url(#curveGrad2)" stroke-width="2.8"></path>
<path class="curve" data-speed="1.25" fill="none" opacity="0.8" stroke="url(#curveGrad3)" stroke-width="2"></path>
<path class="curve" data-speed="0.95" fill="none" opacity="0.5" stroke="url(#curveGrad1)" stroke-width="2.3"></path>
<path class="curve" data-speed="1.35" fill="none" opacity="0.9" stroke="url(#curveGrad2)" stroke-width="1.7"></path>
<path class="curve" data-speed="0.75" fill="none" opacity="0.7" stroke="url(#curveGrad3)" stroke-width="2.5"></path>
</svg>
<div style="position: absolute; top: 0; left: 0; width: 50px; height: 50px; border-left: 2px solid rgba(99, 102, 241, 0.3); border-top: 2px solid rgba(99, 102, 241, 0.3);"></div>
<div style="position: absolute; top: 0; right: 0; width: 50px; height: 50px; border-right: 2px solid rgba(99, 102, 241, 0.3); border-top: 2px solid rgba(99, 102, 241, 0.3);"></div>
<div style="position: absolute; bottom: 0; left: 0; width: 50px; height: 50px; border-left: 2px solid rgba(99, 102, 241, 0.3); border-bottom: 2px solid rgba(99, 102, 241, 0.3);"></div>
<div style="position: absolute; bottom: 0; right: 0; width: 50px; height: 50px; border-right: 2px solid rgba(99, 102, 241, 0.3); border-bottom: 2px solid rgba(99, 102, 241, 0.3);"></div>
<div class="mx-auto max-w-2xl text-center" style="position: relative; z-index: 10;">
<h2 class="text-base/7 font-semibold text-indigo-400">Ready to get started?</h2>
<p class="mt-2 text-4xl font-semibold tracking-tight text-balance text-white sm:text-5xl">Start using our services today</p>
<p class="mx-auto mt-6 max-w-xl text-lg/8 text-pretty text-gray-400">Join thousands of satisfied customers who have transformed their business with our solutions.</p>
<div class="mt-8 flex justify-center">
<a class="rounded-md bg-indigo-500 px-3.5 py-2.5 text-sm font-semibold text-white shadow-xs hover:bg-indigo-400 focus-visible:outline-2 focus-visible:outline-offset-2 focus-visible:outline-indigo-400" href="/en/blog/">Get started</a>
</div>
</div>
</div>
<script>
  (function() {
    'use strict';
    
    const container = document.getElementById('cta-curves-container');
    const svg = document.getElementById('cta-curves-svg');
    if (!container || !svg) return;
    
    const curves = svg.querySelectorAll('.curve');
    let mouseX = 0;
    let mouseY = 0;
    let targetMouseX = 0;
    let targetMouseY = 0;
    const mouseInfluence = 150;
    const mouseStrength = 80;
    
    const curveData = Array.from(curves).map((curve, i) => {
      const speed = parseFloat(curve.getAttribute('data-speed')) || 1;
      return {
        element: curve,
        baseY: 50 + (i * 25),
        offset: Math.random() * Math.PI * 2,
        speed: speed,
        amplitude: 30 + Math.random() * 40
      };
    });
    
    container.addEventListener('mousemove', (e) => {
      const rect = container.getBoundingClientRect();
      targetMouseX = e.clientX - rect.left;
      targetMouseY = e.clientY - rect.top;
    });
    
    container.addEventListener('mouseleave', () => {
      targetMouseX = -1000;
      targetMouseY = -1000;
    });
    
    let time = 0;
    
    function animate() {
      time += 0.01;
      
      mouseX += (targetMouseX - mouseX) * 0.15;
      mouseY += (targetMouseY - mouseY) * 0.15;
      
      const width = svg.clientWidth;
      const height = svg.clientHeight;
      
      curveData.forEach((data, index) => {
        const { baseY, offset, speed, amplitude } = data;
        const phase = time * speed + offset;
        
        let path = `M -200,${baseY}`;
        
        for (let x = -200; x <= width + 200; x += 50) {
          const normalY = baseY + Math.sin((x * 0.005) + phase) * amplitude;
          
          const dx = x - mouseX;
          const dy = normalY - mouseY;
          const distance = Math.sqrt(dx * dx + dy * dy);
          
          let y = normalY;
          if (distance < mouseInfluence) {
            const influence = (1 - distance / mouseInfluence);
            const pushY = (normalY - mouseY) * influence * mouseStrength * 0.01;
            y = normalY + pushY;
          }
          
          const nextX = x + 50;
          const controlX = x + 25;
          const controlY = y;
          path += ` Q ${controlX},${controlY} ${nextX},${y}`;
        }
        
        data.element.setAttribute('d', path);
      });
      
      requestAnimationFrame(animate);
    }
    
    animate();
  })();
  </script>
<div class="mx-auto max-w-7xl px-6 py-16 sm:py-24 lg:px-8 lg:py-32">
<div class="mt-24 border-t border-white/10 pt-12 xl:grid xl:grid-cols-3 xl:gap-8">
<picture class="lazy-picture" data-maxwidth="200">
<source data-original-src="/images/interwork-logo-white-1.webp" data-srcset="/images/interwork-logo-white-1.webp 568w" sizes="200px" type="image/webp">
<img alt="Interwork" class="lazy-image h-9" data-original-src="/images/interwork-logo-white-1.webp" data-src="/images/interwork-logo-white-1.webp" decoding="async" loading="lazy"/>
</source></picture>
<div class="mt-16 grid grid-cols-2 gap-8 xl:col-span-2 xl:mt-0">
<div class="md:grid md:grid-cols-2 md:gap-8">
<div>
<h3 class="text-sm/6 font-semibold text-white">Services</h3>
<ul class="mt-6 space-y-4" role="list">
<li>
<a class="text-sm/6 text-gray-400 hover:text-white" href="#">AI Solutions</a>
</li>
<li>
<a class="text-sm/6 text-gray-400 hover:text-white" href="#">Web Development</a>
</li>
<li>
<a class="text-sm/6 text-gray-400 hover:text-white" href="#">System Development</a>
</li>
<li>
<a class="text-sm/6 text-gray-400 hover:text-white" href="#">Consulting</a>
</li>
</ul>
</div>
<div class="mt-10 md:mt-0">
<h3 class="text-sm/6 font-semibold text-white">Support</h3>
<ul class="mt-6 space-y-4" role="list">
<li>
<a class="text-sm/6 text-gray-400 hover:text-white" href="https://support.smartweb.jp/">Support Portal</a>
</li>
<li>
<a class="text-sm/6 text-gray-400 hover:text-white" href="#">Documentation</a>
</li>
</ul>
</div>
</div>
<div class="md:grid md:grid-cols-2 md:gap-8">
<div>
<h3 class="text-sm/6 font-semibold text-white">Company</h3>
<ul class="mt-6 space-y-4" role="list">
<li>
<a class="text-sm/6 text-gray-400 hover:text-white" href="https://www.intwk.co.jp/about/">About</a>
</li>
<li>
<a class="text-sm/6 text-gray-400 hover:text-white" href="/en/blog/">Blog</a>
</li>
<li>
<a class="text-sm/6 text-gray-400 hover:text-white" href="#">Careers</a>
</li>
<li>
<a class="text-sm/6 text-gray-400 hover:text-white" href="#">News</a>
</li>
</ul>
</div>
<div class="mt-10 md:mt-0">
<h3 class="text-sm/6 font-semibold text-white">Legal</h3>
<ul class="mt-6 space-y-4" role="list">
<li>
<a class="text-sm/6 text-gray-400 hover:text-white" href="/en/privacy-policy/">Privacy Policy</a>
</li>
<li>
<a class="text-sm/6 text-gray-400 hover:text-white" href="/en/ai-chatbot-terms-of-use/">AI Chatbot Terms of Use</a>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="language-selector mt-12 border-t border-white/10 pt-8 md:flex md:items-center md:justify-between">
<div>
<p class="text-sm/6 font-semibold text-white mb-4">Available Languages</p>
<div class="language-selector">
<div class="flex flex-wrap items-center justify-center gap-3">
<a aria-label="日本語" class="inline-flex items-center text-sm hover:opacity-75 transition-opacity" href="/ja/glossary/mhalubench/" hreflang="ja" title="日本語">
<img alt="日本語" class="rounded" height="18" src="/flags/jp.png" width="24"/>
</a>
<span class="inline-flex items-center text-sm opacity-50" title="English">
<img alt="English" class="rounded" height="18" src="/flags/gb.png" width="24"/>
</span>
</div>
</div>
</div>
</div>
<div class="mt-12 border-t border-white/10 pt-8 md:flex md:items-center md:justify-between">
<div class="flex gap-x-6 md:order-2">
<a class="text-gray-400 hover:text-gray-300" href="https://github.com">
<span class="sr-only">GitHub</span>
</a>
<a class="text-gray-400 hover:text-gray-300" href="https://x.com">
<span class="sr-only">X</span>
</a>
<a class="text-gray-400 hover:text-gray-300" href="https://youtube.com">
<span class="sr-only">YouTube</span>
</a>
</div>
<p class="mt-8 text-sm/6 text-gray-400 md:mt-0" style="text-align: center; animation: copyrightGlow 3s ease-in-out infinite;">© 2026 Interwork Corporation All rights reserved.</p>
</div>
</div>
<style>
    @keyframes copyrightGlow {
      0%, 100% {
        opacity: 0.6;
        text-shadow: 0 0 10px rgba(99, 102, 241, 0);
      }
      50% {
        opacity: 1;
        text-shadow: 0 0 20px rgba(99, 102, 241, 0.5), 0 0 30px rgba(99, 102, 241, 0.3);
      }
    }
  </style>
</footer>
<div class="pointer-events-none fixed inset-x-0 bottom-0 px-6 pb-6 z-50 dark" data-cookie-consent-banner="" id="cookie-consent-banner">
<div class="pointer-events-auto max-w-xl rounded-xl section-bg-light dark:section-bg-dark p-6 ring-1 shadow-lg ring-gray-900/10">
<p class="text-secondary text-sm/6"><strong class="text-heading text-md mb-4 font-semibold">Cookie Consent</strong><br/> We use cookies to enhance your browsing experience and analyze our traffic. See our <a class="font-semibold text-primary hover:text-primary-500" href="/en/privacy-policy/">privacy policy</a>.</p>
<div class="mt-4 flex items-center gap-x-3 flex-wrap">
<a aria-label="Accept All" class="btn-primary dark:btn-primary-dark px-3 py-2 text-sm not-prose group" data-cookie-consent="accept-all" href="#" target="_self">
      Accept All
      
      
    </a>
<a aria-label="Reject All" class="btn-secondary dark:btn-secondary-dark px-3 py-2 text-sm not-prose group" data-cookie-consent="accept-necessary" href="#" target="_self">
      Reject All
      
      
    </a>
<a aria-label="Cookie Settings" class="btn-text dark:btn-text-dark px-3 py-2 text-sm not-prose group" data-cookie-consent="settings" href="#" target="_self">
      Cookie Settings
      
      
    </a>
</div>
</div>
</div>
<div class="fixed inset-0 z-50 hidden dark" id="cookie-settings-modal">
<div class="absolute inset-0 bg-black bg-opacity-50" data-cookie-settings-close=""></div>
<div class="relative mx-auto max-w-xl p-4 sm:p-6 section-bg-light dark:section-bg-dark rounded-xl shadow-xl mt-20">
<div class="flex justify-between items-center mb-4">
<h2 class="text-heading text-xl font-bold">Cookie Settings</h2>
<button class="text-gray-400 hover:text-gray-500 dark:text-gray-300 dark:hover:text-white" data-cookie-settings-close="" type="button">
<span class="sr-only">Close</span>
<svg class="h-6 w-6" fill="none" stroke="currentColor" viewbox="0 0 24 24">
<path d="M6 18L18 6M6 6l12 12" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"></path>
</svg>
</button>
</div>
<div class="space-y-4">
<div class="border-gray-200 dark:border-gray-700 border-b pb-4">
<div class="flex items-center justify-between">
<div>
<h3 class="text-heading text-lg font-medium">Necessary Cookies</h3>
<p class="text-tertiary text-sm">These cookies are required for the website to function and cannot be disabled.</p>
</div>
<div class="ml-3 flex h-5 items-center">
<input checked="" class="h-4 w-4 rounded-xl border-gray-300 text-primary focus:ring-primary dark:border-gray-600 dark:bg-gray-700" disabled="" id="necessary-cookies" name="necessary-cookies" type="checkbox"/>
</div>
</div>
</div>
<div class="border-gray-200 dark:border-gray-700 border-b pb-4">
<div class="flex items-center justify-between">
<div>
<h3 class="text-heading text-lg font-medium">Analytics Cookies</h3>
<p class="text-tertiary text-sm">These cookies help us understand how visitors interact with our website.</p>
</div>
<div class="ml-3 flex h-5 items-center">
<input class="h-4 w-4 rounded-xl border-gray-300 text-primary focus:ring-primary dark:border-gray-600 dark:bg-gray-700" id="analytics-cookies" name="analytics-cookies" type="checkbox"/>
</div>
</div>
</div>
</div>
<div class="mt-6 flex justify-end gap-x-3">
<a aria-label="Cancel" class="btn-secondary dark:btn-secondary-dark px-3 py-2 not-prose group" data-cookie-settings-close="" href="#" target="_self">
      Cancel
      
      
    </a>
<a aria-label="Save Preferences" class="btn-primary dark:btn-primary-dark px-3 py-2 not-prose group" data-cookie-settings-save="" href="#" target="_self">
      Save Preferences
      
      
    </a>
</div>
</div>
</div>
<button aria-label="Back to Top" class="fixed bottom-8 right-8 z-[100] p-3 rounded-full bg-indigo-600 text-white shadow-lg transition-all duration-300 transform translate-y-12 opacity-0 invisible hover:bg-indigo-700 hover:shadow-xl focus:outline-none focus:ring-2 focus:ring-indigo-500 focus:ring-offset-2 dark:bg-indigo-500 dark:hover:bg-indigo-400" id="back-to-top-btn" onclick="window.scrollTo({top: 0, behavior: 'smooth'});">
<svg class="h-6 w-6" fill="none" stroke="currentColor" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
<path d="M5 10l7-7m0 0l7 7m-7-7v18" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"></path>
</svg>
</button>
<script>
document.addEventListener('DOMContentLoaded', () => {
  const backToTopBtn = document.getElementById('back-to-top-btn');
  if (!backToTopBtn) return;

  const scrollThreshold = 300;

  const toggleVisibility = () => {
    if (window.scrollY > scrollThreshold) {
      backToTopBtn.classList.remove('translate-y-12', 'opacity-0', 'invisible');
      backToTopBtn.classList.add('translate-y-0', 'opacity-100', 'visible');
    } else {
      backToTopBtn.classList.remove('translate-y-0', 'opacity-100', 'visible');
      backToTopBtn.classList.add('translate-y-12', 'opacity-0', 'invisible');
    }
  };

  let ticking = false;
  window.addEventListener('scroll', () => {
    if (!ticking) {
      window.requestAnimationFrame(() => {
        toggleVisibility();
        ticking = false;
      });
      ticking = true;
    }
  });
});
</script>
<script src="/js/app.js?v=20260111190821"></script>
</body>
</html>