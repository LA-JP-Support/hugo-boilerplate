<!DOCTYPE html>
<html dir="ltr" lang="en">
<head>
<meta content="strict-origin-when-cross-origin" name="referrer"/>
<meta charset="utf-8"/>
<meta content="minimum-scale=1, width=device-width, initial-scale=1.0, shrink-to-fit=no" name="viewport"/>
<title>Reinforcement Learning | SmartWeb</title>
<link href="https://main.d1jtfhinlastnr.amplifyapp.com/en/glossary/reinforcement-learning/" rel="canonical"/>
<link href="/images/faivicon.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="/images/faivicon.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="/images/faivicon.png" rel="apple-touch-icon" sizes="180x180"/>
<link href="/images/faivicon.png" rel="shortcut icon"/>
<link href="https://main.d1jtfhinlastnr.amplifyapp.com/glossary/reinforcement-learning/" hreflang="en" rel="alternate"/>
<link href="https://main.d1jtfhinlastnr.amplifyapp.com/ja/glossary/reinforcement-learning/" hreflang="ja" rel="alternate"/>
<link href="https://main.d1jtfhinlastnr.amplifyapp.com/glossary/reinforcement-learning/" hreflang="x-default" rel="alternate"/>
<meta content="A machine learning approach where an AI agent learns to make better decisions by trying different actions and receiving rewards or penalties based on results." name="description"/>
<meta content="reinforcement learning, machine learning, AI, agent, Markov Decision Process" name="keywords"/>
<meta content="website" property="og:type"/>
<meta content="https://main.d1jtfhinlastnr.amplifyapp.com/en/glossary/reinforcement-learning/" property="og:url"/>
<meta content="Reinforcement Learning | SmartWeb" property="og:title"/>
<meta content="A machine learning approach where an AI agent learns to make better decisions by trying different actions and receiving rewards or penalties based on results." property="og:description"/>
<meta content="" property="og:image"/>
<meta content="1200" property="og:image:width"/>
<meta content="630" property="og:image:height"/>
<meta content="summary_large_image" name="twitter:card"/>
<meta content="https://main.d1jtfhinlastnr.amplifyapp.com/en/glossary/reinforcement-learning/" name="twitter:url"/>
<meta content="Reinforcement Learning | SmartWeb" name="twitter:title"/>
<meta content="A machine learning approach where an AI agent learns to make better decisions by trying different actions and receiving rewards or penalties based on results." name="twitter:description"/>
<meta content="" name="twitter:image"/>
<style>
  :root {
     
    --color-primary: #1a73e8;
    --color-primary-light: #4285f4;
    --color-primary-dark: #1557b0;

    --color-secondary: #34a853;
    --color-accent: #fbbc05;

    --color-text: #202124;
    --color-background: #ffffff;

     
    --gradient-primary: linear-gradient(to right, var(--color-primary), var(--color-primary-light));
  }

   
  .bg-gradient-primary {
    background-image: var(--gradient-primary);
  }

  .text-gradient,
  .text-gradient-primary {
    background-image: var(--gradient-primary);
    background-clip: text;
    -webkit-background-clip: text;
    color: transparent;
    -webkit-text-fill-color: transparent;
  }
</style>
<link as="font" crossorigin="anonymous" href="/fonts/inter/Inter-VariableFont_opsz,wght.woff2" rel="preload" type="font/woff2"/>
<link href="https://fonts.googleapis.com" rel="preconnect"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css2?family=Noto+Sans+JP:wght@400;500;700&amp;family=Noto+Serif+JP:wght@400;500;600;700&amp;display=swap" rel="stylesheet"/>
<link crossorigin="anonymous" href="/css/main.css?v=20260111190821" rel="stylesheet"/>
<link crossorigin="anonymous" href="/css/custom-code-blockquote.css?v=20260111190821" rel="stylesheet"/>
<script defer="" src="/js/main.js?v=20260111190821"></script>
</head>
<body class="antialiased bg-white">
<header class="bg-white">
<nav aria-label="Global" class="mx-auto flex max-w-7xl items-center justify-between gap-x-6 p-6 lg:px-8">
<div class="flex lg:flex-1">
<a class="-m-1.5 p-1.5" href="/en/">
<span class="sr-only">SmartWeb</span>
<picture class="lazy-picture" data-maxwidth="200">
<source data-original-src="/images/smartweb-logo.png" data-srcset="/images/smartweb-logo.png 466w" sizes="200px" type="image/png">
<img alt="SmartWeb Logo" class="lazy-image h-6 sm:h-10 md:h-12 w-auto" data-original-src="/images/smartweb-logo.png" data-src="/images/smartweb-logo.png" decoding="async" loading="lazy"/>
</source></picture>
</a>
</div>
<div class="hidden lg:flex lg:gap-x-12"><a class="text-sm/6 font-semibold text-gray-900" href="/en/">Home</a><a class="text-sm/6 font-semibold text-gray-900" href="/en/blog/">Blog</a><a class="text-sm/6 font-semibold text-gray-900" href="/en/glossary/">Glossary</a><a class="text-sm/6 font-semibold text-gray-900" href="https://www.intwk.co.jp/about/">Company</a></div>
<div class="flex flex-1 items-center justify-end gap-x-6">
<a class="hidden text-sm/6 font-semibold text-gray-900 lg:block" href="https://support.smartweb.jp/">Support</a>
<a class="rounded-md bg-indigo-600 px-3 py-2 text-sm font-semibold text-white shadow-xs hover:bg-indigo-500 focus-visible:outline-2 focus-visible:outline-offset-2 focus-visible:outline-indigo-600" href="/en/blog/">Get Started</a>
</div>
<div class="flex lg:hidden">
<button aria-controls="mobile-menu-1768126101320965000" aria-expanded="false" class="-m-2.5 inline-flex items-center justify-center rounded-xl p-2.5 text-gray-700" type="button">
<span class="sr-only">Open main menu</span>
<svg aria-hidden="true" class="size-6" data-slot="icon" fill="none" stroke="currentColor" stroke-width="1.5" viewbox="0 0 24 24">
<path d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5" stroke-linecap="round" stroke-linejoin="round"></path>
</svg>
</button>
</div>
</nav>
<div aria-modal="true" class="lg:hidden hidden relative z-50" id="mobile-menu-1768126101320965000" role="dialog">
<div class="fixed inset-0 z-50 bg-black bg-opacity-25"></div>
<div class="fixed inset-y-0 right-0 z-50 w-full overflow-y-auto bg-white px-6 py-6 sm:max-w-sm sm:ring-1 sm:ring-gray-900/10 transform transition-transform duration-300 ease-in-out translate-x-full">
<div class="flex items-center gap-x-6">
<a class="-m-1.5 p-1.5" href="/en/">
<span class="sr-only">SmartWeb</span>
<picture class="lazy-picture" data-maxwidth="3000">
<source data-original-src="/images/smartweb-logo.png" data-srcset="/images/smartweb-logo.png 466w" sizes="(max-width: 466px) 466px, 3000px" type="image/png">
<img alt="SmartWeb Logo" class="lazy-image h-6 sm:h-10 md:h-12 w-auto" data-original-src="/images/smartweb-logo.png" data-src="/images/smartweb-logo.png" decoding="async" loading="lazy"/>
</source></picture>
</a>
<a class="ml-auto rounded-md bg-indigo-600 px-3 py-2 text-sm font-semibold text-white shadow-xs hover:bg-indigo-500 focus-visible:outline-2 focus-visible:outline-offset-2 focus-visible:outline-indigo-600" href="/en/blog/">Get Started</a>
<button class="-m-2.5 rounded-xl p-2.5 text-gray-700 close-mobile-menu" type="button">
<span class="sr-only">Close menu</span>
<svg aria-hidden="true" class="size-6" data-slot="icon" fill="none" stroke="currentColor" stroke-width="1.5" viewbox="0 0 24 24">
<path d="M6 18 18 6M6 6l12 12" stroke-linecap="round" stroke-linejoin="round"></path>
</svg>
</button>
</div>
<div class="mt-6 flow-root">
<div class="-my-6 divide-y divide-gray-500/10">
<div class="space-y-2 py-6"><a class="-mx-3 block rounded-xl px-3 py-2 text-base/7 font-semibold text-gray-900 hover:bg-gray-50" href="/en/">Home</a><a class="-mx-3 block rounded-xl px-3 py-2 text-base/7 font-semibold text-gray-900 hover:bg-gray-50" href="/en/blog/">Blog</a><a class="-mx-3 block rounded-xl px-3 py-2 text-base/7 font-semibold text-gray-900 hover:bg-gray-50" href="/en/glossary/">Glossary</a><a class="-mx-3 block rounded-xl px-3 py-2 text-base/7 font-semibold text-gray-900 hover:bg-gray-50" href="https://www.intwk.co.jp/about/">Company</a></div>
<div class="py-6">
<a class="-mx-3 block rounded-lg px-3 py-2.5 text-base/7 font-semibold text-gray-900 hover:bg-gray-50" href="https://support.smartweb.jp/">Support</a>
</div>
</div>
</div>
</div>
</div>
</header>
<script>
  document.addEventListener('DOMContentLoaded', function() {
    const mobileMenuButton = document.querySelector('[aria-controls="mobile-menu-1768126101320965000"]');
    const mobileMenu = document.getElementById('mobile-menu-1768126101320965000');
    const closeButtons = document.querySelectorAll('.close-mobile-menu');
    const mobileMenuContent = mobileMenu.querySelector('.fixed.inset-y-0');
    
    if (mobileMenuButton && mobileMenu) {
      mobileMenuButton.addEventListener('click', function() {
        const expanded = mobileMenuButton.getAttribute('aria-expanded') === 'true';
        
        if (expanded) {
          closeMobileMenu();
        } else {
          openMobileMenu();
        }
      });
      
      
      closeButtons.forEach(button => {
        button.addEventListener('click', closeMobileMenu);
      });
      
      
      mobileMenu.addEventListener('click', function(event) {
        if (event.target === mobileMenu) {
          closeMobileMenu();
        }
      });
      
      
      mobileMenuContent.addEventListener('click', function(event) {
        event.stopPropagation();
      });
      
      function openMobileMenu() {
        mobileMenuButton.setAttribute('aria-expanded', 'true');
        mobileMenu.classList.remove('hidden');
        
        
        mobileMenuContent.offsetHeight;
        
        mobileMenuContent.classList.remove('translate-x-full');
        document.body.classList.add('overflow-hidden');
      }
      
      function closeMobileMenu() {
        mobileMenuButton.setAttribute('aria-expanded', 'false');
        mobileMenuContent.classList.add('translate-x-full');
        
        
        setTimeout(() => {
          mobileMenu.classList.add('hidden');
          document.body.classList.remove('overflow-hidden');
        }, 300);
      }
    }
  });
</script>
<main class="w-full">
<article class="mx-auto max-w-5xl px-4 sm:px-6 lg:px-8">
<header class="py-12 sm:py-16">
<div class="mx-auto max-w-4xl">
<div class="mb-8">
<nav class="text-sm hidden sm:block">
<ol class="flex items-center space-x-2 text-gray-400 dark:text-gray-500">
<li class="flex items-center">
<a class="hover:text-gray-900 dark:hover:text-gray-300 transition-colors flex items-center" href="/en/">
<img alt="Home" class="h-4 w-4 opacity-60" src="/images/home-icon.png"/>
</a>
</li>
<li><span class="mx-2 text-gray-300 dark:text-gray-600">/</span></li>
<li>
<a class="hover:text-gray-900 dark:hover:text-gray-300 transition-colors" href="/en/glossary/">
                Glossary
              </a>
</li>
<li><span class="mx-2 text-gray-300 dark:text-gray-600">/</span></li>
<li class="text-gray-600 dark:text-gray-400 truncate max-w-xs">Reinforcement Learning</li>
</ol>
</nav>
</div>
<div class="mb-6">
<span class="inline-flex items-center text-xs font-medium tracking-wider uppercase text-gray-500 dark:text-gray-400 border-b border-gray-300 dark:border-gray-600 pb-1">
<svg class="mr-2 h-3.5 w-3.5" fill="none" stroke="currentColor" viewbox="0 0 24 24">
<path d="M7 7h.01M7 3h5c.512 0 1.024.195 1.414.586l7 7a2 2 0 010 2.828l-7 7a2 2 0 01-2.828 0l-7-7A1.994 1.994 0 013 12V7a4 4 0 014-4z" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"></path>
</svg>
            AI Chatbot &amp; Automation
          </span>
</div>
<h1 class="text-2xl font-bold tracking-tight text-gray-900 dark:text-white sm:text-3xl md:text-4xl leading-tight mb-2">
        Reinforcement Learning
      </h1>
<div class="mb-6"></div>
<p class="text-base sm:text-lg leading-relaxed text-gray-600 dark:text-gray-300 font-light max-w-3xl">
          A machine learning approach where an AI agent learns to make better decisions by trying different actions and receiving rewards or penalties based on results.
        </p>
<div class="mt-6 mb-4 border-t border-gray-100 dark:border-gray-800"></div>
<div class="flex flex-col sm:flex-row sm:items-center sm:justify-between gap-4">
<div class="flex flex-wrap gap-2">
<span class="inline-flex items-center px-2.5 py-1 text-xs font-medium text-gray-600 dark:text-gray-400 bg-transparent border border-gray-200 dark:border-gray-700 rounded hover:border-gray-400 dark:hover:border-gray-500 transition-colors">
                reinforcement learning
              </span>
<span class="inline-flex items-center px-2.5 py-1 text-xs font-medium text-gray-600 dark:text-gray-400 bg-transparent border border-gray-200 dark:border-gray-700 rounded hover:border-gray-400 dark:hover:border-gray-500 transition-colors">
                machine learning
              </span>
<span class="inline-flex items-center px-2.5 py-1 text-xs font-medium text-gray-600 dark:text-gray-400 bg-transparent border border-gray-200 dark:border-gray-700 rounded hover:border-gray-400 dark:hover:border-gray-500 transition-colors">
                AI
              </span>
<span class="inline-flex items-center px-2.5 py-1 text-xs font-medium text-gray-600 dark:text-gray-400 bg-transparent border border-gray-200 dark:border-gray-700 rounded hover:border-gray-400 dark:hover:border-gray-500 transition-colors">
                agent
              </span>
<span class="inline-flex items-center px-2.5 py-1 text-xs font-medium text-gray-600 dark:text-gray-400 bg-transparent border border-gray-200 dark:border-gray-700 rounded hover:border-gray-400 dark:hover:border-gray-500 transition-colors">
                Markov Decision Process
              </span>
</div>
<div class="text-sm text-gray-500 dark:text-gray-400 flex flex-col items-end gap-y-1 text-right">
<span class="inline-flex items-center justify-end">
<svg class="mr-1.5 h-4 w-4" fill="none" stroke="currentColor" viewbox="0 0 24 24">
<path d="M8 7V5a3 3 0 013-3h2a3 3 0 013 3v2m4 0h-16a2 2 0 00-2 2v9a2 2 0 002 2h16a2 2 0 002-2V9a2 2 0 00-2-2z" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"></path>
</svg>
      
        Created: December 18, 2025
      
    </span>
</div>
</div>
</div>
</header>
<div class="prose prose-base sm:prose-lg dark:prose-invert mx-auto max-w-4xl py-6 sm:py-8">
<h2 id="what-is-reinforcement-learning">What Is Reinforcement Learning?</h2>
<p>Reinforcement learning (RL) is a paradigm of machine learning in which an intelligent agent learns to make optimal sequential decisions by interacting with an environment and receiving feedback in the form of rewards or penalties. Unlike <a data-lb="1" href="/en/glossary/supervised-learning/" title="Supervised Learning glossary entry">supervised learning</a>, where models learn from labeled datasets, RL agents learn through trial and error, discovering which actions yield the best outcomes over time. The agent’s objective is to maximize its cumulative future reward, adapting its strategy based on experience.</p>
<p>RL is mathematically formalized using <strong>Markov Decision Processes (MDPs)</strong>, where an agent’s current decision depends only on the present state, not on the history of prior states. This framework enables RL to solve complex, sequential decision-making problems across domains including robotics, game playing, autonomous vehicles, finance, healthcare, and resource management.</p>
<h2 id="core-concepts-and-components">Core Concepts and Components</h2>
<p>Understanding RL requires familiarity with its fundamental elements:</p>
<table>
<thead>
<tr>
<th>Component</th>
<th>Definition</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Agent</strong></td>
<td>The learner or decision-maker</td>
<td>Robot, game AI, trading algorithm</td>
</tr>
<tr>
<td><strong>Environment</strong></td>
<td>The external system the agent interacts with</td>
<td>Physical world, game board, financial market</td>
</tr>
<tr>
<td><strong>State</strong></td>
<td>Complete description of the agent’s situation</td>
<td>Robot position, chess board configuration</td>
</tr>
<tr>
<td><strong>Action</strong></td>
<td>Possible moves the agent can take</td>
<td>Move forward, place piece, buy/sell</td>
</tr>
<tr>
<td><strong>Reward</strong></td>
<td>Immediate feedback signal from environment</td>
<td>+10 for goal, -1 for collision, +0.5 for progress</td>
</tr>
<tr>
<td><strong>Policy</strong></td>
<td>Strategy mapping states to actions</td>
<td>“If close to goal, move toward it”</td>
</tr>
<tr>
<td><strong>Value Function</strong></td>
<td>Expected cumulative reward from a state</td>
<td>“This position leads to winning 70% of time”</td>
</tr>
</tbody>
</table>
<h3 id="the-agent">The Agent</h3>
<p>The agent is the learner that interacts with the environment by observing states, selecting actions, and updating its policy based on received rewards. Agents can be simple (table-based) or complex (deep <a data-lb="1" href="/en/glossary/neural-networks/" title="Neural Networks glossary entry">neural networks</a>).</p>
<p><strong>Key characteristics:</strong></p>
<ul>
<li>Autonomous decision-making</li>
<li>Learning from experience</li>
<li>Goal-directed behavior</li>
<li>Ability to balance exploration and exploitation</li>
</ul>
<h3 id="the-environment">The Environment</h3>
<p>The environment provides observations (states) and rewards in response to the agent’s actions, and transitions to new states. Environments can be:</p>
<ul>
<li><strong>Deterministic:</strong> Same action from same state always produces same result</li>
<li><strong>Stochastic:</strong> Outcomes have probabilistic variation</li>
<li><strong>Fully Observable:</strong> Agent sees complete state</li>
<li><strong>Partially Observable:</strong> Agent sees incomplete information</li>
<li><strong>Discrete:</strong> Finite states and actions</li>
<li><strong>Continuous:</strong> Infinite states or actions (robotics, control systems)</li>
</ul>
<h3 id="state-space">State Space</h3>
<p>The state is a complete description of the agent’s situation at any given time. In formal RL, states satisfy the <strong>Markov property</strong>: the future depends only on the present state, not on how the agent arrived there.</p>
<p><strong>State representation examples:</strong></p>
<ul>
<li><strong>Chess:</strong> Board position, piece locations, whose turn</li>
<li><strong>Robotics:</strong> Joint angles, velocities, sensor readings</li>
<li><strong>Finance:</strong> Portfolio composition, market prices, economic indicators</li>
</ul>
<h3 id="action-space">Action Space</h3>
<p>Actions are the possible moves available to the agent in each state. Action spaces can be:</p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
<th>Example Applications</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Discrete</strong></td>
<td>Finite set of distinct actions</td>
<td>Board games, text generation, routing</td>
</tr>
<tr>
<td><strong>Continuous</strong></td>
<td>Real-valued action parameters</td>
<td>Robot control, autonomous driving, HVAC systems</td>
</tr>
<tr>
<td><strong>Hybrid</strong></td>
<td>Mix of discrete and continuous</td>
<td>Drone navigation (discrete mode + continuous velocity)</td>
</tr>
</tbody>
</table>
<h3 id="reward-signal">Reward Signal</h3>
<p>The reward is a scalar value provided by the environment after each action, indicating the immediate desirability of that action’s outcome. The reward function defines the goal of the RL problem.</p>
<p><strong>Reward design principles:</strong></p>
<ul>
<li><strong>Clear objective:</strong> Rewards should align with desired behavior</li>
<li><strong>Immediate feedback:</strong> Rewards given promptly after actions</li>
<li><strong>Sparse vs. Dense:</strong> Balance between frequent small rewards and rare large rewards</li>
<li><strong>Avoid reward hacking:</strong> Design to prevent unintended exploitation</li>
</ul>
<p><strong>Example reward structures:</strong></p>
<ul>
<li><strong>Goal-based:</strong> +100 for reaching goal, 0 otherwise</li>
<li><strong>Step-penalized:</strong> -1 per time step to encourage efficiency</li>
<li><strong>Shaped:</strong> Gradual rewards guiding toward goal</li>
</ul>
<h3 id="policy">Policy</h3>
<p>A policy π defines the agent’s behavior, mapping states to actions. Policies can be:</p>
<p><strong>Deterministic Policy:</strong></p>
<pre tabindex="0"><code>π(s) = a
</code></pre><p>Always selects the same action a in state s.</p>
<p><strong>Stochastic Policy:</strong></p>
<pre tabindex="0"><code>π(a|s) = P(action=a | state=s)
</code></pre><p>Selects actions probabilistically, useful for exploration.</p>
<h3 id="value-function">Value Function</h3>
<p>Value functions estimate the expected cumulative reward achievable from states or state-action pairs:</p>
<p><strong>State Value Function V(s):</strong></p>
<pre tabindex="0"><code>V(s) = E[Σ γᵗ rₜ | s₀ = s]
</code></pre><p>Expected return starting from state s.</p>
<p><strong>Action Value Function Q(s,a):</strong></p>
<pre tabindex="0"><code>Q(s,a) = E[Σ γᵗ rₜ | s₀ = s, a₀ = a]
</code></pre><p>Expected return from taking action a in state s.</p>
<p><strong>Advantage Function A(s,a):</strong></p>
<pre tabindex="0"><code>A(s,a) = Q(s,a) - V(s)
</code></pre><p>Measures how much better action a is compared to average.</p>
<h2 id="mathematical-framework-markov-decision-process">Mathematical Framework: Markov Decision Process</h2>
<p>RL problems are formalized as Markov Decision Processes (MDPs):</p>
<table>
<thead>
<tr>
<th>MDP Component</th>
<th>Symbol</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>State Space</strong></td>
<td>S</td>
<td>Set of all possible states</td>
</tr>
<tr>
<td><strong>Action Space</strong></td>
<td>A</td>
<td>Set of all possible actions</td>
</tr>
<tr>
<td><strong>Transition Function</strong></td>
<td>P(s'</td>
<td>s,a)</td>
</tr>
<tr>
<td><strong>Reward Function</strong></td>
<td>R(s,a)</td>
<td>Expected reward for action a in state s</td>
</tr>
<tr>
<td><strong>Discount Factor</strong></td>
<td>γ ∈ [0,1]</td>
<td>Importance weight for future rewards</td>
</tr>
</tbody>
</table>
<h3 id="the-bellman-equation">The Bellman Equation</h3>
<p>The Bellman equation expresses the recursive relationship between the value of a state and its successors:</p>
<p><strong>State Value:</strong></p>
<pre tabindex="0"><code>V(s) = max_a [R(s,a) + γ Σ P(s'|s,a) V(s')]
</code></pre><p><strong>Action Value:</strong></p>
<pre tabindex="0"><code>Q(s,a) = R(s,a) + γ Σ P(s'|s,a) max_a' Q(s',a')
</code></pre><p>This recursion forms the basis for many RL algorithms, enabling value estimation through dynamic programming, temporal difference learning, or Monte Carlo methods.</p>
<h3 id="exploration-vs-exploitation">Exploration vs. Exploitation</h3>
<p>A fundamental challenge in RL is balancing:</p>
<table>
<thead>
<tr>
<th>Strategy</th>
<th>Description</th>
<th>When to Use</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Exploration</strong></td>
<td>Try new actions to discover better strategies</td>
<td>Early learning, high uncertainty</td>
</tr>
<tr>
<td><strong>Exploitation</strong></td>
<td>Choose known best actions to maximize reward</td>
<td>Later learning, confident policies</td>
</tr>
</tbody>
</table>
<p><strong>Common strategies:</strong></p>
<ul>
<li><strong>ε-greedy:</strong> Explore random action with probability ε, exploit best action with probability 1-ε</li>
<li><strong>Softmax/Boltzmann:</strong> Probabilistic selection based on action values</li>
<li><strong>Upper Confidence Bound (UCB):</strong> Balance based on uncertainty estimates</li>
<li><strong>Thompson Sampling:</strong> Bayesian approach to exploration</li>
</ul>
<h2 id="types-of-reinforcement-learning-algorithms">Types of Reinforcement Learning Algorithms</h2>
<h3 id="model-free-vs-model-based-rl">Model-Free vs. Model-Based RL</h3>
<table>
<thead>
<tr>
<th>Approach</th>
<th>Description</th>
<th>Advantages</th>
<th>Disadvantages</th>
<th>Example Algorithms</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Model-Free</strong></td>
<td>Learns directly from experience without building environment model</td>
<td>Simpler, works in complex environments</td>
<td>Less sample efficient</td>
<td>Q-Learning, SARSA, Policy Gradient, PPO</td>
</tr>
<tr>
<td><strong>Model-Based</strong></td>
<td>Builds predictive model of environment for planning</td>
<td>More sample efficient, enables planning</td>
<td>Model errors can degrade performance</td>
<td>Dyna-Q, PETS, World Models</td>
</tr>
</tbody>
</table>
<h3 id="value-based-algorithms">Value-Based Algorithms</h3>
<p>Learn value functions (V or Q) to derive policies:</p>
<p><strong>Q-Learning (Off-Policy TD Control):</strong></p>
<pre tabindex="0"><code>Q(s,a) ← Q(s,a) + α[r + γ max_a' Q(s',a') - Q(s,a)]
</code></pre><p><strong>Key properties:</strong></p>
<ul>
<li>Off-policy: Learns optimal policy while following different policy</li>
<li>Guaranteed convergence in tabular settings</li>
<li>Foundation for Deep Q-Networks (DQN)</li>
</ul>
<p><strong>SARSA (On-Policy TD Control):</strong></p>
<pre tabindex="0"><code>Q(s,a) ← Q(s,a) + α[r + γ Q(s',a') - Q(s,a)]
</code></pre><p><strong>Key properties:</strong></p>
<ul>
<li>On-policy: Learns value of policy being followed</li>
<li>More conservative than Q-Learning</li>
<li>Better for safety-critical applications</li>
</ul>
<h3 id="policy-based-algorithms">Policy-Based Algorithms</h3>
<p>Learn policies directly without explicit value functions:</p>
<p><strong>Policy Gradient (REINFORCE):</strong></p>
<pre tabindex="0"><code>∇J(θ) = E[∇log π(a|s,θ) Q(s,a)]
</code></pre><p><strong>Advantages:</strong></p>
<ul>
<li>Handles continuous action spaces naturally</li>
<li>Can learn stochastic policies</li>
<li>Better convergence properties in some cases</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li>High variance in gradient estimates</li>
<li>Sample inefficient</li>
<li>Requires many episodes</li>
</ul>
<h3 id="actor-critic-algorithms">Actor-Critic Algorithms</h3>
<p>Combine value-based and policy-based approaches:</p>
<table>
<thead>
<tr>
<th>Component</th>
<th>Role</th>
<th>Implementation</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Actor</strong></td>
<td>Learns and executes policy</td>
<td>Policy network π(a</td>
</tr>
<tr>
<td><strong>Critic</strong></td>
<td>Evaluates actions</td>
<td>Value network V(s,w) or Q(s,a,w)</td>
</tr>
</tbody>
</table>
<p><strong>Popular Actor-Critic Methods:</strong></p>
<p><strong>Advantage Actor-Critic (A2C):</strong></p>
<ul>
<li>Uses advantage function to reduce variance</li>
<li>Synchronous updates across parallel environments</li>
</ul>
<p><strong>Asynchronous Advantage Actor-Critic (A3C):</strong></p>
<ul>
<li>Multiple agents learn in parallel</li>
<li>Asynchronous updates for faster training</li>
</ul>
<p><strong>Proximal Policy Optimization (PPO):</strong></p>
<ul>
<li>Constrains policy updates for stability</li>
<li>Industry standard for many applications</li>
</ul>
<p><strong>Deep Deterministic Policy Gradient (DDPG):</strong></p>
<ul>
<li>Actor-critic for continuous control</li>
<li>Uses experience replay and target networks</li>
</ul>
<p><strong>Twin Delayed DDPG (TD3):</strong></p>
<ul>
<li>Addresses overestimation <a data-lb="1" href="/en/glossary/bias/" title="Bias glossary entry">bias</a> in DDPG</li>
<li>Uses twin Q-networks and delayed updates</li>
</ul>
<p><strong>Soft Actor-Critic (SAC):</strong></p>
<ul>
<li>Maximizes entropy for robustness</li>
<li>State-of-the-art for continuous control</li>
</ul>
<h2 id="deep-reinforcement-learning">Deep Reinforcement Learning</h2>
<p>Deep RL combines RL with deep neural networks to handle high-dimensional state and action spaces:</p>
<table>
<thead>
<tr>
<th>Technique</th>
<th>Purpose</th>
<th>Key Insight</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Deep Q-Networks (DQN)</strong></td>
<td>Value-based learning with neural networks</td>
<td>Experience replay + target networks</td>
</tr>
<tr>
<td><strong>Double DQN</strong></td>
<td>Reduce Q-value overestimation</td>
<td>Separate networks for action selection and evaluation</td>
</tr>
<tr>
<td><strong>Dueling DQN</strong></td>
<td>Separate value and advantage estimation</td>
<td>V(s) + A(s,a) architecture</td>
</tr>
<tr>
<td><strong>Prioritized Experience Replay</strong></td>
<td>Focus learning on important transitions</td>
<td>Weight samples by TD error</td>
</tr>
<tr>
<td><strong>Rainbow DQN</strong></td>
<td>Combine multiple DQN improvements</td>
<td>Integration of 6+ enhancements</td>
</tr>
</tbody>
</table>
<p><strong>Breakthrough applications:</strong></p>
<ul>
<li><a data-lb="1" href="/en/glossary/alphago/" title="AlphaGo is DeepMind's AI program that defeated the world Go champion in 2016, using deep reinforcement learning and Monte Carlo tree search.">AlphaGo</a>: Defeated world champion Go player</li>
<li><a data-lb="1" href="/en/glossary/openai/" title="OpenAI glossary entry">OpenAI</a> Five: Achieved superhuman performance in Dota 2</li>
<li>MuZero: Mastered Chess, Shogi, Go, and Atari without rules</li>
</ul>
<h2 id="practical-applications-and-use-cases">Practical Applications and Use Cases</h2>
<h3 id="robotics-and-control">Robotics and Control</h3>
<table>
<thead>
<tr>
<th>Application</th>
<th>RL Approach</th>
<th>Impact</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Manipulation</strong></td>
<td>Model-free policy learning</td>
<td>Adaptive grasping, assembly tasks</td>
</tr>
<tr>
<td><strong>Locomotion</strong></td>
<td>Deep RL with physics simulation</td>
<td>Stable walking, running, jumping</td>
</tr>
<tr>
<td><strong>Navigation</strong></td>
<td>Q-learning with vision</td>
<td>Autonomous exploration, obstacle avoidance</td>
</tr>
</tbody>
</table>
<p><strong>Example:</strong> Boston Dynamics uses RL for dynamic movement control in their robots.</p>
<h3 id="game-playing">Game Playing</h3>
<table>
<thead>
<tr>
<th>Game Type</th>
<th>RL Method</th>
<th>Achievement</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Board Games</strong></td>
<td>AlphaGo (MCTS + Deep RL)</td>
<td>Superhuman performance in Go, Chess, Shogi</td>
</tr>
<tr>
<td><strong>Video Games</strong></td>
<td>DQN, PPO</td>
<td>Human-level play in Atari, StarCraft II</td>
</tr>
<tr>
<td><strong>Card Games</strong></td>
<td>Counterfactual Regret Minimization</td>
<td>Poker champion (Libratus, Pluribus)</td>
</tr>
</tbody>
</table>
<h3 id="autonomous-vehicles">Autonomous Vehicles</h3>
<p><strong>RL applications:</strong></p>
<ul>
<li>Lane keeping and lane changing</li>
<li>Traffic light optimization</li>
<li>Route planning under uncertainty</li>
<li>Adaptive cruise control</li>
<li>Parking and maneuvering</li>
</ul>
<p><strong>Challenges:</strong></p>
<ul>
<li>Safety constraints</li>
<li>Real-world deployment risks</li>
<li>Sim-to-real transfer</li>
</ul>
<h3 id="resource-management">Resource Management</h3>
<table>
<thead>
<tr>
<th>Domain</th>
<th>RL Application</th>
<th>Benefit</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Data Centers</strong></td>
<td>HVAC control</td>
<td>40% energy reduction (<a data-lb="1" href="/en/glossary/google-deepmind/" title="Google DeepMind is an AI research laboratory under Alphabet, known for breakthroughs like AlphaGo and AlphaFold that advance artificial general intelligence.">Google DeepMind</a>)</td>
</tr>
<tr>
<td><strong>Energy Grids</strong></td>
<td>Load balancing</td>
<td>Optimized renewable integration</td>
</tr>
<tr>
<td><strong><a data-lb="1" href="/en/glossary/cloud-computing/" title="Cloud Computing glossary entry">Cloud Computing</a></strong></td>
<td>Resource allocation</td>
<td>Dynamic scaling, cost optimization</td>
</tr>
</tbody>
</table>
<h3 id="finance-and-trading">Finance and Trading</h3>
<p><strong>Use cases:</strong></p>
<ul>
<li>Algorithmic trading strategies</li>
<li>Portfolio optimization</li>
<li>Risk management</li>
<li>Market making</li>
<li>Options pricing</li>
</ul>
<p><strong>Example:</strong> JPMorgan uses RL for optimal trade execution.</p>
<h3 id="healthcare">Healthcare</h3>
<table>
<thead>
<tr>
<th>Application</th>
<th>Description</th>
<th>Outcome</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Treatment Planning</strong></td>
<td>Personalized therapy sequences</td>
<td>Improved patient outcomes</td>
</tr>
<tr>
<td><strong>Drug Discovery</strong></td>
<td>Molecular optimization</td>
<td>Accelerated compound development</td>
</tr>
<tr>
<td><strong>Robotic Surgery</strong></td>
<td>Adaptive surgical assistance</td>
<td>Precision and safety</td>
</tr>
</tbody>
</table>
<h3 id="recommendation-systems">Recommendation Systems</h3>
<p><strong>RL advantages over traditional methods:</strong></p>
<ul>
<li>Long-term user engagement optimization</li>
<li>Sequential recommendation adaptation</li>
<li>Exploration of diverse content</li>
<li>Balancing business objectives</li>
</ul>
<p><strong>Examples:</strong> YouTube, Spotify, Netflix use RL for content recommendations.</p>
<h3 id="natural-language-processing">Natural Language Processing</h3>
<p><strong>Applications:</strong></p>
<ul>
<li><a data-lb="1" href="/en/glossary/dialogue-management/" title="Dialogue Management glossary entry">Dialogue management</a> in chatbots</li>
<li>Text summarization</li>
<li>Machine translation</li>
<li>Question answering systems</li>
</ul>
<h2 id="benefits-of-reinforcement-learning">Benefits of Reinforcement Learning</h2>
<table>
<thead>
<tr>
<th>Benefit</th>
<th>Description</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Adaptivity</strong></td>
<td>Learns optimal behavior in dynamic environments</td>
<td>Adaptive robot control</td>
</tr>
<tr>
<td><strong>Autonomy</strong></td>
<td>No labeled data required</td>
<td>Self-learning game agents</td>
</tr>
<tr>
<td><strong>Long-Term Optimization</strong></td>
<td>Maximizes cumulative rewards</td>
<td>Strategic planning</td>
</tr>
<tr>
<td><strong>Continuous Improvement</strong></td>
<td>Performance improves with experience</td>
<td>Online learning systems</td>
</tr>
<tr>
<td><strong>Discovery</strong></td>
<td>Can find novel, non-obvious solutions</td>
<td>AlphaGo’s creative moves</td>
</tr>
<tr>
<td><strong>Generalization</strong></td>
<td>Transfer learning across similar tasks</td>
<td>Multi-task RL</td>
</tr>
</tbody>
</table>
<h2 id="challenges-and-limitations">Challenges and Limitations</h2>
<h3 id="technical-challenges">Technical Challenges</h3>
<table>
<thead>
<tr>
<th>Challenge</th>
<th>Description</th>
<th>Mitigation Strategies</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Sample Inefficiency</strong></td>
<td>Requires millions of interactions</td>
<td>Model-based RL, transfer learning, curriculum learning</td>
</tr>
<tr>
<td><strong>Reward Design</strong></td>
<td>Hard to specify desired behavior</td>
<td>Inverse RL, learning from demonstrations</td>
</tr>
<tr>
<td><strong>Exploration Complexity</strong></td>
<td>Difficult in large state spaces</td>
<td>Intrinsic motivation, curiosity-driven learning</td>
</tr>
<tr>
<td><strong>Credit Assignment</strong></td>
<td>Determining action responsibility for delayed rewards</td>
<td>Eligibility traces, attention mechanisms</td>
</tr>
<tr>
<td><strong>Stability</strong></td>
<td>Training can be unstable</td>
<td>Experience replay, target networks, PPO clipping</td>
</tr>
<tr>
<td><strong>Sim-to-Real Gap</strong></td>
<td>Simulation≠reality</td>
<td>Domain randomization, reality augmentation</td>
</tr>
</tbody>
</table>
<h3 id="computational-requirements">Computational Requirements</h3>
<p><strong>Training demands:</strong></p>
<ul>
<li>GPU/TPU clusters for deep RL</li>
<li>Parallel environment simulation</li>
<li>Extensive hyperparameter tuning</li>
<li>Long training times (days to weeks)</li>
</ul>
<h3 id="safety-and-reliability">Safety and Reliability</h3>
<p><strong>Concerns:</strong></p>
<ul>
<li>Unsafe exploration in physical systems</li>
<li>Reward hacking and specification gaming</li>
<li>Unpredictable behavior in novel situations</li>
<li>Lack of interpretability</li>
</ul>
<p><strong>Solutions:</strong></p>
<ul>
<li>Safe RL with constraint satisfaction</li>
<li>Human-in-the-loop learning</li>
<li>Robust policy verification</li>
<li>Uncertainty quantification</li>
</ul>
<h2 id="implementation-example-q-learning-for-grid-navigation">Implementation Example: Q-Learning for Grid Navigation</h2>
<p><strong>Scenario:</strong> Agent navigates 5x5 grid to reach goal, avoiding obstacles.</p>
<p><strong>Environment setup:</strong></p>
<div class="highlight"><pre style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;" tabindex="0"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">import</span> numpy <span style="color:#fff;font-weight:bold">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># State space: 25 positions</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># Action space: up, down, left, right</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># Rewards: +10 goal, -10 obstacle, -1 step</span>
</span></span></code></pre></div><p><strong>Q-Learning implementation:</strong></p>
<div class="highlight"><pre style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;" tabindex="0"><code class="language-python" data-lang="python"><span style="display:flex;"><span>Q = np.zeros((num_states, num_actions))
</span></span><span style="display:flex;"><span><span style="color:#fff;font-weight:bold">for</span> episode in <span style="color:#fff;font-weight:bold">range</span>(num_episodes):
</span></span><span style="display:flex;"><span>    state = env.reset()
</span></span><span style="display:flex;"><span>    <span style="color:#fff;font-weight:bold">while</span> not done:
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># ε-greedy action selection</span>
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">if</span> np.random.random() &lt; epsilon:
</span></span><span style="display:flex;"><span>            action = env.action_space.sample()
</span></span><span style="display:flex;"><span>        <span style="color:#fff;font-weight:bold">else</span>:
</span></span><span style="display:flex;"><span>            action = np.argmax(Q[state])
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        next_state, reward, done = env.step(action)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#007f7f"># Q-learning update</span>
</span></span><span style="display:flex;"><span>        Q[state, action] += alpha * (
</span></span><span style="display:flex;"><span>            reward + gamma * np.max(Q[next_state]) - Q[state, action]
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        state = next_state
</span></span></code></pre></div><h2 id="rl-vs-other-machine-learning-paradigms">RL vs. Other Machine Learning Paradigms</h2>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Reinforcement Learning</th>
<th>Supervised Learning</th>
<th>Unsupervised Learning</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Training Data</strong></td>
<td>Experience from environment</td>
<td>Labeled input-output pairs</td>
<td>Unlabeled data</td>
</tr>
<tr>
<td><strong>Objective</strong></td>
<td>Maximize cumulative reward</td>
<td>Minimize prediction error</td>
<td>Discover structure</td>
</tr>
<tr>
<td><strong>Feedback</strong></td>
<td>Delayed, sequential</td>
<td>Immediate, explicit</td>
<td>None</td>
</tr>
<tr>
<td><strong>Learning Style</strong></td>
<td>Trial and error</td>
<td>Pattern matching</td>
<td>Pattern discovery</td>
</tr>
<tr>
<td><strong>Exploration</strong></td>
<td>Critical requirement</td>
<td>Not applicable</td>
<td>Not applicable</td>
</tr>
<tr>
<td><strong>Typical Applications</strong></td>
<td>Control, sequential decisions</td>
<td>Classification, regression</td>
<td>Clustering, dimensionality reduction</td>
</tr>
<tr>
<td><strong>Sample Efficiency</strong></td>
<td>Low (requires many interactions)</td>
<td>Moderate to high</td>
<td>High</td>
</tr>
<tr>
<td><strong>Deployment Complexity</strong></td>
<td>High (online learning)</td>
<td>Low (batch prediction)</td>
<td>Moderate</td>
</tr>
</tbody>
</table>
<h2 id="future-directions-and-research-frontiers">Future Directions and Research Frontiers</h2>
<h3 id="emerging-areas">Emerging Areas</h3>
<p><strong>Offline RL (Batch RL):</strong></p>
<ul>
<li>Learn from fixed datasets without environment interaction</li>
<li>Critical for high-stakes domains (healthcare, finance)</li>
</ul>
<p><strong>Multi-Agent RL:</strong></p>
<ul>
<li>Cooperative and competitive multi-agent systems</li>
<li>Emergent communication and coordination</li>
</ul>
<p><strong>Meta-RL:</strong></p>
<ul>
<li>Learning to learn: fast adaptation to new tasks</li>
<li>Few-shot RL for rapid deployment</li>
</ul>
<p><strong>Hierarchical RL:</strong></p>
<ul>
<li>Learning at multiple time scales</li>
<li>Temporal abstractions and skill composition</li>
</ul>
<p><strong>Causal RL:</strong></p>
<ul>
<li>Incorporating causal reasoning</li>
<li>Robust to distribution shifts</li>
</ul>
<p><strong>Explainable RL:</strong></p>
<ul>
<li>Interpretable policies and value functions</li>
<li>Building trust in RL systems</li>
</ul>
<h2 id="frequently-asked-questions">Frequently Asked Questions</h2>
<p><strong>Q: How does RL differ from supervised learning?</strong>
A: RL learns from sequential experience and delayed rewards, while supervised learning learns from labeled examples with immediate feedback.</p>
<p><strong>Q: When should I use RL vs. supervised learning?</strong>
A: Use RL for sequential decision-making where optimal behavior emerges through interaction. Use supervised learning when you have labeled datasets and static predictions.</p>
<p><strong>Q: How much data does RL require?</strong>
A: RL typically requires millions of interactions, though model-based methods and transfer learning can reduce this significantly.</p>
<p><strong>Q: Can RL work without rewards?</strong>
A: Yes, through inverse RL (learning rewards from demonstrations) or intrinsic motivation (curiosity-driven learning).</p>
<p><strong>Q: Is RL suitable for real-time applications?</strong>
A: Yes, once trained. Training is computationally intensive, but inference is fast.</p>
<h2 id="references">References</h2>
<ul>
<li><a href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf" rel="nofollow noopener noreferrer" target="_blank">Sutton &amp; Barto: Reinforcement Learning: An Introduction (2nd Edition)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Reinforcement_learning" rel="nofollow noopener noreferrer" target="_blank">Wikipedia: Reinforcement Learning</a></li>
<li><a href="https://www.geeksforgeeks.org/machine-learning/what-is-reinforcement-learning/" rel="nofollow noopener noreferrer" target="_blank">GeeksforGeeks: What is Reinforcement Learning?</a></li>
<li><a href="https://www.ibm.com/topics/reinforcement-learning" rel="nofollow noopener noreferrer" target="_blank">IBM: What is Reinforcement Learning?</a></li>
<li><a href="https://aws.amazon.com/machine-learning/what-is-reinforcement-learning/" rel="nofollow noopener noreferrer" target="_blank">AWS: What is Reinforcement Learning?</a></li>
<li><a href="https://www.synopsys.com/glossary/what-is-reinforcement-learning.html" rel="nofollow noopener noreferrer" target="_blank">Synopsys: What is Reinforcement Learning?</a></li>
<li><a href="https://www.salesforce.com/blog/reinforcement-learning/" rel="nofollow noopener noreferrer" target="_blank">Salesforce: What is Reinforcement Learning?</a></li>
<li><a href="https://wayve.ai/blog/reinforcement-learning-for-autonomous-driving/" rel="nofollow noopener noreferrer" target="_blank">Wayve: Reinforcement Learning for Autonomous Driving</a></li>
<li><a href="https://www.nature.com/articles/nature16961" rel="nofollow noopener noreferrer" target="_blank">Nature: Mastering the Game of Go with Deep Neural Networks</a></li>
<li><a href="https://www.youtube.com/watch?v=V1eYniJ0Rnk" rel="nofollow noopener noreferrer" target="_blank">DeepMind: Atari Playing Agent (YouTube)</a></li>
<li><a href="https://spinningup.openai.com/" rel="nofollow noopener noreferrer" target="_blank">OpenAI: Spinning Up in Deep RL</a></li>
<li><a href="http://web.stanford.edu/class/cs234/" rel="nofollow noopener noreferrer" target="_blank">Stanford CS234: Reinforcement Learning</a></li>
<li><a href="https://rail.eecs.berkeley.edu/deeprlcourse/" rel="nofollow noopener noreferrer" target="_blank">UC Berkeley CS285: Deep Reinforcement Learning</a></li>
<li><a href="https://www.geeksforgeeks.org/machine-learning/markov-decision-process/" rel="nofollow noopener noreferrer" target="_blank">GeeksforGeeks: Markov Decision Process</a></li>
<li><a href="https://www.geeksforgeeks.org/machine-learning/q-learning-in-python/" rel="nofollow noopener noreferrer" target="_blank">GeeksforGeeks: Q-Learning in Python</a></li>
</ul>
</div>
<div class="mt-16 sm:mt-20 border-t border-gray-200 dark:border-gray-800 pt-12 sm:pt-16">
<h2 class="text-2xl sm:text-3xl font-bold tracking-tight text-gray-900 dark:text-white mb-8 sm:mb-10">
        
          Related Terms
        
      </h2>
<div class="grid gap-6 sm:gap-8 grid-cols-1 sm:grid-cols-2 lg:grid-cols-3">
<article class="group relative flex flex-col overflow-hidden border border-gray-200 dark:border-gray-700 bg-white dark:bg-gray-900 hover:border-gray-400 dark:hover:border-gray-500 transition-all duration-300">
<div class="flex flex-1 flex-col p-5 sm:p-6">
<h3 class="text-base sm:text-lg font-semibold text-gray-900 dark:text-white line-clamp-2 mb-3">
<a class="hover:text-gray-600 dark:hover:text-gray-300 transition-colors" href="https://main.d1jtfhinlastnr.amplifyapp.com/en/glossary/chatbot/">
                    Chatbot
                  </a>
</h3>
<p class="text-sm text-gray-600 dark:text-gray-400 flex-1 line-clamp-3">
                    A computer program that simulates human conversation through text or voice, available 24/7 to automa...
                  </p>
<div class="mt-4 pt-4 border-t border-gray-100 dark:border-gray-800">
<a class="inline-flex items-center text-sm font-medium text-gray-900 dark:text-gray-100 group-hover:translate-x-1 transition-transform" href="https://main.d1jtfhinlastnr.amplifyapp.com/en/glossary/chatbot/">
                    View details
                    <svg class="ml-1.5 h-4 w-4" fill="none" stroke="currentColor" viewbox="0 0 24 24">
<path d="M9 5l7 7-7 7" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"></path>
</svg>
</a>
</div>
</div>
</article>
<article class="group relative flex flex-col overflow-hidden border border-gray-200 dark:border-gray-700 bg-white dark:bg-gray-900 hover:border-gray-400 dark:hover:border-gray-500 transition-all duration-300">
<div class="flex flex-1 flex-col p-5 sm:p-6">
<h3 class="text-base sm:text-lg font-semibold text-gray-900 dark:text-white line-clamp-2 mb-3">
<a class="hover:text-gray-600 dark:hover:text-gray-300 transition-colors" href="https://main.d1jtfhinlastnr.amplifyapp.com/en/glossary/continuous-learning/">
                    Continuous Learning (Continual Learning)
                  </a>
</h3>
<p class="text-sm text-gray-600 dark:text-gray-400 flex-1 line-clamp-3">
                    An <a data-lb="1" href="/en/glossary/artificial-intelligence/" title="Artificial Intelligence (AI) glossary entry">AI system</a> that learns and improves continuously from new data without forgetting previous knowled...
                  </p>
<div class="mt-4 pt-4 border-t border-gray-100 dark:border-gray-800">
<a class="inline-flex items-center text-sm font-medium text-gray-900 dark:text-gray-100 group-hover:translate-x-1 transition-transform" href="https://main.d1jtfhinlastnr.amplifyapp.com/en/glossary/continuous-learning/">
                    View details
                    <svg class="ml-1.5 h-4 w-4" fill="none" stroke="currentColor" viewbox="0 0 24 24">
<path d="M9 5l7 7-7 7" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"></path>
</svg>
</a>
</div>
</div>
</article>
<article class="group relative flex flex-col overflow-hidden border border-gray-200 dark:border-gray-700 bg-white dark:bg-gray-900 hover:border-gray-400 dark:hover:border-gray-500 transition-all duration-300">
<div class="flex flex-1 flex-col p-5 sm:p-6">
<h3 class="text-base sm:text-lg font-semibold text-gray-900 dark:text-white line-clamp-2 mb-3">
<a class="hover:text-gray-600 dark:hover:text-gray-300 transition-colors" href="https://main.d1jtfhinlastnr.amplifyapp.com/en/glossary/precision/">
                    Precision
                  </a>
</h3>
<p class="text-sm text-gray-600 dark:text-gray-400 flex-1 line-clamp-3">
<a data-lb="1" href="/en/glossary/precision/" title="Precision glossary entry">Precision</a> measures how often an AI model's positive predictions are actually correct. It's essential...
                  </p>
<div class="mt-4 pt-4 border-t border-gray-100 dark:border-gray-800">
<a class="inline-flex items-center text-sm font-medium text-gray-900 dark:text-gray-100 group-hover:translate-x-1 transition-transform" href="https://main.d1jtfhinlastnr.amplifyapp.com/en/glossary/precision/">
                    View details
                    <svg class="ml-1.5 h-4 w-4" fill="none" stroke="currentColor" viewbox="0 0 24 24">
<path d="M9 5l7 7-7 7" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"></path>
</svg>
</a>
</div>
</div>
</article>
<article class="group relative flex flex-col overflow-hidden border border-gray-200 dark:border-gray-700 bg-white dark:bg-gray-900 hover:border-gray-400 dark:hover:border-gray-500 transition-all duration-300">
<div class="flex flex-1 flex-col p-5 sm:p-6">
<h3 class="text-base sm:text-lg font-semibold text-gray-900 dark:text-white line-clamp-2 mb-3">
<a class="hover:text-gray-600 dark:hover:text-gray-300 transition-colors" href="https://main.d1jtfhinlastnr.amplifyapp.com/en/glossary/agent-training/">
                    Agent Training
                  </a>
</h3>
<p class="text-sm text-gray-600 dark:text-gray-400 flex-1 line-clamp-3">
                    Agent training is a process of teaching <a data-lb="1" href="/en/glossary/artificial-intelligence/" title="Artificial Intelligence (AI) glossary entry">AI systems</a> to learn from experience and make independent dec...
                  </p>
<div class="mt-4 pt-4 border-t border-gray-100 dark:border-gray-800">
<a class="inline-flex items-center text-sm font-medium text-gray-900 dark:text-gray-100 group-hover:translate-x-1 transition-transform" href="https://main.d1jtfhinlastnr.amplifyapp.com/en/glossary/agent-training/">
                    View details
                    <svg class="ml-1.5 h-4 w-4" fill="none" stroke="currentColor" viewbox="0 0 24 24">
<path d="M9 5l7 7-7 7" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"></path>
</svg>
</a>
</div>
</div>
</article>
<article class="group relative flex flex-col overflow-hidden border border-gray-200 dark:border-gray-700 bg-white dark:bg-gray-900 hover:border-gray-400 dark:hover:border-gray-500 transition-all duration-300">
<div class="flex flex-1 flex-col p-5 sm:p-6">
<h3 class="text-base sm:text-lg font-semibold text-gray-900 dark:text-white line-clamp-2 mb-3">
<a class="hover:text-gray-600 dark:hover:text-gray-300 transition-colors" href="https://main.d1jtfhinlastnr.amplifyapp.com/en/glossary/accuracy-measurement/">
                    Accuracy Measurement
                  </a>
</h3>
<p class="text-sm text-gray-600 dark:text-gray-400 flex-1 line-clamp-3">
<a data-lb="1" href="/en/glossary/accuracy-measurement/" title="Accuracy Measurement glossary entry">Accuracy Measurement</a> is a metric that shows how often an AI system makes correct predictions or deci...
                  </p>
<div class="mt-4 pt-4 border-t border-gray-100 dark:border-gray-800">
<a class="inline-flex items-center text-sm font-medium text-gray-900 dark:text-gray-100 group-hover:translate-x-1 transition-transform" href="https://main.d1jtfhinlastnr.amplifyapp.com/en/glossary/accuracy-measurement/">
                    View details
                    <svg class="ml-1.5 h-4 w-4" fill="none" stroke="currentColor" viewbox="0 0 24 24">
<path d="M9 5l7 7-7 7" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"></path>
</svg>
</a>
</div>
</div>
</article>
<article class="group relative flex flex-col overflow-hidden border border-gray-200 dark:border-gray-700 bg-white dark:bg-gray-900 hover:border-gray-400 dark:hover:border-gray-500 transition-all duration-300">
<div class="flex flex-1 flex-col p-5 sm:p-6">
<h3 class="text-base sm:text-lg font-semibold text-gray-900 dark:text-white line-clamp-2 mb-3">
<a class="hover:text-gray-600 dark:hover:text-gray-300 transition-colors" href="https://main.d1jtfhinlastnr.amplifyapp.com/en/glossary/intent-recognition/">
                    Intent Recognition
                  </a>
</h3>
<p class="text-sm text-gray-600 dark:text-gray-400 flex-1 line-clamp-3">
<a data-lb="1" href="/en/glossary/intent-recognition/" title="Intent Recognition glossary entry">Intent recognition</a> is an AI technology that understands what users want by analyzing the meaning beh...
                  </p>
<div class="mt-4 pt-4 border-t border-gray-100 dark:border-gray-800">
<a class="inline-flex items-center text-sm font-medium text-gray-900 dark:text-gray-100 group-hover:translate-x-1 transition-transform" href="https://main.d1jtfhinlastnr.amplifyapp.com/en/glossary/intent-recognition/">
                    View details
                    <svg class="ml-1.5 h-4 w-4" fill="none" stroke="currentColor" viewbox="0 0 24 24">
<path d="M9 5l7 7-7 7" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"></path>
</svg>
</a>
</div>
</div>
</article>
</div>
</div>
<div class="mt-12 sm:mt-16 py-8 border-t border-gray-200 dark:border-gray-800">
<a class="inline-flex items-center text-sm font-medium text-gray-600 dark:text-gray-400 hover:text-gray-900 dark:hover:text-gray-100 transition-colors" href="/en/glossary/">
<svg class="mr-2 h-5 w-5" fill="none" stroke="currentColor" viewbox="0 0 24 24">
<path d="M10 19l-7-7m0 0l7-7m-7 7h18" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"></path>
</svg>
      
        Back to Glossary
      
    </a>
</div>
</article>
</main>
<footer style="background-color: #000000;">
<div id="cta-curves-container" style="
    position: relative;
    background: #000000;
    padding: 4rem 2rem;
    overflow: hidden;
  ">
<svg id="cta-curves-svg" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; z-index: 1; opacity: 0.7;" xmlns="http://www.w3.org/2000/svg">
<defs>
<lineargradient id="curveGrad1" x1="0%" x2="100%" y1="0%" y2="0%">
<stop offset="0%" style="stop-color:rgba(99, 102, 241, 0); stop-opacity:0"></stop>
<stop offset="50%" style="stop-color:rgba(99, 102, 241, 0.9); stop-opacity:1"></stop>
<stop offset="100%" style="stop-color:rgba(99, 102, 241, 0); stop-opacity:0"></stop>
</lineargradient>
<lineargradient id="curveGrad2" x1="0%" x2="100%" y1="0%" y2="0%">
<stop offset="0%" style="stop-color:rgba(139, 92, 246, 0); stop-opacity:0"></stop>
<stop offset="50%" style="stop-color:rgba(139, 92, 246, 0.8); stop-opacity:1"></stop>
<stop offset="100%" style="stop-color:rgba(139, 92, 246, 0); stop-opacity:0"></stop>
</lineargradient>
<lineargradient id="curveGrad3" x1="0%" x2="100%" y1="0%" y2="0%">
<stop offset="0%" style="stop-color:rgba(59, 130, 246, 0); stop-opacity:0"></stop>
<stop offset="50%" style="stop-color:rgba(59, 130, 246, 0.7); stop-opacity:1"></stop>
<stop offset="100%" style="stop-color:rgba(59, 130, 246, 0); stop-opacity:0"></stop>
</lineargradient>
</defs>
<path class="curve" data-speed="0.8" fill="none" stroke="url(#curveGrad1)" stroke-width="2"></path>
<path class="curve" data-speed="1.2" fill="none" opacity="0.8" stroke="url(#curveGrad2)" stroke-width="2.5"></path>
<path class="curve" data-speed="0.6" fill="none" opacity="0.6" stroke="url(#curveGrad3)" stroke-width="1.5"></path>
<path class="curve" data-speed="1.5" fill="none" opacity="0.7" stroke="url(#curveGrad1)" stroke-width="2"></path>
<path class="curve" data-speed="0.9" fill="none" opacity="0.5" stroke="url(#curveGrad2)" stroke-width="3"></path>
<path class="curve" data-speed="1.3" fill="none" opacity="0.9" stroke="url(#curveGrad3)" stroke-width="1.8"></path>
<path class="curve" data-speed="0.7" fill="none" opacity="0.6" stroke="url(#curveGrad1)" stroke-width="2.2"></path>
<path class="curve" data-speed="1.1" fill="none" opacity="0.8" stroke="url(#curveGrad2)" stroke-width="2"></path>
<path class="curve" data-speed="1.4" fill="none" opacity="0.5" stroke="url(#curveGrad3)" stroke-width="2.5"></path>
<path class="curve" data-speed="0.85" fill="none" opacity="0.7" stroke="url(#curveGrad1)" stroke-width="1.5"></path>
<path class="curve" data-speed="1.0" fill="none" opacity="0.6" stroke="url(#curveGrad2)" stroke-width="2.8"></path>
<path class="curve" data-speed="1.25" fill="none" opacity="0.8" stroke="url(#curveGrad3)" stroke-width="2"></path>
<path class="curve" data-speed="0.95" fill="none" opacity="0.5" stroke="url(#curveGrad1)" stroke-width="2.3"></path>
<path class="curve" data-speed="1.35" fill="none" opacity="0.9" stroke="url(#curveGrad2)" stroke-width="1.7"></path>
<path class="curve" data-speed="0.75" fill="none" opacity="0.7" stroke="url(#curveGrad3)" stroke-width="2.5"></path>
</svg>
<div style="position: absolute; top: 0; left: 0; width: 50px; height: 50px; border-left: 2px solid rgba(99, 102, 241, 0.3); border-top: 2px solid rgba(99, 102, 241, 0.3);"></div>
<div style="position: absolute; top: 0; right: 0; width: 50px; height: 50px; border-right: 2px solid rgba(99, 102, 241, 0.3); border-top: 2px solid rgba(99, 102, 241, 0.3);"></div>
<div style="position: absolute; bottom: 0; left: 0; width: 50px; height: 50px; border-left: 2px solid rgba(99, 102, 241, 0.3); border-bottom: 2px solid rgba(99, 102, 241, 0.3);"></div>
<div style="position: absolute; bottom: 0; right: 0; width: 50px; height: 50px; border-right: 2px solid rgba(99, 102, 241, 0.3); border-bottom: 2px solid rgba(99, 102, 241, 0.3);"></div>
<div class="mx-auto max-w-2xl text-center" style="position: relative; z-index: 10;">
<h2 class="text-base/7 font-semibold text-indigo-400">Ready to get started?</h2>
<p class="mt-2 text-4xl font-semibold tracking-tight text-balance text-white sm:text-5xl">Start using our services today</p>
<p class="mx-auto mt-6 max-w-xl text-lg/8 text-pretty text-gray-400">Join thousands of satisfied customers who have transformed their business with our solutions.</p>
<div class="mt-8 flex justify-center">
<a class="rounded-md bg-indigo-500 px-3.5 py-2.5 text-sm font-semibold text-white shadow-xs hover:bg-indigo-400 focus-visible:outline-2 focus-visible:outline-offset-2 focus-visible:outline-indigo-400" href="/en/blog/">Get started</a>
</div>
</div>
</div>
<script>
  (function() {
    'use strict';
    
    const container = document.getElementById('cta-curves-container');
    const svg = document.getElementById('cta-curves-svg');
    if (!container || !svg) return;
    
    const curves = svg.querySelectorAll('.curve');
    let mouseX = 0;
    let mouseY = 0;
    let targetMouseX = 0;
    let targetMouseY = 0;
    const mouseInfluence = 150;
    const mouseStrength = 80;
    
    const curveData = Array.from(curves).map((curve, i) => {
      const speed = parseFloat(curve.getAttribute('data-speed')) || 1;
      return {
        element: curve,
        baseY: 50 + (i * 25),
        offset: Math.random() * Math.PI * 2,
        speed: speed,
        amplitude: 30 + Math.random() * 40
      };
    });
    
    container.addEventListener('mousemove', (e) => {
      const rect = container.getBoundingClientRect();
      targetMouseX = e.clientX - rect.left;
      targetMouseY = e.clientY - rect.top;
    });
    
    container.addEventListener('mouseleave', () => {
      targetMouseX = -1000;
      targetMouseY = -1000;
    });
    
    let time = 0;
    
    function animate() {
      time += 0.01;
      
      mouseX += (targetMouseX - mouseX) * 0.15;
      mouseY += (targetMouseY - mouseY) * 0.15;
      
      const width = svg.clientWidth;
      const height = svg.clientHeight;
      
      curveData.forEach((data, index) => {
        const { baseY, offset, speed, amplitude } = data;
        const phase = time * speed + offset;
        
        let path = `M -200,${baseY}`;
        
        for (let x = -200; x <= width + 200; x += 50) {
          const normalY = baseY + Math.sin((x * 0.005) + phase) * amplitude;
          
          const dx = x - mouseX;
          const dy = normalY - mouseY;
          const distance = Math.sqrt(dx * dx + dy * dy);
          
          let y = normalY;
          if (distance < mouseInfluence) {
            const influence = (1 - distance / mouseInfluence);
            const pushY = (normalY - mouseY) * influence * mouseStrength * 0.01;
            y = normalY + pushY;
          }
          
          const nextX = x + 50;
          const controlX = x + 25;
          const controlY = y;
          path += ` Q ${controlX},${controlY} ${nextX},${y}`;
        }
        
        data.element.setAttribute('d', path);
      });
      
      requestAnimationFrame(animate);
    }
    
    animate();
  })();
  </script>
<div class="mx-auto max-w-7xl px-6 py-16 sm:py-24 lg:px-8 lg:py-32">
<div class="mt-24 border-t border-white/10 pt-12 xl:grid xl:grid-cols-3 xl:gap-8">
<picture class="lazy-picture" data-maxwidth="200">
<source data-original-src="/images/interwork-logo-white-1.webp" data-srcset="/images/interwork-logo-white-1.webp 568w" sizes="200px" type="image/webp">
<img alt="Interwork" class="lazy-image h-9" data-original-src="/images/interwork-logo-white-1.webp" data-src="/images/interwork-logo-white-1.webp" decoding="async" loading="lazy"/>
</source></picture>
<div class="mt-16 grid grid-cols-2 gap-8 xl:col-span-2 xl:mt-0">
<div class="md:grid md:grid-cols-2 md:gap-8">
<div>
<h3 class="text-sm/6 font-semibold text-white">Services</h3>
<ul class="mt-6 space-y-4" role="list">
<li>
<a class="text-sm/6 text-gray-400 hover:text-white" href="#">AI Solutions</a>
</li>
<li>
<a class="text-sm/6 text-gray-400 hover:text-white" href="#">Web Development</a>
</li>
<li>
<a class="text-sm/6 text-gray-400 hover:text-white" href="#">System Development</a>
</li>
<li>
<a class="text-sm/6 text-gray-400 hover:text-white" href="#">Consulting</a>
</li>
</ul>
</div>
<div class="mt-10 md:mt-0">
<h3 class="text-sm/6 font-semibold text-white">Support</h3>
<ul class="mt-6 space-y-4" role="list">
<li>
<a class="text-sm/6 text-gray-400 hover:text-white" href="https://support.smartweb.jp/">Support Portal</a>
</li>
<li>
<a class="text-sm/6 text-gray-400 hover:text-white" href="#">Documentation</a>
</li>
</ul>
</div>
</div>
<div class="md:grid md:grid-cols-2 md:gap-8">
<div>
<h3 class="text-sm/6 font-semibold text-white">Company</h3>
<ul class="mt-6 space-y-4" role="list">
<li>
<a class="text-sm/6 text-gray-400 hover:text-white" href="https://www.intwk.co.jp/about/">About</a>
</li>
<li>
<a class="text-sm/6 text-gray-400 hover:text-white" href="/en/blog/">Blog</a>
</li>
<li>
<a class="text-sm/6 text-gray-400 hover:text-white" href="#">Careers</a>
</li>
<li>
<a class="text-sm/6 text-gray-400 hover:text-white" href="#">News</a>
</li>
</ul>
</div>
<div class="mt-10 md:mt-0">
<h3 class="text-sm/6 font-semibold text-white">Legal</h3>
<ul class="mt-6 space-y-4" role="list">
<li>
<a class="text-sm/6 text-gray-400 hover:text-white" href="/en/privacy-policy/">Privacy Policy</a>
</li>
<li>
<a class="text-sm/6 text-gray-400 hover:text-white" href="/en/ai-chatbot-terms-of-use/">AI Chatbot Terms of Use</a>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="language-selector mt-12 border-t border-white/10 pt-8 md:flex md:items-center md:justify-between">
<div>
<p class="text-sm/6 font-semibold text-white mb-4">Available Languages</p>
<div class="language-selector">
<div class="flex flex-wrap items-center justify-center gap-3">
<a aria-label="日本語" class="inline-flex items-center text-sm hover:opacity-75 transition-opacity" href="/ja/glossary/reinforcement-learning/" hreflang="ja" title="日本語">
<img alt="日本語" class="rounded" height="18" src="/flags/jp.png" width="24"/>
</a>
<span class="inline-flex items-center text-sm opacity-50" title="English">
<img alt="English" class="rounded" height="18" src="/flags/gb.png" width="24"/>
</span>
</div>
</div>
</div>
</div>
<div class="mt-12 border-t border-white/10 pt-8 md:flex md:items-center md:justify-between">
<div class="flex gap-x-6 md:order-2">
<a class="text-gray-400 hover:text-gray-300" href="https://github.com">
<span class="sr-only">GitHub</span>
</a>
<a class="text-gray-400 hover:text-gray-300" href="https://x.com">
<span class="sr-only">X</span>
</a>
<a class="text-gray-400 hover:text-gray-300" href="https://youtube.com">
<span class="sr-only">YouTube</span>
</a>
</div>
<p class="mt-8 text-sm/6 text-gray-400 md:mt-0" style="text-align: center; animation: copyrightGlow 3s ease-in-out infinite;">© 2026 Interwork Corporation All rights reserved.</p>
</div>
</div>
<style>
    @keyframes copyrightGlow {
      0%, 100% {
        opacity: 0.6;
        text-shadow: 0 0 10px rgba(99, 102, 241, 0);
      }
      50% {
        opacity: 1;
        text-shadow: 0 0 20px rgba(99, 102, 241, 0.5), 0 0 30px rgba(99, 102, 241, 0.3);
      }
    }
  </style>
</footer>
<div class="pointer-events-none fixed inset-x-0 bottom-0 px-6 pb-6 z-50 dark" data-cookie-consent-banner="" id="cookie-consent-banner">
<div class="pointer-events-auto max-w-xl rounded-xl section-bg-light dark:section-bg-dark p-6 ring-1 shadow-lg ring-gray-900/10">
<p class="text-secondary text-sm/6"><strong class="text-heading text-md mb-4 font-semibold">Cookie Consent</strong><br/> We use cookies to enhance your browsing experience and analyze our traffic. See our <a class="font-semibold text-primary hover:text-primary-500" href="/en/privacy-policy/">privacy policy</a>.</p>
<div class="mt-4 flex items-center gap-x-3 flex-wrap">
<a aria-label="Accept All" class="btn-primary dark:btn-primary-dark px-3 py-2 text-sm not-prose group" data-cookie-consent="accept-all" href="#" target="_self">
      Accept All
      
      
    </a>
<a aria-label="Reject All" class="btn-secondary dark:btn-secondary-dark px-3 py-2 text-sm not-prose group" data-cookie-consent="accept-necessary" href="#" target="_self">
      Reject All
      
      
    </a>
<a aria-label="Cookie Settings" class="btn-text dark:btn-text-dark px-3 py-2 text-sm not-prose group" data-cookie-consent="settings" href="#" target="_self">
      Cookie Settings
      
      
    </a>
</div>
</div>
</div>
<div class="fixed inset-0 z-50 hidden dark" id="cookie-settings-modal">
<div class="absolute inset-0 bg-black bg-opacity-50" data-cookie-settings-close=""></div>
<div class="relative mx-auto max-w-xl p-4 sm:p-6 section-bg-light dark:section-bg-dark rounded-xl shadow-xl mt-20">
<div class="flex justify-between items-center mb-4">
<h2 class="text-heading text-xl font-bold">Cookie Settings</h2>
<button class="text-gray-400 hover:text-gray-500 dark:text-gray-300 dark:hover:text-white" data-cookie-settings-close="" type="button">
<span class="sr-only">Close</span>
<svg class="h-6 w-6" fill="none" stroke="currentColor" viewbox="0 0 24 24">
<path d="M6 18L18 6M6 6l12 12" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"></path>
</svg>
</button>
</div>
<div class="space-y-4">
<div class="border-gray-200 dark:border-gray-700 border-b pb-4">
<div class="flex items-center justify-between">
<div>
<h3 class="text-heading text-lg font-medium">Necessary Cookies</h3>
<p class="text-tertiary text-sm">These cookies are required for the website to function and cannot be disabled.</p>
</div>
<div class="ml-3 flex h-5 items-center">
<input checked="" class="h-4 w-4 rounded-xl border-gray-300 text-primary focus:ring-primary dark:border-gray-600 dark:bg-gray-700" disabled="" id="necessary-cookies" name="necessary-cookies" type="checkbox"/>
</div>
</div>
</div>
<div class="border-gray-200 dark:border-gray-700 border-b pb-4">
<div class="flex items-center justify-between">
<div>
<h3 class="text-heading text-lg font-medium">Analytics Cookies</h3>
<p class="text-tertiary text-sm">These cookies help us understand how visitors interact with our website.</p>
</div>
<div class="ml-3 flex h-5 items-center">
<input class="h-4 w-4 rounded-xl border-gray-300 text-primary focus:ring-primary dark:border-gray-600 dark:bg-gray-700" id="analytics-cookies" name="analytics-cookies" type="checkbox"/>
</div>
</div>
</div>
</div>
<div class="mt-6 flex justify-end gap-x-3">
<a aria-label="Cancel" class="btn-secondary dark:btn-secondary-dark px-3 py-2 not-prose group" data-cookie-settings-close="" href="#" target="_self">
      Cancel
      
      
    </a>
<a aria-label="Save Preferences" class="btn-primary dark:btn-primary-dark px-3 py-2 not-prose group" data-cookie-settings-save="" href="#" target="_self">
      Save Preferences
      
      
    </a>
</div>
</div>
</div>
<button aria-label="Back to Top" class="fixed bottom-8 right-8 z-[100] p-3 rounded-full bg-indigo-600 text-white shadow-lg transition-all duration-300 transform translate-y-12 opacity-0 invisible hover:bg-indigo-700 hover:shadow-xl focus:outline-none focus:ring-2 focus:ring-indigo-500 focus:ring-offset-2 dark:bg-indigo-500 dark:hover:bg-indigo-400" id="back-to-top-btn" onclick="window.scrollTo({top: 0, behavior: 'smooth'});">
<svg class="h-6 w-6" fill="none" stroke="currentColor" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
<path d="M5 10l7-7m0 0l7 7m-7-7v18" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"></path>
</svg>
</button>
<script>
document.addEventListener('DOMContentLoaded', () => {
  const backToTopBtn = document.getElementById('back-to-top-btn');
  if (!backToTopBtn) return;

  const scrollThreshold = 300;

  const toggleVisibility = () => {
    if (window.scrollY > scrollThreshold) {
      backToTopBtn.classList.remove('translate-y-12', 'opacity-0', 'invisible');
      backToTopBtn.classList.add('translate-y-0', 'opacity-100', 'visible');
    } else {
      backToTopBtn.classList.remove('translate-y-0', 'opacity-100', 'visible');
      backToTopBtn.classList.add('translate-y-12', 'opacity-0', 'invisible');
    }
  };

  let ticking = false;
  window.addEventListener('scroll', () => {
    if (!ticking) {
      window.requestAnimationFrame(() => {
        toggleVisibility();
        ticking = false;
      });
      ticking = true;
    }
  });
});
</script>
<script src="/js/app.js?v=20260111190821"></script>
</body>
</html>