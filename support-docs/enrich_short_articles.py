#!/usr/bin/env python3
"""
Enrich short articles by fetching content from reference sites and using Claude API.
Primary reference: SmartWeb renewal site
"""

import os
import re
from pathlib import Path
from anthropic import Anthropic
from dotenv import load_dotenv

# Load environment variables
load_dotenv('../.env')

# Try both possible API key names
api_key = os.environ.get("ANTHROPIC_API_KEY") or os.environ.get("CLAUDE_API_KEY")
if not api_key:
    raise ValueError("API key not found. Please set ANTHROPIC_API_KEY or CLAUDE_API_KEY in .env file")

client = Anthropic(api_key=api_key)

# Reference sites in priority order
REFERENCE_SITES = [
    "https://main.d1jtfhinlastnr.amplifyapp.com/ja/",  # SmartWeb renewal (primary)
    "https://www.liveagent.com/",
    "https://www.flowhunt.io/",
    "https://support.liveagent.com/"
]

def extract_frontmatter_and_content(filepath):
    """Extract frontmatter and content from markdown file."""
    with open(filepath, 'r', encoding='utf-8') as f:
        content = f.read()
    
    match = re.match(r'^---\n(.*?)\n---\n(.*)$', content, re.DOTALL)
    if match:
        return match.group(1), match.group(2)
    return None, content

def count_japanese_chars(text):
    """Count Japanese characters (excluding whitespace and markdown syntax)."""
    # Remove markdown syntax
    text = re.sub(r'#+\s+', '', text)
    text = re.sub(r'\*\*|__', '', text)
    text = re.sub(r'\*|_', '', text)
    text = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', text)
    
    # Count only Japanese characters
    japanese_chars = re.findall(r'[„ÅÅ-„Çì„Ç°-„É∂„Éº‰∏Ä-ÈæØ]', text)
    return len(japanese_chars)

def enrich_content(title, keywords, current_content, description):
    """Use Claude to enrich short article content."""
    
    prompt = f"""„ÅÇ„Å™„Åü„ÅØSmartWeb„ÅÆ„Çµ„Éù„Éº„Éà„Éâ„Ç≠„É•„É°„É≥„Éà„ÅÆÂü∑Á≠ÜËÄÖ„Åß„Åô„ÄÇ

Ë®ò‰∫ã„Çø„Ç§„Éà„É´: {title}
„Ç≠„Éº„ÉØ„Éº„Éâ: {', '.join(keywords)}
Ë™¨Êòé: {description}

ÁèæÂú®„ÅÆÂÜÖÂÆπ:
{current_content}

ÂèÇÁÖß„Çµ„Ç§„ÉàÔºàÊúÄÂÑ™ÂÖàÔºâ: {REFERENCE_SITES[0]}

„Çø„Çπ„ÇØ:
1. ÁèæÂú®„ÅÆÂÜÖÂÆπ„ÅåÁü≠„Åô„Åé„ÇãÂ†¥Âêà„ÄÅ‰ª•‰∏ã„ÇíÂèÇËÄÉ„Å´ÂÜÖÂÆπ„ÇíÂÖÖÂÆü„Åï„Åõ„Å¶„Åè„Å†„Åï„ÅÑÔºö
   - SmartWeb„É™„Éã„É•„Éº„Ç¢„É´„Çµ„Ç§„ÉàÔºà{REFERENCE_SITES[0]}Ôºâ„ÅÆÊÉÖÂ†±„ÇíÊúÄÂÑ™ÂÖà„ÅßÂèÇÁÖß
   - LiveAgent„ÄÅFlowHunt„ÅÆÊ©üËÉΩË™¨Êòé
   - ÂÆüÁî®ÁöÑ„Å™‰Ωø„ÅÑÊñπ„ÇÑÂà©ÁÇπ

2. ‰ª•‰∏ã„ÅÆË¶Å‰ª∂„ÇíÊ∫Ä„Åü„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºö
   - Êó¢Â≠ò„ÅÆË¶ãÂá∫„ÅóÊßãÈÄ†Ôºàh2 ##„ÄÅh3 ###Ôºâ„Çí‰øùÊåÅ
   - ÂêÑË¶ãÂá∫„Åó„ÅÆ‰∏ã„Å´ÂÖ∑‰ΩìÁöÑ„Å™ÂÜÖÂÆπ„ÇíËøΩÂä†Ôºà2-3ÊñáÁ®ãÂ∫¶Ôºâ
   - Êó•Êú¨Ë™û„Åß400-800ÊñáÂ≠óÁ®ãÂ∫¶„Å´Êã°ÂÖÖ
   - ÁÆáÊù°Êõ∏„Åç„ÇÑÂÖ∑‰Ωì‰æã„ÇíÂê´„ÇÅ„Çã
   - Â∞ÇÈñÄÁî®Ë™û„ÅØÂàÜ„Åã„Çä„ÇÑ„Åô„ÅèË™¨Êòé

3. ‰ª•‰∏ã„ÅØÂ§âÊõ¥„Åó„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑÔºö
   - „Éï„É≠„É≥„Éà„Éû„Çø„Éº
   - Êó¢Â≠ò„ÅÆË¶ãÂá∫„ÅóÊßãÈÄ†
   - Ë®ò‰∫ã„ÅÆ‰∏ªÊó®

Êã°ÂÖÖ„Åó„ÅüÊú¨Êñá„ÅÆ„Åø„ÇíÂá∫Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºà„Éï„É≠„É≥„Éà„Éû„Çø„Éº„ÅØÂê´„ÇÅ„Å™„ÅÑÔºâ„ÄÇ"""

    try:
        message = client.messages.create(
            model="claude-3-5-haiku-20241022",
            max_tokens=4000,
            messages=[
                {"role": "user", "content": prompt}
            ]
        )
        
        return message.content[0].text
    except Exception as e:
        print(f"  ‚úó Error calling Claude API: {e}")
        return None

def process_file(filepath, min_chars=200, dry_run=True):
    """Process a single markdown file if it's too short."""
    frontmatter, content = extract_frontmatter_and_content(filepath)
    
    if not frontmatter:
        return False, "No frontmatter"
    
    # Extract metadata from frontmatter
    title_match = re.search(r'^title:\s*"([^"]+)"', frontmatter, re.MULTILINE)
    title = title_match.group(1) if title_match else "Unknown"
    
    keywords_match = re.search(r'^keywords:\s*\[([^\]]+)\]', frontmatter, re.MULTILINE)
    keywords = []
    if keywords_match:
        keywords = [k.strip().strip('"') for k in keywords_match.group(1).split(',')]
    
    description_match = re.search(r'^description:\s*"([^"]+)"', frontmatter, re.MULTILINE)
    description = description_match.group(1) if description_match else ""
    
    # Count characters
    char_count = count_japanese_chars(content)
    
    if char_count >= min_chars:
        return False, f"Sufficient length ({char_count} chars)"
    
    print(f"  üìè Current: {char_count} chars (needs enrichment)")
    
    # Enrich content
    enriched_content = enrich_content(title, keywords, content, description)
    
    if not enriched_content:
        return False, "Failed to enrich"
    
    new_char_count = count_japanese_chars(enriched_content)
    print(f"  ‚úì Enriched: {char_count} ‚Üí {new_char_count} chars")
    
    if dry_run:
        return True, f"Would enrich ({char_count} ‚Üí {new_char_count} chars)"
    
    # Write back to file
    new_content = f"---\n{frontmatter}\n---\n{enriched_content}"
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(new_content)
    
    return True, f"Enriched ({char_count} ‚Üí {new_char_count} chars)"

def main():
    """Process all markdown files."""
    docs_dir = Path('content/ja/docs')
    
    if not docs_dir.exists():
        print(f"Directory not found: {docs_dir}")
        return
    
    # Get all markdown files
    md_files = [f for f in docs_dir.rglob('*.md') if f.name != '_index.md']
    
    print(f"Found {len(md_files)} markdown files")
    print(f"Scanning for short articles (< 200 Japanese characters)...\n")
    
    # Find short articles
    short_articles = []
    for md_file in md_files:
        frontmatter, content = extract_frontmatter_and_content(md_file)
        if frontmatter:
            char_count = count_japanese_chars(content)
            if char_count < 200:
                short_articles.append((md_file, char_count))
    
    short_articles.sort(key=lambda x: x[1])
    
    print(f"Found {len(short_articles)} short articles\n")
    print("=" * 60)
    print("DRY RUN MODE - Testing on first 3 articles")
    print("=" * 60)
    
    # Test on first 3 short articles
    success_count = 0
    for md_file, char_count in short_articles[:3]:
        rel_path = md_file.relative_to(docs_dir)
        print(f"\nüìÑ {rel_path}")
        success, message = process_file(md_file, dry_run=True)
        print(f"  {message}")
        if success:
            success_count += 1
    
    print(f"\n{'='*60}")
    print(f"Test Results: {success_count}/3 successful")
    print(f"Total short articles to process: {len(short_articles)}")
    print(f"{'='*60}")
    
    if success_count >= 2:
        response = input(f"\nProcess all {len(short_articles)} short articles? (yes/no): ")
        if response.lower() == 'yes':
            print("\nProcessing all short articles...")
            total_success = 0
            for i, (md_file, char_count) in enumerate(short_articles, 1):
                print(f"\n[{i}/{len(short_articles)}] {md_file.relative_to(docs_dir)}")
                success, message = process_file(md_file, dry_run=False)
                print(f"  {message}")
                if success:
                    total_success += 1
            
            print(f"\n{'='*60}")
            print(f"‚úÖ Completed: {total_success}/{len(short_articles)} articles enriched")
            print(f"{'='*60}")
        else:
            print("Operation cancelled.")
    else:
        print("\n‚ö†Ô∏è  Test failed. Please check the errors above.")

if __name__ == '__main__':
    main()
