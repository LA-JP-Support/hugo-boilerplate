---
title: "Chroma"
date: 2025-11-25
lastmod: 2025-12-05
translationKey: "chroma"
description: "Explore Chroma, an open-source vector database for AI-native applications. Learn about its core concepts, architecture, use cases like RAG and semantic search, and compare it with alternatives."
keywords: ["Chroma", "vector database", "embeddings", "AI-native applications", "semantic search"]
category: "General"
type: "glossary"
draft: false
---
## Definition: What is Chroma?

**Chroma** is an open-source vector (embedding) database engineered for AI-native applications, particularly those using large language models (LLMs) and multimodal AI. Unlike traditional databases, Chroma specializes in storing, indexing, and retrieving high-dimensional vector embeddings—numerical representations of text, images, and other unstructured data.

Chroma’s core mission is to make it easy for developers and organizations to add semantic search, recommendation, RAG, and AI-native capabilities to their applications, with minimal setup and maximum flexibility. It provides:

- Native support for storing and searching embeddings alongside documents and metadata
- Fast approximate nearest neighbor (ANN) search via [HNSW](/en/glossary/hnsw--hierarchical-navigable-small-world-/) indexing
- Multimodal support (text, images, and more)
- Hybrid queries: semantic + keyword search, plus metadata filtering
- Developer-friendly APIs (Python, JS), and native integrations with frameworks like LangChain and LlamaIndex
- Open-source Apache 2.0 licensing
- Both self-hosted and managed cloud options
## Core Concepts

### Embeddings

**Embeddings** are dense vectors that encode the semantic meaning of data. For example, a sentence, image, or audio clip can be transformed into a vector of hundreds or thousands of numbers. Similar data points in meaning will have similar embeddings (i.e., be “close” in vector space), even if the raw data is very different.

Chroma supports embeddings generated by popular models, including:
- OpenAI’s text-embedding models (e.g., `text-embedding-3-small`)
- HuggingFace models (e.g., `all-MiniLM-L6-v2`)
- Cohere, OpenCLIP, and custom embeddings

This is foundational for semantic search, recommendations, and retrieval-augmented generation.
### Collections

A **collection** in Chroma is a logical grouping of documents, embeddings, and their associated metadata. Each collection has its own configuration, including:
- Embedding function/model
- Storage location (in-memory or persistent)
- Optional custom settings for performance or filtering

This allows separate AI applications or projects to run side-by-side, each tuned for its specific needs.
### Metadata and Hybrid Search

Chroma allows arbitrary key-value metadata to be associated with each document or vector. This enables **hybrid search**: filtering results by metadata (e.g., author, date, tags) and ranking them by vector similarity.

Operators include:
- Equality and inequality
- Range queries (`$gt`, `$lt`)
- Set membership (`$in`)
- Logical combinations (`$and`, `$or`)
## How Chroma Works

### Vector Indexing & Similarity Search

Chroma uses **Hierarchical Navigable Small World (HNSW)** graphs for fast, approximate nearest neighbor (ANN) search. HNSW is a state-of-the-art algorithm for high-dimensional vector similarity search, balancing recall (accuracy) and speed, and scaling to millions of vectors.

Key properties:
- Sublinear search time for large datasets
- High recall/accuracy (configurable)
- Supports dynamic inserts and efficient deletion
### Document and Metadata Storage

Each entry in Chroma includes:
- The raw document/content (text, image URI, etc.)
- The vector embedding
- Associated metadata (arbitrary key-value JSON)

This enables hybrid queries and full semantic search.

Chroma can store data:
- In-memory (fastest, non-persistent)
- On disk (SQLite for metadata, binary files for vectors)
- In Chroma Cloud (fully managed)
### APIs and Client Libraries

Chroma provides a minimal, intuitive API with four main operations:
- **Add:** Insert documents (optionally with embeddings and metadata)
- **Update:** Modify stored entries
- **Delete:** Remove entries
- **Query:** Retrieve similar documents via vector search, with optional metadata filters

Client libraries exist for:
- **Python:** [chromadb](https://pypi.org/project/chromadb/)
- **JavaScript/TypeScript:** [Chroma Node.js Client](https://github.com/chroma-core/chroma/tree/main/clients/js)

Chroma integrates natively with frameworks like:
- **LangChain** ([integration guide](https://docs.langchain.com/oss/python/integrations/providers/chroma))
- **LlamaIndex**
## Architecture & Deployment

### Open-Source (Self-Hosted)

Chroma can be run locally or on your own infrastructure in three modes:
- **In-memory:** Fast, ephemeral, ideal for prototyping or testing
- **Persistent:** Stores data on disk (SQLite + binary vector files), suitable for local/small production
- **Client-server:** Run as a standalone server, connect via HTTP API (supports multi-user, multi-process)

**Example Server Start:**
```shell
chroma run --path ./db --port 8000
```
**Python Client:**
```python
import chromadb
client = chromadb.HttpClient(host="localhost", port=8000)
```
### Chroma Cloud (Serverless)

**Chroma Cloud** is a fully managed, serverless deployment. It handles:
- Elastic scaling
- Automatic backup & [high availability](/en/glossary/high-availability--ha-/)
- Maintenance and monitoring

**Connect Example:**
```python
import chromadb
client = chromadb.HttpClient(
    host="api.trychroma.com",
    headers={"Authorization": f"Bearer {CHROMA_API_KEY}"}
)
```
## Setup and Integration

### Installation

**Python:**
```bash
pip install chromadb
```
For LangChain integration:
```bash
pip install langchain-chroma
```
### Basic Usage Example

```python
import chromadb

client = chromadb.Client()
collection = client.create_collection("documents")

collection.add(
    documents=[
        "Artificial intelligence is transforming healthcare diagnostics",
        "Machine learning models predict patient outcomes with increasing accuracy",
        "Neural networks analyze medical imaging faster than radiologists"
    ],
    ids=["doc1", "doc2", "doc3"]
)

results = collection.query(
    query_texts=["AI applications in medicine"],
    n_results=2
)

print(results)
```
This code creates a collection, inserts documents, and runs a semantic search query.
### Embedding Function Configuration

Chroma collections can use different embedding models. To use OpenAI embeddings:
```python
from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction

openai_ef = OpenAIEmbeddingFunction(
    api_key="your-api-key",
    model_name="text-embedding-3-small"
)

collection = client.create_collection(
    name="openai_embeddings",
    embedding_function=openai_ef
)
```
### LangChain Integration

LangChain provides a native wrapper for Chroma, supporting advanced workflows like RAG, chatbots, and memory.

**Example:**
```python
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
vector_store = Chroma(
    collection_name="example_collection",
    embedding_function=embeddings,
    persist_directory="./chroma_langchain_db", # optional
)
```
## Core Features

- **Open-source Apache 2.0**: No lock-in, extensible, community-driven ([GitHub Stars 24k+](https://github.com/chroma-core/chroma))
- **Fast ANN search**: HNSW graph indexing for sublinear search time
- **Document & metadata storage**: Each embedding relates to a document and user-defined metadata
- **Hybrid search**: Combine semantic (vector) and keyword search
- **Multimodal support**: Store/search text, images, and more
- **Batch ops**: Bulk insert and query for efficiency
- **Simple API**: Add, update, delete, search
- **Integration**: Native with LangChain, LlamaIndex, OpenAI, HuggingFace, Cohere, OpenCLIP, and more
- **Flexible deployment**: In-memory, persistent, client-server, and managed cloud
- **Active community**: Discord, GitHub, docs
## Key Use Cases

### Semantic Search

Chroma powers semantic search by comparing embeddings, not just keywords.
- **E-commerce**: Search for “comfortable summer shoes” returns relevant results, even with different wording.
- **Knowledge Management**: Search across internal wikis, support tickets, codebases.
- **Healthcare**: Find similar cases, research, or diagnostic images.
### Recommendation Systems

Find similar items/users by embedding similarity:
- Personalized product/news/article recommendations
- Item-to-item or user-to-user matching

### Retrieval-Augmented Generation (RAG)

**RAG** allows LLMs to access external knowledge bases in real time, improving accuracy and reducing hallucinations.
- Chatbots cite specific documents
- Assistants answer with up-to-date company knowledge
### Image, Audio, and Multimodal Search

Embed images, text, and more into a shared vector space.
- Visual search (find similar images)
- Cross-modal (search images with text, vice versa)
- Organize multimedia datasets

### Chatbots and AI Applications

Chroma acts as persistent, semantic memory for LLMs and chatbots.
- Retrieve conversation history or relevant knowledge snippets
- Power context-aware responses

### Data Science and Analysis

- Exploratory data analysis on high-dimensional data
- Anomaly detection in financial/security logs
- Build knowledge graphs, semantic maps
## Performance Optimization

Chroma is designed for developer speed and efficiency, but optimization tips include:

- **Batch operations**: Insert/query in bulk to reduce overhead
- **Embedding dimensionality**: Lower-dimension vectors use less memory, faster search (at possible cost to accuracy)
- **Index compaction**: Compact HNSW index after frequent deletes/updates
- **Metadata pre-filtering**: Filter by metadata before similarity to reduce computation

**Example:**
```python
collection.add(
    documents=large_document_list,
    ids=id_list,
    metadatas=metadata_list
)
```
## Comparisons and Alternatives

### Chroma vs. Pinecone, Faiss, Weaviate, Qdrant, Milvus, and Others

**Ecosystem snapshot:**
- **Chroma**: OSS, easy setup, hybrid search, best for prototyping/dev velocity
- **Pinecone**: Managed, distributed, enterprise-grade, multi-index support, high scale
- **Faiss**: OSS, research/ML focus, C++/Python, not a database (no doc/meta storage)
- **Weaviate**: OSS, distributed, hybrid search, schema, multi-tenant
- **Qdrant**: OSS, distributed, filtering, REST/gRPC, high perf
- **Milvus**: OSS, cloud-native, GPU support, very high scale
### Comparison Table

| Feature                | **Chroma**       | **Pinecone**    | **Faiss**         | **Weaviate**     | **Qdrant**    | **Milvus**    |
|------------------------|------------------|-----------------|-------------------|------------------|--------------|--------------|
| Open-source            | ✅               | ❌              | ✅                | ✅ (open-core)   | ✅           | ✅           |
| Ease of setup          | Very simple      | Managed, easy   | Complex           | Moderate         | Moderate     | Moderate     |
| Language support       | Python, JS       | Python, JS, Go  | Python, C++       | Python, JS, Go   | Python, REST | Python, REST |
| Vector indexing        | HNSW             | Multiple        | Multiple          | HNSW, others     | HNSW         | IVF, HNSW    |
| Document storage       | Built-in         | No              | No                | Built-in         | Built-in     | Built-in     |
| Metadata filtering     | Yes              | Yes             | Limited           | Yes              | Yes          | Yes          |
| Hybrid search          | Yes              | No              | No                | Yes              | No           | No           |
| Cloud/serverless       | Chroma Cloud     | Yes             | No                | Yes              | Yes          | Yes          |
| RBAC/[Multi-tenancy](/en/glossary/multi-tenancy/)     | No               | Yes             | No                | Yes              | Yes          | Yes          |
| Scale                  | Single-node      | Distributed     | Local, dist.      | Distributed      | Distributed  | Distributed  |
| Community/support      | Active, OSS      | Commercial      | Large OSS         | Commercial + OSS | Active OSS   | Active OSS   |
| Best for               | Dev velocity, prototyping | Large scale | Research, custom ML |
