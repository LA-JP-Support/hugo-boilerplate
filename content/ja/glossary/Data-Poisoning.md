---
title: データポイズニング
date: 2025-11-25
translationKey: data-poisoning
description: データポイズニングとは、AI/ML学習データセットに破損したデータを注入し、モデルの動作を操作したり、パフォーマンスを低下させたり、隠れた脆弱性を埋め込んだりする悪意のある攻撃です。
keywords: ["データポイズニング", "AIセキュリティ", "機械学習", "敵対的攻撃", "モデルの整合性"]
category: AI Ethics & Safety Mechanisms
type: glossary
draft: false
e-title: Data Poisoning
term: データポイズニング
reading: データポイズニング
kana_head: た
---
## データポイズニングとは?

**データポイズニング**とは、機械学習(ML)または人工知能(AI)モデルのトレーニングに使用されるデータセットに対して、意図的にデータを挿入、変更、または削除し、結果として得られるモデルの動作を破壊または操作することを目的とした行為です。これらの攻撃は、微妙な脆弱性の導入、出力のバイアス化、パフォーマンスの低下、または特定の条件下で発動する隠れた動作(バックドア)の埋め込みを引き起こす可能性があります。

データポイズニング攻撃は、わずかな汚染(トレーニングデータの0.001%程度)でもモデル精度を最大30%低下させることが示されており、安全性が重要なシステムにおいて決定境界を歪める可能性があります([Hartle et al., 2025](https://iacis.org/iis/2025/4_iis_2025_433-442.pdf))。攻撃者は、このような攻撃を利用してスパイ活動を可能にし、金銭的損失を引き起こし、またはAIシステムに対する公共の信頼を損なう可能性があります。

> 包括的な技術的入門については、[CrowdStrikeのデータポイズニング解説](https://www.crowdstrike.com/en-us/cybersecurity-101/cyberattacks/data-poisoning/)および[Lakeraのデータポイズニングブログ](https://www.lakera.ai/blog/training-data-poisoning)を参照してください。

## 背景:AIの倫理と安全性におけるデータポイズニングの重要性

### データポイズニングリスクを高める主要トレンド

- **重要なAI採用:**AIは、金融、医療、防衛、重要インフラなど、モデルの完全性が最も重要な高リスク領域でますます使用されています。
- **信頼できないデータソース:**多くのMLモデルは、公開データ、Webスクレイピング、またはクラウドソーシングされたデータでトレーニングされており、意図的な操作への露出が高まっています([Nisos, 2024](https://nisos.com/research/building-trustworthy-ai/))。
- **複雑で動的なパイプライン:**頻繁なモデル更新、継続的学習、および検索拡張生成(RAG)により、汚染されたサンプルの取り込みポイントが繰り返し提供されます。
- **攻撃者の高度化の進展:**スクリプトキディから国家主体まで、攻撃者は分割ビューポイズニング、ステルストリガー、サプライチェーン攻撃を開発しています([West Point Lieber Institute](https://lieber.westpoint.edu/data-poisoning-covert-weapon-securing-us-military-superiority-ai-driven-warfare/))。

データポイズニングは、バイアスの導入、公平性の損ない、自動意思決定の信頼性低下による害を引き起こす可能性があるため、AIの倫理的使用に対する直接的な脅威です([Lakera, 2025 Perspective](https://www.lakera.ai/blog/training-data-poisoning))。

## データポイズニング攻撃の仕組み

### 攻撃ベクトルとライフサイクルステージ

データポイズニングは、機械学習パイプラインのあらゆるポイントを標的にすることができます:

| ステージ | ポイズニングベクトルの例 | 影響 |
|--------------------|---------------------------------------------------------------------|-------------------------------------------------------|
| 事前トレーニング | オープンソースデータセットまたはWebスクレイピングへの悪意のあるサンプルの挿入 | 体系的なバイアス、グローバルなモデルドリフト、永続的なバックドア |
| ファインチューニング | 改ざんまたは誤ラベル付けされたドメイン固有のデータ、コードリポジトリ | 標的エラー、モデル固有のバックドア |
| 検索(RAG) | 外部ナレッジベースへの悪意のあるドキュメントの挿入 | 汚染された回答、ハルシネーション |
| 合成データ | 隠れたトリガーでシードされた生成データパイプライン | ポイズンの伝播、世代間汚染 |
| モデルサプライチェーン | 公開リポジトリにアップロードされた悪意を持ってトレーニングされたモデル | ダウンストリームの侵害、サプライチェーンリスク |

([Palo Alto Networks](https://www.paloaltonetworks.com/cyberpedia/what-is-data-poisoning), [Hartle et al., 2025](https://iacis.org/iis/2025/4_iis_2025_433-442.pdf))

#### 攻撃手法

- **インジェクション:**新しい攻撃者作成のデータポイントの導入(例:偽レビュー、改変されたコード)
- **変更:**既存のレコードの微妙な編集によるバイアスまたはトリガーの導入
- **ラベルフリッピング:**教師ありデータセットのラベル変更による誤分類の誘発([Ndanusa et al., 2025](https://arxiv.org/pdf/2503.09302))
- **バックドア埋め込み:**トリガーで悪意のある動作を発動する隠れた信号の植え付け
- **削除:**エッジケースまたは重要なデータの削除によるレアシナリオでのエラー率の増加

### 攻撃者の動機と脅威アクター

- **内部者:**直接アクセス権を持つ内部者(エンジニア、データサイエンティスト)は、ステルスで標的を絞った攻撃を実行できます。
- **外部攻撃者:**攻撃者は、公開データソース、オープンリポジトリ、または連合学習ノードを標的にする可能性があります。
- **サプライチェーン攻撃者:**信頼されたプラットフォーム(例:Hugging Face、GitHub)を介して配布される汚染されたモデルまたはデータセット。
- **国家および軍事アクター:**国家主体の作戦は、戦略的混乱またはインテリジェンスのためにデータポイズニングを使用する可能性があります([Lieber Institute](https://lieber.westpoint.edu/data-poisoning-covert-weapon-securing-us-military-superiority-ai-driven-warfare/))。

## データポイズニング攻撃の種類

データポイズニング攻撃は、攻撃者の意図、方法、およびステルスレベルによって分類されます。

### 攻撃分類表

| 攻撃タイプ | 説明 | シナリオ例 | ステルス性 |
|------------------------|-----------------------------------------------------------------------------------------|--------------------------------------------------|---------|
| **ラベルフリッピング** | トレーニングサンプルのラベルを変更して誤分類を誘発 | メールフィルタリングにおけるスパム/ハムの反転 | 中程度 |
| **ポイズンインジェクション** | ラベルの有無にかかわらず、作成されたデータポイントを追加 | 偽レビュー、ボット生成コンテンツ | 低〜中 |
| **データ変更** | 既存データの特徴を編集してバイアスまたはトリガーを導入 | 改ざんされた医療記録、コードベースの変更 | 高 |
| **バックドア/トリガー型** | 特定の条件下で悪意のある動作を発動する隠れたパターンの埋め込み | 秘密のフレーズトリガー、画像透かし | 非常に高 |
| **クリーンラベル** | 有効に見え、正しいラベルを持つ汚染されたサンプル | ステルス画像摂動 | 高 |
| **ダーティラベル** | 意図的に誤ったラベルを持つ汚染されたサンプル | 入れ替えられた画像キャプションペア | 中程度 |
| **分割ビュー/ゆでガエル** | 検出を回避するためにトレーニングサイクル全体で段階的にポイズニング | ニュースコーパスへの緩やかなバイアス注入 | 非常に高 |
| **直接/間接** | 直接:トレーニングパイプライン内;間接:公開データを介した上流 | データセットにスクレイピングされた偽Webページ | 可変 |

([Nisos, 2024](https://nisos.com/research/building-trustworthy-ai/), [Hartle et al., 2025](https://iacis.org/iis/2025/4_iis_2025_433-442.pdf))

## 症状と検出

### データポイズニングの一般的な兆候

- **モデル精度の低下:**精度、適合率、または再現率の突然または説明のつかない低下。
- **予期しない出力:**異常、不規則、または文脈的に不合理な予測。
- **バイアス/有害性:**人口統計学的またはトピック的なバイアス、または攻撃的なコンテンツの出現。
- **バックドア発動:**レアなトリガーが存在する場合を除き、通常の動作。
- **モデルドリフト:**出力分布のシフト、特にエッジケースまたはカナリアケースで。

検出の課題は、攻撃者によるステルス、クリーンラベル、または段階的に導入された汚染データの使用に起因します。高度な検出には、統計的異常検出、敵対的プローブ、および継続的な監視が必要です([Ndanusa et al., 2025](https://arxiv.org/pdf/2503.09302))。

#### 診断表

| **症状** | **診断質問** |
|----------------------------|--------------------------------------------------------------------------------------------------|
| モデルの劣化 | 明確な原因なしにモデルのパフォーマンスが低下しましたか? |
| 意図しない出力 | 説明のつかない、または不規則な予測がありますか? |
| 偽陽性/偽陰性の急増 | 誤分類またはエラー率の増加がありますか? |
| バイアスのある結果 | 出力に予期しない人口統計学的またはトピック的なバイアスが見られますか? |
| バックドアトリガー | モデルは特定のレアな入力に対して異常に反応しますか? |
| セキュリティイベント | 最近、データ/モデルリソースへの侵害または異常なアクセスがありましたか? |
| 疑わしい内部者活動 | トレーニングデータまたはAIセキュリティ対策に異常な関心を示した従業員はいますか? |

([CrowdStrike](https://www.crowdstrike.com/en-us/cybersecurity-101/cyberattacks/data-poisoning/))

## 実世界のインシデントと研究

### 文書化されたケース

- **Basilisk Venom (2025):**  
  GitHubコードコメント内の隠されたプロンプトがファインチューニングされたLLMを汚染しました。特定のフレーズが現れると、モデルはトレーニングから数か月後、オフラインでも攻撃者の指示を実行しました([Lakera](https://www.lakera.ai/blog/training-data-poisoning), [Odin AI](https://0din.ai/blog/poison-in-the-pipeline-liberating-models-with-basilisk-venom))。
- **Qwen 2.5 Jailbreak (2025):**  
  インターネット全体にシードされた悪意のあるWebテキストにより、LLMが作成されたクエリで露骨なコンテンツを出力し、RAGを介したポイズニングを実証しました([The Stack](https://www.thestack.technology/ai-agent-whisperer-liberates-llm-to-spout-filthy-cardy-b-lyrics))。
- **Virus Infection Attack (2025):**  
  汚染された合成データがモデルの世代を通じて伝播し、初期のポイズニングを増幅しました([arXiv:2509.23041v1](https://arxiv.org/html/2509.23041v1))。
- **ConfusedPilot (2024):**  
  Microsoft 365 CopilotのRAG参照ドキュメント内の悪意のあるデータは、削除後もハルシネーションされた汚染された結果を持続させました([Infosecurity Magazine](https://www.infosecurity-magazine.com/news/confusedpilot-attack-targets-ai/))。
- **MITRE ATLAS: Tayケース:**  
  MicrosoftのTayチャットボットは、会話トレーニングの敵対的ポイズニング後に攻撃的な出力を生成しました([MITRE ATLAS](https://atlas.mitre.org/studies/AML.CS0009/))。
- **Hugging Faceサプライチェーン脅威 (2024):**  
  攻撃者は汚染されたデータセットでトレーニングされたモデルを公開リポジトリにアップロードし、ダウンストリームの消費者を脅かしました([Wiz Blog](https://www.wiz.io/blog/wiz-and-hugging-face-address-risks-to-ai-infrastructure))。
- **PoisonBench (2024):**  
  ポイズニングに対するモデルの感受性をベンチマーク化;大規模モデルは本質的に耐性があるわけではなく、攻撃は未知のトリガーに一般化します([PoisonBench arXiv](https://ar5iv.labs.arxiv.org/html/2410.08811v2))。

#### 主要研究

- **2018〜2025年の体系的レビュー:**  
  最小限の敵対的摂動(汚染データの0.001%程度)でも、精度を最大30%低下させ、安全性が重要なシステムで境界を歪め、永続的なバックドアを可能にします([Hartle et al., 2025](https://iacis.org/iis/2025/4_iis_2025_433-442.pdf))。
- **検出と予防:**  
  統計的異常検出、ロバスト最適化、敵対的トレーニング、およびアンサンブル手法は、総合的にモデルの回復力を向上させます。アンサンブルアプローチは、敵対的データからの偽陽性/偽陰性を削減します([Ndanusa et al., 2025](https://arxiv.org/pdf/2503.09302))。
- **医療への影響:**  
  誤情報を含む0.001%のトークンのポイズニングにより、医療LLMにおける有害な完了が7〜11%増加しました—標準ベンチマークでは検出されませんでした([Nature Medicine, 2024](https://www.nature.com/articles/s41591-024-03445-1))。
- **サイレントブランディングとコントロールの喪失:**  
  汚染された画像生成モデルは、テキストの手がかりがなくても、微妙なトリガーでロゴまたはNSFWコンテンツを再現します([Silent Branding](https://arxiv.org/abs/2503.09669), [Losing Control](https://arxiv.org/abs/2507.04726))。

## 結果とリスク

### ビジネスおよび安全性への影響表

| 影響領域 | 結果例 | リスクレベル |
|------------------------|---------------------------------------------------------------------|-----------------|
| セキュリティ | バックドアトリガーにより認証バイパスまたはデータ流出が可能に | クリティカル |
| 安全性が重要なシステム | 自動運転車が標識/物体を誤分類し、衝突のリスク | クリティカル |
| 医療 | バイアスのある医療LLMが安全でない治療を推奨 | 高 |
| 金融 | 不正検出モデルが犯罪パターンを見逃す | 高 |
| 一般的なモデル品質 | 精度の低下、バイアスのある出力、信頼の喪失 | 深刻 |
| 規制コンプライアンス | 出力が法的/倫理的ガイドラインに違反 | 高 |
| サプライチェーン | 汚染されたオープンソースモデルがダウンストリームの消費者に影響 | 深刻 |

ポイズニングによる金銭的、評判的、および安全性の損害は、コストのかかる再トレーニング、インシデント対応、および規制上の是正を必要とする場合があります。影響は、侵害されたデータが削除された後も持続することがよくあります([Nisos, 2024](https://nisos.com/research/building-trustworthy-ai/))。

## 検出と予防のベストプラクティス

### 包括的な防御チェックリスト

#### データの出所と検証

- 信頼できるリポジトリからのみソースを取得;データの起源の詳細な記録を維持([OWASP LLM Top 10](https://genai.owasp.org/llmrisk/llm042025-data-and-model-poisoning/), [NIST AI RMF](https://www.nist.gov/itl/ai-risk-management-framework))
- 継続的なデータ検証:重複排除、品質チェック、および有害性、バイアス、または異常の自動フィルタリング
- 合成データ汚染の監視:汚染されたサンプルの伝播を追跡([Virus Infection Attack](https://arxiv.org/html/2509.23041v1))

#### アクセス制御と安全なデータ処理

- 最小権限アクセスを実施し、保存時および転送時のデータを暗号化
- 異常または不正なアクティビティについてアクセスログを監査

#### 監視と異常検出

- 説明のつかないドリフトまたはエラー率の急増についてモデルの動作を継続的に監視
- データ/モデル出力の外れ値にフラグを立てるために、統計的およびMLベースの異常検出を展開
- 標的攻撃を検出するために、カナリア/エッジケースでモデルのパフォーマンスをテスト

#### 敵対的テストとレッドチーム演習

- レッドチーム演習を使用してポイズニング攻撃をシミュレート([Lakera Red Teaming Playbook](https://www.lakera.ai/ai-security-guides/ai-red-teaming-insights-from-the-worlds-largest-red-team))
- バックドアトリガーとエッジケースの失敗をプローブ

#### データのバージョン管理とリカバリ

- 侵害後のロールバックを可能にするためにデータバージョン管理(DVC)を実装([OWASP](https://genai.owasp.org/llmrisk/llm042025-data-and-model-poisoning/))
- 検証とリカバリのためにクリーンな参照セットを維持

#### ランタイムガードレール

- 異常または非準拠のモデル動作を制限するために、出力監視とポリシーベースの制御を展開

#### ユーザー教育と意識向上

- ポイズニングの症状を認識し、疑わしいモデルの動作を報告するようにスタッフをトレーニング
- 明確なインシデント対応プロトコルを確立

#### サプライチェーンとインフラストラクチャのセキュリティ

- サードパーティのデータベンダーとオープンソースソースを審査
- 改ざんに対してモデルリポジトリとアーティファクトストレージを強化([JFrog Blog](https://jfrog.com/blog/data-scientists-targeted-by-malicious-hugging-face-ml-models-with-silent-backdoor/))
- 意図されたデータソースのみへのモデルアクセスを制限

#### 技術的予防メカニズム

- **敵対的トレーニング:**敵対的に生成されたサンプルでモデルをトレーニングして堅牢性を高める([Ndanusa et al., 2025](https://arxiv.org/pdf/2503.09302))
- **アンサンブル学習:**複数のモデルを使用し、出力を比較してポイズニングによる不一致を検出
- **データ出所追跡:**不変のデータ系統のためにブロックチェーンまたは暗号化手法を活用([Baracaldo et al., 2017](https://arxiv.org/abs/1706.08890))
- **定期的なベンチマーク:**敵対的および汚染データベンチマークを使用して回復力をテスト([PoisonBench arXiv](https://ar5iv.labs.arxiv.org/html/2410.08811v2))

## 参考資料

- [Building Trustworthy AI: Contending with Data Poisoning (Nisos, 2024)](https://nisos.com/research/building-trustworthy-ai/)
- [Data poisoning 2018–2025: A systematic review (Hartle et al., 2025)](https://iacis.org/iis/2025/4_iis_2025_433-442.pdf)
- [Detecting and Preventing Data Poisoning Attacks on AI Models (Ndanusa et al., 2025)](https://arxiv.org/pdf/2503.09302)
- [OWASP LLM Top 10: Data and Model Poisoning](https://genai.owasp.org/llmrisk/llm042025-data-and-model-poisoning/)
- [Lakera: Introduction to Data Poisoning](https://www.lakera.ai/blog/training-data-poisoning)
- [Palo Alto Networks: What Is Data Poisoning?](https://www.paloaltonetworks.com/cyberpedia/what-is-data-poisoning)