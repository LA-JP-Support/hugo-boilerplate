---
title: 教師なし学習
lastmod: '2025-12-19'
date: '2025-12-19'
translationKey: unsupervised-learning
description: 教師なし学習に関する包括的な用語集。定義、クラスタリング、アソシエーションルール、次元削減などのアルゴリズム、応用例、利点、課題について解説しています。
keywords:
- 教師なし学習
- クラスタリング
- 次元削減
- アソシエーションルール
- 機械学習
category: Machine Learning
type: glossary
draft: false
e-title: Unsupervised Learning
term: きょうしなしがくしゅう
url: "/ja/glossary/unsupervised-learning/"
---
## 教師なし学習とは何か?
教師なし学習は、アルゴリズムが明示的なラベルや事前定義された目標変数なしにデータを分析・整理する、機械学習の基本的なパラダイムです。入力を既知の出力にマッピングするラベル付き訓練例に依存する教師あり学習とは異なり、教師なし学習はラベルなしデータセット内の隠れたパターン、構造、関係性を自律的に発見します。モデルは独立してデータを探索し、正解についての人間の指導なしに、固有のグループ化、関連性、または簡略化された表現を特定します。

このアプローチは、人間が自然に情報を整理する方法を反映しています。パターンを認識し、類似したアイテムを分類し、明示的な指示なしに関係性を発見します。新しい都市を探索する際、私たちは直感的に特性によって地域をグループ化し、交通パターンを識別し、ラベル付きの地図なしに場所間の関係を発見します。同様に、教師なし学習アルゴリズムはデータ分布内の構造を検出し、人間の観察者にはすぐには明らかでない洞察を明らかにします。

教師なし学習の数学的基盤は、入力データの基礎となる確率分布のモデリングに集中しています。入力から出力へのマッピング関数を学習するのではなく、これらのアルゴリズムはデータ生成プロセス自体を理解しようと試み、データセットを特徴づける統計的規則性、依存関係、組織原理を明らかにします。この知識発見プロセスは、ラベル付きデータが利用できない場合、取得に法外なコストがかかる場合、または目標が特定の予測タスクではなく探索的分析である場合に非常に価値があります。

## 中核原理と方法論

### 基本的アプローチ

教師なし学習は入力特徴ベクトルのみで動作し、外部の監督なしにデータの本質的な構造を明らかにしようとします。アルゴリズムは、既知のラベルに対する予測精度ではなく、内部データ特性を測定する目的関数を最適化します。これには、発見されたグループ内の類似性、変数間の関連強度、または次元削減中の情報保存などが含まれます。

### 学習プロセス

**データ収集**カテゴリラベルや目標値を欠くデータセットを収集します。例として、セグメンテーションのない顧客取引履歴、トピックラベルのないテキスト文書、または異常アノテーションのないセンサー読み取り値などがあります。**アルゴリズム選択**分析目標に基づいて適切な教師なし技術を選択します:類似インスタンスをグループ化するためのクラスタリング、関係性を発見するための相関ルールマイニング、または簡略化と可視化のための次元削減。**パターン発見**アルゴリズムを実行して自然なデータ組織を特定します。クラスタリングアルゴリズムはデータを凝集性のあるグループに分割し、相関ルール学習者は共起パターンを抽出し、次元削減技術は本質的な情報を保持するコンパクトな表現を見つけます。**解釈と検証**ドメイン専門家が発見されたパターンを解釈し、その意味性と実行可能性を評価し、ビジネスロジック、外部データソース、または発見された構造を特徴として使用する後続の教師あり学習タスクを通じて発見を検証します。

### 教師なし学習を適用すべき場合

**探索的データ分析**新しいデータセットの理解、予期しないパターンの識別、外れ値の検出、さらなる調査のための仮説生成。**教師あり学習の前処理**次元削減による情報量の多い特徴の作成、階層化モデリングを導く自然なデータセグメンテーションの発見、または教師ありモデル訓練を改善する異常の検出と除去。**知識発見**科学データにおける隠れた関係の明示、ターゲットマーケティングのための顧客セグメントの発見、または医療記録からの疾患サブタイプの識別。**ラベルのない状況**ラベリングが実用的でない場合(大規模データセット)、高価な場合(専門家のアノテーションが必要)、主観的な場合(曖昧な正解)、または不可能な場合(人間が識別していない新しいパターンの発見)。

## 教師なし学習の主要カテゴリ

### 1. クラスタリングアルゴリズム

クラスタリングは、クラスタ内のインスタンスが高い類似性を示し、クラスタ間のインスタンスが大きく異なるグループにデータを分割します。この組織構造は、母集団構造の理解、ターゲット戦略の開発、パターン認識に有用な自然なデータセグメンテーションを明らかにします。

#### クラスタリング方法論

**重心ベースクラスタリング**アルゴリズムは、クラスタメンバーから重心までの距離を最小化する中心点の周りにデータを整理します。最も広く使用される重心ベース手法であるK-meansは、点を最も近い重心に割り当て、収束するまで重心を再計算することを反復的に行います。高速でスケーラブルなこれらのアプローチは、球形クラスタには適していますが、不規則な形状や密度の変動には苦労します。**密度ベースクラスタリング**手法は、疎な領域によって分離された密な領域としてクラスタを識別し、任意のクラスタ形状を自然に処理し、低密度点として外れ値を検出します。DBSCAN(Density-Based Spatial Clustering of Applications with Noise)は、点が密集している領域としてクラスタを定義し、クラスタ数を自動的に決定し、ノイズを識別します。OPTICSはこの概念を拡張し、階層的密度構造を明らかにします。**階層的クラスタリング**技術は、凝集的(ボトムアップ)または分割的(トップダウン)戦略を通じて、ネストされたクラスタ構造を構築します。凝集的手法は個々の点から始まり、望ましい粒度に達するまで類似したクラスタを段階的にマージします。分割的アプローチは、すべてのデータを1つのクラスタに入れることから始まり、目標構造を達成するまで再帰的に分割します。デンドログラムはこの階層を可視化し、複数の粒度レベルでの同時分析を可能にします。**分布ベースクラスタリング**確率モデルは、統計分布の混合から生成されたデータを仮定します。ガウス混合モデル(GMM)は、各クラスタをガウス分布として表現し、期待値最大化アルゴリズムを使用してパラメータを推定します。これらのソフトクラスタリング手法は、ハード割り当てではなくメンバーシップ確率を割り当て、クラスタ所属の不確実性を捉えます。**ファジィクラスタリング**部分的なクラスタメンバーシップを許可する拡張。ファジィC-Meansは、各点に合計が1になる複数のクラスタにわたるメンバーシップ度を割り当て、鋭い分割ではなくクラスタ境界間の段階的な遷移を表現します。**スペクトラルクラスタリング**ノードがデータ点を表し、エッジの重みが類似性を示す類似性グラフを構築するグラフ理論的アプローチ。グラフラプラシアン行列のスペクトル分解がクラスタ構造を明らかにし、従来の手法が見逃す非凸クラスタに特に効果的です。

#### クラスタリングの応用

**顧客セグメンテーション**購買行動、人口統計的特性、またはエンゲージメントパターンによって顧客をグループ化し、パーソナライズされたマーケティング、カスタマイズされた製品推奨、ターゲットを絞った維持戦略を可能にします。**画像セグメンテーション**類似した色、テクスチャ、または意味的内容の領域に画像を分割し、物体認識、医療画像分析、自律走行車の知覚をサポートします。**異常検出**正常なパターンでクラスタ化できないデータ点を識別し、金融取引における詐欺、ネットワーク侵入の検出、または製造欠陥の識別にフラグを立てます。**文書整理**類似した文書を自動的にグループ化し、トピック別にニュース記事を整理し、主題領域別に研究論文をクラスタ化し、または顧客サポートチケットを分類します。

### 2. 相関ルール学習

相関ルール学習は、大規模データセット内の変数間の興味深い関係、共起、依存関係を発見し、通常は「if-then」ルールとして表現され、アイテムやイベントがどのくらいの頻度で一緒に現れるかを定量化します。

#### 主要アルゴリズム

**Aprioriアルゴリズム**反復的な候補生成とテストを通じて頻出アイテムセットを体系的に識別します。頻出する個々のアイテムから始めて、Aprioriは段階的により大きなアイテムセットを構築し、反単調性プロパティに基づいて頻出できない候補を剪定します:アイテムセットが非頻出であれば、そのスーパーセットも非頻出でなければなりません。概念的には洗練されていますが、Aprioriは大規模データセットや高次元アイテム空間では計算コストが高くなる可能性があります。**FP-Growthアルゴリズム**トランザクションデータベースを表現するコンパクトなツリー構造(FP-tree)を通じて、高価な候補生成を回避し、より良いパフォーマンスを達成します。FP-Growthは2回のデータベーススキャンでこのツリーを構築し、その後再帰的なツリー走査を通じて頻出パターンをマイニングし、多くの場合Aprioriよりも大幅に高速です。**Eclatアルゴリズム**トランザクションからアイテムへのマッピングではなく、アイテムからトランザクションへのマッピングとしてトランザクションを表現する垂直データ形式を使用します。頻出アイテムセットは集合交差を通じて出現し、ほとんどのアイテムが頻繁に共起する密なデータセットに特に効率的です。

#### 相関ルールメトリクス

**サポート**アイテムセットを含むトランザクションの割合で、パターンがどのくらいの頻度で現れるかを示します。**信頼度**前件が存在する場合に後件が現れる条件付き確率で、ルールの信頼性を測定します。**リフト**アイテムが独立である場合の期待される出現に対する観測された共起の比率で、関連性がランダムな偶然を超えるかどうかを明らかにします。

#### 実用的応用

**マーケットバスケット分析**店舗レイアウトの最適化、クロスセリング戦略、プロモーションバンドリングを導く製品購入パターンを発見します。古典的な例:おむつを購入する顧客は頻繁にビールを購入し、戦略的な製品配置を示唆します。**推薦システム**履歴データで観察された共同購入または共同消費パターンに基づいて、製品、コンテンツ、またはサービスの推奨を生成します。**ウェブ使用マイニング**クリックストリームデータを分析してナビゲーションパターンを明らかにし、ウェブサイト構造の最適化を通知する一般的にアクセスされるページシーケンスを識別します。**医療診断**症状と疾患の関連性、薬物相互作用パターン、または臨床意思決定をサポートする治療結果の相関を発見します。

### 3. 次元削減

次元削減は、高次元データを、本質的な情報を保持しながら冗長性とノイズを除去する低次元表現に変換します。この簡略化は、計算効率を改善し、可視化を可能にし、後続の教師あり学習における過学習を減らし、多くの場合解釈可能な基礎要因を明らかにします。

#### 主要技術

**主成分分析(PCA)**最も広く使用される次元削減技術であるPCAは、データの最大分散を捉える直交方向(主成分)を識別します。第1主成分は最大の分散を説明し、第2は第1に直交する次に大きな分散を捉え、以下同様です。PCAは相関のない特徴を生成し、多重共線性を除去し、多くの場合、ほとんどの情報を保持しながら高次元データの2Dまたは3D可視化を可能にします。**線形判別分析(LDA)**教師ありコンテキストで適用されることが多いですが、LDAはクラス分離性を最大化する特徴の線形結合を見つけます。クラスラベルを必要としますが、自然なデータ分離を明らかにし、後続の教師なし分析を通知する探索的目的に役立ちます。**非負値行列因子分解(NMF)**データ行列を解釈可能な部分ベースの表現を提供する非負因子に分解します。画像やテキストなどの非負性制約を持つデータに特に効果的で、NMFはPCAが生成する双極因子ではなく加法的成分を発見します。**t-SNE(t-Distributed Stochastic Neighbor Embedding)**可視化のために最適化された非線形次元削減で、グローバルパターンを明らかにしながらローカル近傍構造を保持します。t-SNEは複雑な高次元データの2Dまたは3D可視化の作成に優れていますが、計算コストが高く、特徴抽出ではなく主に可視化に適しています。**UMAP(Uniform Manifold Approximation and Projection)**より良い計算効率とスケーラビリティでローカルとグローバルの両方の構造を保持するt-SNEの現代的な代替手段。UMAPは、単一細胞ゲノミクス、テキスト埋め込み可視化、一般的な探索的分析に応用されています。**オートエンコーダ**ボトルネック隠れ層を通じて入力を再構築するように訓練することで圧縮表現を学習するニューラルネットワークアーキテクチャ。圧縮された中間層は次元削減された表現として機能し、非線形変換は線形手法が見逃す複雑なパターンを捉えます。

#### 削減の応用

**データ可視化**高次元データを2Dまたは3D空間に投影し、元の次元では不可能な人間の解釈とパターン認識を可能にします。**ノイズ削減**主成分からデータを再構築することでランダムな変動と測定誤差を除去し、低分散次元で捉えられたノイズを破棄しながら信号を保持します。**特徴エンジニアリング**教師あり学習モデルのための情報量の多い特徴を作成し、多くの場合、計算要件と過学習リスクを減らしながら予測精度を改善します。**圧縮**データをより効率的に保存および送信し、特に画像、ゲノム配列、センサーネットワークに関連します。

## 比較:教師なし学習 vs. 教師あり学習

| **側面**|**教師なし学習**|**教師あり学習**|
|-----------|---------------------------|------------------------|
| **データ要件**| ラベルなしデータのみ | ラベル付き訓練例 |
| **学習目標**| 隠れたパターンの発見 | 入力-出力マッピングの学習 |
| **評価**| 間接的メトリクス、ドメイン専門知識 | 正解に対する直接的な精度 |
| **一般的なタスク**| クラスタリング、次元削減、相関ルール | 分類、回帰 |
| **例**| 顧客セグメンテーション、異常検出 | スパム検出、住宅価格予測 |
| **スケーラビリティ**| より多くの場合スケーラブル(ラベル不要) | ラベル付きデータの可用性によって制限 |
| **解釈可能性**| パターンはドメイン解釈が必要 | 予測はラベルと直接比較可能 |

### ハイブリッドアプローチ

**半教師あり学習**小規模なラベル付きデータセットと大規模なラベルなしデータセットを組み合わせ、教師なし学習を使用して限られた監督を補強します。クラスタリングはラベル伝播を導く構造を明らかにするか、次元削減はより少ないラベルで教師ありモデルのパフォーマンスを改善する特徴を作成する可能性があります。**自己教師あり学習**データ構造自体から監督信号を作成します。言語モデルはコンテキストから次の単語を予測し、画像モデルは回転を予測したりマスクされた領域を埋めたりし、特定の教師ありタスクでファインチューニングする前に事前訓練のために大規模なラベルなしデータセットを活用します。

## 業界全体の実世界応用

### ビジネスとマーケティング

**顧客分析**行動、好み、価値によって顧客をセグメント化し、パーソナライズされた体験、ターゲットキャンペーン、効率的なリソース配分を可能にします。クラスタリングは、異なるエンゲージメント戦略を必要とする明確な顧客グループを明らかにします。**解約予測準備**クラスタリングと相関ルールを通じて解約顧客間のパターンを発見し、教師あり解約予測モデルの特徴エンジニアリングを通知します。**製品開発**顧客フィードバック、使用パターン、クラスタリングと次元削減によって明らかにされた競争的ポジショニングの分析を通じて、満たされていないニーズと市場ギャップを識別します。

### 金融と詐欺検出

**異常検出**密度ベースクラスタリングまたはオートエンコーダ再構築誤差を通じて、異常なトランザクションパターン、潜在的に詐欺的な活動、または市場操作を識別します。**ポートフォリオ最適化**次元削減とクラスタリングを通じて資産相関と市場レジーム構造を発見し、分散戦略を通知します。**信用リスクセグメンテーション**履歴デフォルトデータが限られている場合にリスクプロファイルによってローン申請者をグループ化し、引受戦略と価格設定を導きます。

### ヘルスケアとライフサイエンス

**疾患サブタイプ発見**患者の症状、遺伝マーカー、または治療反応のクラスタリングを通じて明確な疾患症状を識別し、精密医療アプローチを可能にします。**遺伝子発現分析**基礎となる生物学的プロセスを明らかにし、バイオマーカーを識別し、または疾患メカニズムを発見するゲノムデータの次元を削減します。**医療画像分析**解剖学的構造をセグメント化し、正常パターンとの比較を通じて異常を検出し、診断意思決定をサポートします。

### テクノロジーとデータサイエンス

**推薦システム**eコマースプラットフォーム、ストリーミングサービス、コンテンツプラットフォームでパーソナライズされた推奨を導くユーザー好みパターンとアイテム関係を発見します。**テキストマイニング**大規模な文書コレクションを整理し、次元削減(トピックモデリング)を通じて潜在的なトピックを発見し、ソーシャルメディアやニュースの新興トレンドを識別します。**ネットワーク分析**ソーシャルネットワークのコミュニティを検出し、影響力のあるノードを識別し、複雑なネットワークの組織構造を明らかにします。

## 教師なし学習の利点

**ラベル独立性**高価で時間のかかる、または主観的な手動ラベリングなしで動作し、アノテーションが実用的でない大規模データセットの分析を可能にします。**パターン発見**人間の観察者が見落とす可能性のある予期しない構造、関係、外れ値を明らかにし、さらなる調査のための仮説を生成します。**汎用性**タスク固有の適応なしに、多様なドメインとデータタイプ(テキスト、画像、センサーデータ、トランザクション記録、生物学的配列)に適用可能です。**前処理基盤**特徴を作成し、次元を削減し、パフォーマンスと効率を改善した後続の教師あり学習をサポートする構造を識別します。**スケーラビリティ**多くのアルゴリズムは大規模データセットに効率的にスケールし、適切な技術で数百万のインスタンスまたは数千の次元を処理します。

## 課題と制限

### 評価の難しさ

正解ラベルがないため、教師なし学習の品質を評価することは困難です。内部メトリクス(シルエットスコア、クラスタ内分散)はアルゴリズムのプロパティを測定しますが、必ずしも実世界の有用性を測定するわけではありません。ドメイン専門知識、後続の教師あり学習パフォーマンス、またはビジネス成果を通じた外部検証は依然として不可欠ですが主観的です。

### 解釈可能性の曖昧さ

発見されたパターンは人間の解釈を必要とし、直感的なカテゴリや実行可能な洞察と一致しない可能性があります。クラスタは意味のある区別ではなくデータ収集アーティファクトを反映する可能性があります。相関ルールは統計的に有意かもしれませんが、実際には無関係かもしれません。

### ノイズと外れ値への感度

多くのアルゴリズムは、ノイズの多いデータや外れ値でパフォーマンスが低下します。密度ベースの手法は重心ベースのアプローチよりも外れ値をよりよく処理しますが、前処理と堅牢なアルゴリズム選択は依然として重要です。

### ハイパーパラメータ選択

クラスタリングにはクラスタ数または密度パラメータの指定が必要で、次元削減には目標次元の選択が必要で、相関ルールにはサポート/信頼度閾値が必要です。これらの選択は結果に大きく影響し、多くの場合反復的な実験が必要です。

### 計算複雑性

一部の手法は、データサイズや次元性に対してスケールが悪いです。階層的クラスタリングにはO(n²)またはO(n³)の操作が必要で、スペクトル手法には高価な行列分解が含まれ、t-SNEは非常に大規模なデータセットでは実用的ではなくなります。

## 実装のベストプラクティス

### データ準備

**クリーニングと前処理**補完または除去を通じて欠損値を処理し、外れ値を適切に検出して対処し、クラスタリングにおける距離計算をスケールの違いが支配するのを防ぐために特徴を正規化または標準化します。**特徴エンジニアリング**関連する特徴を選択し、ドメイン知識を捉える派生変数を作成し、アルゴリズムのパフォーマンスと解釈可能性を改善する冗長または無関係な次元を除去します。**探索的分析**洗練されたアルゴリズムを適用する前に、データ分布を可視化し、相関を調べ、ドメインコンテキストを理解します。単純な可視化は、多くの場合、手法選択を導く構造を明らかにします。

### アルゴリズム選択とチューニング

**シンプルから始める**複雑な代替案を探索する前に、解釈可能なベースライン手法(k-means、PCA)から始めます。単純なアプローチは、多くの場合、より簡単な解釈で十分な結果を提供します。**複数の手法で検証**発見されたパターンの一貫性をチェックする複数のアルゴリズムを適用します。多様な手法にわたって現れる構造は、アルゴリズムアーティファクトではなく実際のパターンを表す可能性が高いです。**反復的改善**ハイパーパラメータを実験し、内部メトリクスとドメイン専門知識を通じて結果を評価し、発見に基づいて前処理またはアルゴリズムの選択を改善します。

### 解釈とアクション

**ドメイン専門家の関与**発見されたパターンを解釈し、意味性を検証し、発見を実行可能な洞察に変換する主題専門家を関与させます。**健全性チェック**ビジネスロジック、外部データソース、常識に対してパターンを検証します。統計的に有意な発見は実際には意味がないかもしれません。**文書化**方法論的選択、パラメータ設定、解釈の根拠を記録し、再現性を可能にし、組織学習をサポートします。

## よくある質問

**教師あり学習の代わりに教師なし学習をいつ使用すべきですか?**ラベルが利用できない、高価、または主観的な場合、予期しないパターンを発見する探索的分析のため、後続の教師あり学習のための特徴を作成するため、または目標が特定の予測ではなくデータ構造の理解である場合に教師なし学習を使用します。**クラスタ数をどのように選択しますか?**方法には、クラスタ内分散対クラスタ数を示すエルボープロット、クラスタ品質を測定するシルエット分析、自然なグループ化を示唆するドメイン知識、および結果として得られるセグメントのビジネス関連性を評価する複数の値の試行が含まれます。**教師なし学習は間違っている可能性がありますか?**はい。アルゴリズムは常に出力を生成しますが、発見されたパターンは意味のある構造ではなく、ノイズ、アルゴリズムバイアス、またはデータ収集アーティファクトを反映する可能性があります。ドメイン専門知識、外部データ、またはダウンストリームパフォーマンスを通じた検証は依然として重要です。**教師なし学習の結果をどのように評価しますか?**内部メトリクス(シルエットスコア、Davies-Bouldinインデックス)、ドメイン専門家の解釈、発見された特徴を使用したダウンストリーム教師あり学習パフォーマンス、発見に基づくアクションの影響を測定するビジネスメトリクス、および複数の手法にわたる一貫性を使用します。**教師なし学習と特徴エンジニアリングの関係は何ですか?**次元削減は教師ありモデルの特徴を作成し、クラスタリングはカテゴリ特徴(クラスタメンバーシップ)を生成し、発見されたパターンはドメイン理解に基づく手動特徴作成を通知します。

## 参考文献


1. DeepAI. (n.d.). Unsupervised Learning. DeepAI Machine Learning Glossary. URL: https://deepai.org/machine-learning-glossary-and-terms/unsupervised-learning

2. IBM. (n.d.). What is Unsupervised Learning?. IBM Think Topics. URL: https://www.ibm.com/think/topics/unsupervised-learning

3. Vation Ventures. (n.d.). Unsupervised Learning. Vation Ventures Glossary. URL: https://www.vationventures.com/glossary/unsupervised-learning-definition-explanation-and-use-cases

4. Mind Foundry. (n.d.). Unsupervised Learning Glossary. Mind Foundry AI Glossary. URL: https://www.mindfoundry.ai/ai-glossary/unsupervised-learning

5. GeeksforGeeks. (n.d.). What is Unsupervised Learning. GeeksforGeeks Machine Learning. URL: https://www.geeksforgeeks.org/machine-learning/unsupervised-learning/

6. GeeksforGeeks. (n.d.). Clustering in Machine Learning. GeeksforGeeks Machine Learning. URL: https://www.geeksforgeeks.org/machine-learning/clustering-in-machine-learning/

7. GeeksforGeeks. (n.d.). K-Means Clustering. GeeksforGeeks Machine Learning. URL: https://www.geeksforgeeks.org/machine-learning/k-means-clustering-introduction/

8. GeeksforGeeks. (n.d.). DBSCAN Clustering. GeeksforGeeks Machine Learning. URL: https://www.geeksforgeeks.org/machine-learning/dbscan-clustering-in-ml-density-based-clustering/

9. GeeksforGeeks. (n.d.). Hierarchical Clustering. GeeksforGeeks Machine Learning. URL: https://www.geeksforgeeks.org/machine-learning/agglomerative-methods-in-machine-learning/

10. GeeksforGeeks. (n.d.). Dimensionality Reduction. GeeksforGeeks Machine Learning. URL: https://www.geeksforgeeks.org/machine-learning/dimensionality-reduction/

11. GeeksforGeeks. (n.d.). Principal Component Analysis. GeeksforGeeks Data Analysis. URL: https://www.geeksforgeeks.org/data-analysis/principal-component-analysis-pca/

12. GeeksforGeeks. (n.d.). Association Rule Learning. GeeksforGeeks Machine Learning. URL: https://www.geeksforgeeks.org/machine-learning/association-rule/

13. GeeksforGeeks. (n.d.). Apriori Algorithm. GeeksforGeeks Machine Learning. URL: https://www.geeksforgeeks.org/machine-learning/apriori-algorithm/

14. Scikit-learn. (n.d.). Unsupervised Dimensionality Reduction. Scikit-learn Documentation. URL: https://scikit-learn.org/stable/modules/unsupervised_reduction.html

15. Scikit-learn. (n.d.). Clustering. Scikit-learn Documentation. URL: https://scikit-learn.org/stable/modules/clustering.html

16. Wikipedia. (n.d.). Association Rule Learning. Wikipedia. URL: https://en.wikipedia.org/wiki/Association_rule_learning
