---
title: MHaluBench
lastmod: '2025-12-19'
date: '2025-12-19'
translationKey: mhalubench
description: MHaluBenchは、マルチモーダル大規模言語モデル(MLLM)におけるI2TおよびT2Iタスク全体での、きめ細かいクレームレベルのハルシネーション検出のためのメタ評価および注釈ベンチマークです。
keywords:
- MHaluBench
- ハルシネーション検出
- マルチモーダルLLM
- AIベンチマーク
- I2T T2I
category: Multimodal AI
type: glossary
draft: false
e-title: MHaluBench
term: エムハルベンチ
url: "/ja/glossary/MHaluBench/"
---
## MHaluBenchとは?
MHaluBenchは、マルチモーダル大規模言語モデル(MLLM)における幻覚(ハルシネーション)を、細粒度のクレームレベルで評価・検出するための包括的なベンチマークです。画像からテキスト(I2T)とテキストから画像(T2I)の両タスクにわたる標準化された評価を提供し、マルチモーダルコンテキストにおけるモデルの信頼性と精度を正確に評価できます。

従来のベンチマークが応答文や文レベルで動作するのに対し、MHaluBenchはモデル出力を原子的な事実クレームに分解し、各クレームに幻覚タイプとカテゴリーを注釈付けし、検出システムのメタ評価のためのグラウンドトゥルースを提供します。この粒度の高いアプローチにより、マルチモーダルAIシステムにおける特定の障害モードを的確に診断できます。

このベンチマークは、医療画像診断、自律システム、コンテンツモデレーション、支援技術など、精度と真実性が最重要となる本番環境でのMLLM展開における重要な課題に対処します。標準化された評価手法と高品質な注釈を提供することで、MHaluBenchはより信頼性の高いマルチモーダルAIシステムの研究開発を加速します。

## マルチモーダルAIにおける幻覚の理解

MLLMにおける幻覚は、生成された出力コンテンツが構文的にはもっともらしいものの、提供された入力に対して意味的に不誠実であったり、確立された世界知識と矛盾したりする場合に発生します。この現象は視覚、テキスト、クロスモーダル出力全体に現れ、テキストのみの言語モデルと比較して独特の課題を提示します。

### 幻覚の分類体系

<strong>忠実性幻覚:</strong>出力が直接的な入力コンテキストと矛盾する。例:入力画像に存在しないオブジェクトや属性を記述すること。例えば、画像に猫しか写っていないのに「犬が走っている」と主張する場合。

<strong>事実性幻覚:</strong>出力がもっともらしい外観にもかかわらず、確立された外部知識と矛盾する。例:キャプション生成や視覚的質問応答時に「エッフェル塔はロンドンにある」と主張する場合。

<strong>モダリティ矛盾幻覚:</strong>入力または出力のモダリティ間の直接的な矛盾。例:テキスト記述が「赤い車」と述べているのに関連画像が青い車を示している場合、またはT2Iモデルが入力テキストプロンプトと矛盾する画像を生成する場合。

<strong>事実矛盾幻覚:</strong>出力が合理的に見えるが、世界知識や常識に違反する。例:歴史的写真の人物がスマートフォンを使用していると主張したり、建物の建設年代を不可能な時期に設定したりする場合。

### 粒度レベル

<strong>オブジェクトレベル:</strong>エンティティの誤認識、省略、または捏造。例:存在しない人物を検出する、主要な被写体を見逃す、またはオブジェクトカテゴリーを誤認識する場合。

<strong>属性レベル:</strong>正しく識別されたオブジェクトに誤ったプロパティが割り当てられる。例:誤った色、不正確なサイズ、誤った材質、または不正確な空間関係。

<strong>シーンレベル:</strong>全体的なコンテキスト、関係、またはイベントの誤表現。例:屋内シーンを屋外として記述する、活動やイベントを誤認識する、設定の特徴付けが不正確な場合。

<strong>シーンテキスト:</strong>画像内のテキストの認識または生成におけるエラー。例:標識の誤読、テキストコンテンツの捏造、またはT2I出力における誤った書記言語の生成。

## ベンチマーク構造

### データセット構成

MHaluBenchは、3つのタスクカテゴリーにわたる620の慎重に厳選されたインスタンスで構成され、各インスタンスは包括的な評価のためにセグメントレベルとクレームレベルの両方で注釈付けされています。

<strong>画像キャプショニング(IC) - 200サンプル:</strong>- ソース: MS-COCO 2014 Validationデータセット
- 出力生成元: mPLUG、LLaVA、MiniGPT-4
- 注釈の焦点: 視覚コンテンツへの忠実性、オブジェクト/属性の精度

<strong>視覚的質問応答(VQA) - 200サンプル:</strong>- ソース: TextVQAテストセット
- タスク: シーンテキスト認識、視覚的推論、属性識別
- 注釈の焦点: 回答の精度、シーンテキスト幻覚、推論の正確性

<strong>テキストから画像生成(T2I) - 220サンプル:</strong>- ソースプロンプト: DrawBench、T2I-CompBench
- 評価モデル: DALL-E 2、DALL-E 3、Stable Diffusionバリアント
- 注釈の焦点: プロンプトへの準拠、属性の忠実性、構成の精度

総カバレッジ: 620インスタンスで二重レベル注釈(セグメントとクレーム)、全カテゴリーにわたり2,847の注釈付きクレームを提供。

### 注釈手法

<strong>クレーム抽出:</strong>- GPT-4VとGeminiを使用した自動抽出
- 注釈者による手動検証と改良
- 複雑な文から原子的事実主張を分離
- 独立性を確保しながら意味的完全性を維持

<strong>ラベリングプロセス:</strong>- 3名の大学院レベルの注釈者が各クレームを独立してラベル付け
- 二値分類: 幻覚的 vs. 非幻覚的
- カテゴリー割り当て: オブジェクト、属性、シーンテキスト、事実
- 不一致は多数決で解決
- 注釈者間一致度: Fleissのカッパ κ = 0.822(強い一致)

<strong>伝播ルール:</strong>- 含まれるクレームのいずれかが幻覚的であれば、セグメントは幻覚的とマーク
- いずれかのセグメントが幻覚的であれば、応答は幻覚的とマーク
- 細粒度分析を保持しながら粗粒度評価を可能にする

### データスキーマ

各ベンチマークエントリには以下が含まれます:

<strong>識別子:</strong>一意のID、タスクタイプ指定、ソースデータセット参照

<strong>入力:</strong>元画像(I2Tタスク)またはテキストプロンプト(T2Iタスク)

<strong>出力:</strong>生成されたテキスト(I2T)または合成画像(T2I)

<strong>セグメント:</strong>幻覚ラベル付きの論理的テキスト分割(文または節)

<strong>クレーム:</strong>詳細な注釈付き原子的事実主張。クレームテキスト、カテゴリー分類、幻覚ステータス、裏付け根拠を含む

<strong>メタデータ:</strong>モデル情報、生成パラメータ、注釈タイムスタンプ

## UNIHD検出フレームワーク

統合幻覚検出(UNIHD)は、クレーム抽出から検証までのエンドツーエンドパイプラインを提供する、自動幻覚検出の最先端アプローチを表します。

### 4段階パイプライン

<strong>ステージ1 - 本質的クレーム抽出:</strong>- 複雑な出力を原子的事実クレームに分解
- クレームが独立して検証可能であることを保証
- コンテキスト依存クレームの意味的関係を維持
- 非事実的コンテンツ(意見、質問、命令)をフィルタリング

<strong>ステージ2 - 自律的ツール選択:</strong>- 各クレームの検証要件を分析
- 検証のための的を絞ったクエリを策定
- ツールキットから適切な検証ツールを選択:
  - オブジェクト検出器(Grounding DINO、YOLO)
  - 属性分類器(色、サイズ、材質)
  - OCRシステム(シーンテキスト認識)
  - 知識ベース(事実検証)

<strong>ステージ3 - 並列ツール実行:</strong>- 効率性のために選択されたツールを同時展開
- 複数のソースから検証証拠を取得
- ツールの失敗と不確実な結果を適切に処理
- 出所を維持しながら結果を集約

<strong>ステージ4 - 幻覚検証:</strong>- クレームを検証証拠と比較
- 決定に対する人間が読める根拠を生成
- 検出に信頼度スコアを割り当て
- 最終的な幻覚的/非幻覚的ラベルを生成

### 検出アプローチ

<strong>ブラックボックス手法:</strong>内部アクセスなしに入力出力ペアのみを使用してモデルを評価。例: FaithScore、GAVIE、HaELM。利点: モデル非依存、アーキテクチャ知識なしで展開可能。制限: 内部状態を活用できない、説明可能性が限定的。

<strong>ホワイトボックス手法:</strong>モデル内部(注意重み、隠れ状態、トークン確率)を活用。例: DHCP、OPERA、ContextualLens。利点: 不確実性シグナルへの直接アクセス、詳細な解釈可能性。制限: モデル固有の実装、アーキテクチャ知識が必要。

<strong>ツール拡張手法:</strong>証拠ベースの検出のために外部検証ツールを活用。例: UNIHD、FactChecker、CutPaste & Find。利点: 外部証拠に基づく、新しいツールで拡張可能。制限: ツールの精度に依存、潜在的なスケーラビリティの課題。

<strong>ハイブリッドアプローチ:</strong>シナリオと障害モード全体で堅牢なパフォーマンスを実現するために、複数の検出パラダイムを組み合わせる。

## ベンチマークの位置づけ

### 関連ベンチマークとの比較

| ベンチマーク | モダリティ | タスク | 粒度 | カテゴリー | 注釈 | 独自の特徴 |
|-----------|-----------|-------|-------------|------------|------------|----------------|
| <strong>HaluEval</strong>| テキスト | QA、要約 | 応答 | 事実性 | 応答レベル | 大規模テキスト焦点 |
| <strong>POPE</strong>| 画像+テキスト | キャプショニング | 応答 | 忠実性 | 応答レベル | 視覚オブジェクトの存在 |
| <strong>HalluCode</strong>| コード | コード生成 | トークン | マッピング、命名、ロジック | トークンレベル | 実行ベースの検証 |
| <strong>CodeHalu</strong>| コード | コード生成 | スパン | リソース、ロジック | スパンレベル | 包括的コード分析 |
| <strong>Collu-Bench</strong>| コード | 生成と修復 | トークン | 複数 | トークンレベル | マルチLLM比較 |
| <strong>MHaluBench</strong>| 画像+テキスト | I2T、T2I | クレーム | オブジェクト、属性、シーン、事実 | クレーム+セグメント | 統合マルチモーダルカバレッジ |

<strong>独自の利点:</strong>- I2TとT2Iの両タスクを包括的にカバーする唯一のベンチマーク
- クレームレベル注釈による最高粒度で的を絞った分析を可能にする
- モダリティ矛盾と事実矛盾を区別する明示的な分類体系
- 検出システムのメタ評価専用に設計
- 幻覚カテゴリーとタスクタイプ全体でバランスの取れたカバレッジ

## 実用的応用

### モデル開発

<strong>的を絞った改善:</strong>特定の障害モード(例:医療画像における属性幻覚)を特定し、焦点を絞ったモデル改良を可能にする。

<strong>アブレーション研究:</strong>アーキテクチャの変更やトレーニング手順が特定の幻覚カテゴリーに与える影響を評価。

<strong>比較分析:</strong>標準化された幻覚メトリクスで複数のモデルバリアントやアーキテクチャをベンチマーク。

### 検出システム評価

<strong>メタ評価:</strong>グラウンドトゥルースに対して検出システムを評価し、カテゴリー全体で精度、再現率、F1を測定。

<strong>堅牢性テスト:</strong>多様なシナリオ、ドメイン、幻覚タイプ全体で検出器のパフォーマンスを評価。

<strong>ツール開発:</strong>特定の幻覚カテゴリー用の専門検出ツールの開発を導く。

### 本番展開

<strong>品質保証:</strong>本番システムで許容可能な幻覚率の閾値と監視を確立。

<strong>ユーザー信頼:</strong>展開決定とユーザー期待を通知する証拠ベースの信頼性メトリクスを提供。

<strong>リスク軽減:</strong>人間の監視や追加検証が必要な高リスクシナリオを特定。

## 実例

### 画像からテキストへの幻覚

<strong>入力:</strong>右側に青いユニフォームを着た選手を示すサッカー試合の写真

<strong>モデル出力:</strong>「右側の選手は赤いユニフォームを着ており、クラブ・アメリカに所属しています。」

<strong>抽出されたクレーム:</strong>1. 「右側の選手は赤いユニフォームを着ている」 - 幻覚的(属性レベル、モダリティ矛盾:画像は青いユニフォームを示している)
2. 「選手はクラブ・アメリカに所属している」 - 事実確認が必要(事実レベル:チームメンバーシップの外部検証が必要)

<strong>検出プロセス:</strong>オブジェクト検出器が選手の存在と位置を確認、属性分類器がクレーム1と矛盾する青いユニフォームを識別、知識ベースクエリがクレーム2のチーム情報を検証。

### テキストから画像への幻覚

<strong>入力プロンプト:</strong>「パリのエッフェル塔の前に駐車された黄色いスクールバス」

<strong>生成画像:</strong>未確認のランドマークの前の赤いバス

<strong>抽出されたクレーム:</strong>1. 「画像には黄色いスクールバスが含まれている」 - 幻覚的(オブジェクト/属性レベル、モダリティ矛盾:画像は赤いバスを示している)
2. 「バスはエッフェル塔の前に位置している」 - 幻覚的(事実レベル、モダリティ矛盾:ランドマークがエッフェル塔として識別できない)

<strong>検出プロセス:</strong>オブジェクト検出がバスを識別するが色が間違っている、ランドマーク認識がエッフェル塔を確認できない、属性検証が黄色のクレームと矛盾。

## 制限と今後の方向性

<strong>規模の制約:</strong>現在の620インスタンスは大規模テキストベンチマークより小さい。より多くのドメインとモダリティにわたる数千のインスタンスへの拡張を計画中。

<strong>注釈コスト:</strong>人間による注釈はリソース集約的で迅速なスケーリングを制限。将来の研究では人間検証を伴う半自動注釈を探求。

<strong>モダリティカバレッジ:</strong>現在は画像テキストペアに限定。ビデオ、オーディオ、3D、センサーデータモダリティへの拡張を検討中。

<strong>ツール依存性:</strong>検出パフォーマンスは外部ツールの精度に制約される。ツールの信頼性向上と専門検証システムの開発が進行中。

<strong>動的評価:</strong>静的ベンチマークは実世界の展開課題を反映しない可能性。ライブシステム用の動的評価プロトコルの開発が必要。

<strong>文化的・言語的多様性:</strong>現在は英語と一般的な画像ドメインに焦点。多言語設定と多様な文化的コンテキストへの拡張を計画中。

<strong>緩和統合:</strong>ベンチマークは検出に焦点を当てている。修正および緩和システムとの統合は今後の方向性を表す。

## 実装リソース

<strong>データセットアクセス:</strong>包括的なドキュメント、評価スクリプト、ベースライン結果を備えたHuggingFace Datasetsプラットフォームで利用可能。

<strong>評価ツール:</strong>検出メトリクスの計算、エラーパターンの分析、詳細レポートの生成のためのPythonツールキット。

<strong>ベースライン実装:</strong>再現と拡張のためのUNIHDおよび他の検出アプローチの参照実装。

<strong>コミュニティ貢献:</strong>公開リーダーボードへの検出システム提出、ベンチマークカバレッジの拡張への公開招待。

## 参考文献


1. Shi, W., et al. (2024). Unified Hallucination Detection. ACL 2024 Proceedings.

2. Anonymous. (2024). Multimodal Hallucination Survey. arXiv.

3. OpenKG. (n.d.). MHaluBench Dataset. HuggingFace.

4. COCO Consortium. (n.d.). MS-COCO Dataset. COCO Dataset.

5. TextVQA Team. (n.d.). TextVQA Dataset. TextVQA.

6. Anonymous. (2022). DrawBench Benchmark. arXiv.

7. Anonymous. (2023). T2I-CompBench. arXiv.

8. Anonymous. (2023). HaluEval Paper. arXiv.

9. Anonymous. (2024). HalluCode Benchmark. arXiv.

10. Anonymous. (2024). CodeHalu Benchmark. arXiv.

11. Anonymous. (2024). Collu-Bench. arXiv.

12. mPLUG. (n.d.). mPLUG Framework. URL: https://github.com/X-PLUG/mPLUG

13. LLaVA. (n.d.). LLaVA Model. URL: https://llava-vl.github.io/

14. MiniGPT-4. (n.d.). MiniGPT-4. URL: https://minigpt-4.github.io/

15. OpenAI. (n.d.). DALL-E 2. URL: https://openai.com/research/dall-e

16. OpenAI. (n.d.). DALL-E 3. URL: https://openai.com/blog/dall-e-3

17. IDEA Research. (n.d.). Grounding DINO. URL: https://github.com/IDEA-Research/GroundingDINO

18. OpenAI. (n.d.). GPT-4V Vision. URL: https://platform.openai.com/docs/guides/vision

19. Google DeepMind. (n.d.). Gemini Multimodal. URL: https://deepmind.google/technologies/gemini/

20. Emergent Mind. (n.d.). MMHal-Bench. URL: https://www.emergentmind.com/topics/mmhal-bench
