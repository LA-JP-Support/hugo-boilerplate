<!DOCTYPE html>
<html dir="ltr" lang="en">
<head>
<meta content="strict-origin-when-cross-origin" name="referrer"/>
<meta charset="utf-8"/>
<meta content="minimum-scale=1, width=device-width, initial-scale=1.0, shrink-to-fit=no" name="viewport"/>
<title>Long Short-Term Memory (LSTM) | SmartWeb</title>
<link href="https://main.d1jtfhinlastnr.amplifyapp.com/en/glossary/long-short-term-memory--lstm-/" rel="canonical"/>
<link href="/images/faivicon.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="/images/faivicon.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="/images/faivicon.png" rel="apple-touch-icon" sizes="180x180"/>
<link href="/images/faivicon.png" rel="shortcut icon"/>
<link href="%!s(&lt;nil&gt;)%!s(&lt;nil&gt;)" hreflang="en" rel="alternate"/>
<link href="%!s(&lt;nil&gt;)%!s(&lt;nil&gt;)" hreflang="ja" rel="alternate"/>
<link href="%!s(&lt;nil&gt;)%!s(&lt;nil&gt;)" hreflang="x-default" rel="alternate"/>
<meta content="A neural network that remembers important information over long sequences of data, making it useful for tasks like predicting stock prices or understanding sentences in text." name="description"/>
<meta content="LSTM neural networks, recurrent neural networks, sequential data processing, time series analysis, deep learning architecture" name="keywords"/>
<meta content="website" property="og:type"/>
<meta content="https://main.d1jtfhinlastnr.amplifyapp.com/en/glossary/long-short-term-memory--lstm-/" property="og:url"/>
<meta content="Long Short-Term Memory (LSTM) | SmartWeb" property="og:title"/>
<meta content="A neural network that remembers important information over long sequences of data, making it useful for tasks like predicting stock prices or understanding sentences in text." property="og:description"/>
<meta content="" property="og:image"/>
<meta content="1200" property="og:image:width"/>
<meta content="630" property="og:image:height"/>
<meta content="summary_large_image" name="twitter:card"/>
<meta content="https://main.d1jtfhinlastnr.amplifyapp.com/en/glossary/long-short-term-memory--lstm-/" name="twitter:url"/>
<meta content="Long Short-Term Memory (LSTM) | SmartWeb" name="twitter:title"/>
<meta content="A neural network that remembers important information over long sequences of data, making it useful for tasks like predicting stock prices or understanding sentences in text." name="twitter:description"/>
<meta content="" name="twitter:image"/>
<style>
  :root {
     
    --color-primary: #1a73e8;
    --color-primary-light: #4285f4;
    --color-primary-dark: #1557b0;

    --color-secondary: #34a853;
    --color-accent: #fbbc05;

    --color-text: #202124;
    --color-background: #ffffff;

     
    --gradient-primary: linear-gradient(to right, var(--color-primary), var(--color-primary-light));
  }

   
  .bg-gradient-primary {
    background-image: var(--gradient-primary);
  }

  .text-gradient,
  .text-gradient-primary {
    background-image: var(--gradient-primary);
    background-clip: text;
    -webkit-background-clip: text;
    color: transparent;
    -webkit-text-fill-color: transparent;
  }
</style>
<link as="font" crossorigin="anonymous" href="/fonts/inter/Inter-VariableFont_opsz,wght.woff2" rel="preload" type="font/woff2"/>
<link href="https://fonts.googleapis.com" rel="preconnect"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css2?family=Noto+Sans+JP:wght@400;500;700&amp;family=Noto+Serif+JP:wght@400;500;600;700&amp;display=swap" rel="stylesheet"/>
<link crossorigin="anonymous" href="/css/main.css?v=20260111165525" rel="stylesheet"/>
<link crossorigin="anonymous" href="/css/custom-code-blockquote.css?v=20260111165525" rel="stylesheet"/>
<script defer="" src="/js/main.js?v=20260111165525"></script>
</head>
<body class="antialiased bg-white">
<header class="bg-white">
<nav aria-label="Global" class="mx-auto flex max-w-7xl items-center justify-between gap-x-6 p-6 lg:px-8">
<div class="flex lg:flex-1">
<a class="-m-1.5 p-1.5" href="/en/">
<span class="sr-only">SmartWeb</span>
<picture class="lazy-picture" data-maxwidth="200">
<source data-original-src="/images/smartweb-logo.png" data-srcset="/images/smartweb-logo.png 466w" sizes="200px" type="image/png">
<img alt="SmartWeb Logo" class="lazy-image h-6 sm:h-10 md:h-12 w-auto" data-original-src="/images/smartweb-logo.png" data-src="/images/smartweb-logo.png" decoding="async" loading="lazy"/>
</source></picture>
</a>
</div>
<div class="hidden lg:flex lg:gap-x-12"><a class="text-sm/6 font-semibold text-gray-900" href="/en/">Home</a><a class="text-sm/6 font-semibold text-gray-900" href="/en/blog/">Blog</a><a class="text-sm/6 font-semibold text-gray-900" href="/en/glossary/">Glossary</a><a class="text-sm/6 font-semibold text-gray-900" href="https://www.intwk.co.jp/about/">Company</a></div>
<div class="flex flex-1 items-center justify-end gap-x-6">
<a class="hidden text-sm/6 font-semibold text-gray-900 lg:block" href="https://support.smartweb.jp/">Support</a>
<a class="rounded-md bg-indigo-600 px-3 py-2 text-sm font-semibold text-white shadow-xs hover:bg-indigo-500 focus-visible:outline-2 focus-visible:outline-offset-2 focus-visible:outline-indigo-600" href="/en/blog/">Get Started</a>
</div>
<div class="flex lg:hidden">
<button aria-controls="mobile-menu-1768118125587168000" aria-expanded="false" class="-m-2.5 inline-flex items-center justify-center rounded-xl p-2.5 text-gray-700" type="button">
<span class="sr-only">Open main menu</span>
<svg aria-hidden="true" class="size-6" data-slot="icon" fill="none" stroke="currentColor" stroke-width="1.5" viewbox="0 0 24 24">
<path d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5" stroke-linecap="round" stroke-linejoin="round"></path>
</svg>
</button>
</div>
</nav>
<div aria-modal="true" class="lg:hidden hidden relative z-50" id="mobile-menu-1768118125587168000" role="dialog">
<div class="fixed inset-0 z-50 bg-black bg-opacity-25"></div>
<div class="fixed inset-y-0 right-0 z-50 w-full overflow-y-auto bg-white px-6 py-6 sm:max-w-sm sm:ring-1 sm:ring-gray-900/10 transform transition-transform duration-300 ease-in-out translate-x-full">
<div class="flex items-center gap-x-6">
<a class="-m-1.5 p-1.5" href="/en/">
<span class="sr-only">SmartWeb</span>
<picture class="lazy-picture" data-maxwidth="3000">
<source data-original-src="/images/smartweb-logo.png" data-srcset="/images/smartweb-logo.png 466w" sizes="(max-width: 466px) 466px, 3000px" type="image/png">
<img alt="SmartWeb Logo" class="lazy-image h-6 sm:h-10 md:h-12 w-auto" data-original-src="/images/smartweb-logo.png" data-src="/images/smartweb-logo.png" decoding="async" loading="lazy"/>
</source></picture>
</a>
<a class="ml-auto rounded-md bg-indigo-600 px-3 py-2 text-sm font-semibold text-white shadow-xs hover:bg-indigo-500 focus-visible:outline-2 focus-visible:outline-offset-2 focus-visible:outline-indigo-600" href="/en/blog/">Get Started</a>
<button class="-m-2.5 rounded-xl p-2.5 text-gray-700 close-mobile-menu" type="button">
<span class="sr-only">Close menu</span>
<svg aria-hidden="true" class="size-6" data-slot="icon" fill="none" stroke="currentColor" stroke-width="1.5" viewbox="0 0 24 24">
<path d="M6 18 18 6M6 6l12 12" stroke-linecap="round" stroke-linejoin="round"></path>
</svg>
</button>
</div>
<div class="mt-6 flow-root">
<div class="-my-6 divide-y divide-gray-500/10">
<div class="space-y-2 py-6"><a class="-mx-3 block rounded-xl px-3 py-2 text-base/7 font-semibold text-gray-900 hover:bg-gray-50" href="/en/">Home</a><a class="-mx-3 block rounded-xl px-3 py-2 text-base/7 font-semibold text-gray-900 hover:bg-gray-50" href="/en/blog/">Blog</a><a class="-mx-3 block rounded-xl px-3 py-2 text-base/7 font-semibold text-gray-900 hover:bg-gray-50" href="/en/glossary/">Glossary</a><a class="-mx-3 block rounded-xl px-3 py-2 text-base/7 font-semibold text-gray-900 hover:bg-gray-50" href="https://www.intwk.co.jp/about/">Company</a></div>
<div class="py-6">
<a class="-mx-3 block rounded-lg px-3 py-2.5 text-base/7 font-semibold text-gray-900 hover:bg-gray-50" href="https://support.smartweb.jp/">Support</a>
</div>
</div>
</div>
</div>
</div>
</header>
<script>
  document.addEventListener('DOMContentLoaded', function() {
    const mobileMenuButton = document.querySelector('[aria-controls="mobile-menu-1768118125587168000"]');
    const mobileMenu = document.getElementById('mobile-menu-1768118125587168000');
    const closeButtons = document.querySelectorAll('.close-mobile-menu');
    const mobileMenuContent = mobileMenu.querySelector('.fixed.inset-y-0');
    
    if (mobileMenuButton && mobileMenu) {
      mobileMenuButton.addEventListener('click', function() {
        const expanded = mobileMenuButton.getAttribute('aria-expanded') === 'true';
        
        if (expanded) {
          closeMobileMenu();
        } else {
          openMobileMenu();
        }
      });
      
      
      closeButtons.forEach(button => {
        button.addEventListener('click', closeMobileMenu);
      });
      
      
      mobileMenu.addEventListener('click', function(event) {
        if (event.target === mobileMenu) {
          closeMobileMenu();
        }
      });
      
      
      mobileMenuContent.addEventListener('click', function(event) {
        event.stopPropagation();
      });
      
      function openMobileMenu() {
        mobileMenuButton.setAttribute('aria-expanded', 'true');
        mobileMenu.classList.remove('hidden');
        
        
        mobileMenuContent.offsetHeight;
        
        mobileMenuContent.classList.remove('translate-x-full');
        document.body.classList.add('overflow-hidden');
      }
      
      function closeMobileMenu() {
        mobileMenuButton.setAttribute('aria-expanded', 'false');
        mobileMenuContent.classList.add('translate-x-full');
        
        
        setTimeout(() => {
          mobileMenu.classList.add('hidden');
          document.body.classList.remove('overflow-hidden');
        }, 300);
      }
    }
  });
</script>
<main class="w-full">
<article class="mx-auto max-w-5xl px-4 sm:px-6 lg:px-8">
<header class="py-12 sm:py-16">
<div class="mx-auto max-w-4xl">
<div class="mb-8">
<nav class="text-sm hidden sm:block">
<ol class="flex items-center space-x-2 text-gray-400 dark:text-gray-500">
<li class="flex items-center">
<a class="hover:text-gray-900 dark:hover:text-gray-300 transition-colors flex items-center" href="/en/">
<img alt="Home" class="h-4 w-4 opacity-60" src="/images/home-icon.png"/>
</a>
</li>
<li><span class="mx-2 text-gray-300 dark:text-gray-600">/</span></li>
<li>
<a class="hover:text-gray-900 dark:hover:text-gray-300 transition-colors" href="/en/glossary/">
                Glossary
              </a>
</li>
<li><span class="mx-2 text-gray-300 dark:text-gray-600">/</span></li>
<li class="text-gray-600 dark:text-gray-400 truncate max-w-xs">Long Short-Term Memory (LSTM)</li>
</ol>
</nav>
</div>
<div class="mb-6">
<span class="inline-flex items-center text-xs font-medium tracking-wider uppercase text-gray-500 dark:text-gray-400 border-b border-gray-300 dark:border-gray-600 pb-1">
<svg class="mr-2 h-3.5 w-3.5" fill="none" stroke="currentColor" viewbox="0 0 24 24">
<path d="M7 7h.01M7 3h5c.512 0 1.024.195 1.414.586l7 7a2 2 0 010 2.828l-7 7a2 2 0 01-2.828 0l-7-7A1.994 1.994 0 013 12V7a4 4 0 014-4z" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"></path>
</svg>
            Application &amp; Use-Cases
          </span>
</div>
<h1 class="text-2xl font-bold tracking-tight text-gray-900 dark:text-white sm:text-3xl md:text-4xl leading-tight mb-2">
        Long Short-Term Memory (LSTM)
      </h1>
<div class="mb-6"></div>
<p class="text-base sm:text-lg leading-relaxed text-gray-600 dark:text-gray-300 font-light max-w-3xl">
          A neural network that remembers important information over long sequences of data, making it useful for tasks like predicting stock prices or understanding sentences in text.
        </p>
<div class="mt-6 mb-4 border-t border-gray-100 dark:border-gray-800"></div>
<div class="flex flex-col sm:flex-row sm:items-center sm:justify-between gap-4">
<div class="flex flex-wrap gap-2">
<span class="inline-flex items-center px-2.5 py-1 text-xs font-medium text-gray-600 dark:text-gray-400 bg-transparent border border-gray-200 dark:border-gray-700 rounded hover:border-gray-400 dark:hover:border-gray-500 transition-colors">
                LSTM neural networks
              </span>
<span class="inline-flex items-center px-2.5 py-1 text-xs font-medium text-gray-600 dark:text-gray-400 bg-transparent border border-gray-200 dark:border-gray-700 rounded hover:border-gray-400 dark:hover:border-gray-500 transition-colors">
                recurrent neural networks
              </span>
<span class="inline-flex items-center px-2.5 py-1 text-xs font-medium text-gray-600 dark:text-gray-400 bg-transparent border border-gray-200 dark:border-gray-700 rounded hover:border-gray-400 dark:hover:border-gray-500 transition-colors">
                sequential data processing
              </span>
<span class="inline-flex items-center px-2.5 py-1 text-xs font-medium text-gray-600 dark:text-gray-400 bg-transparent border border-gray-200 dark:border-gray-700 rounded hover:border-gray-400 dark:hover:border-gray-500 transition-colors">
                time series analysis
              </span>
<span class="inline-flex items-center px-2.5 py-1 text-xs font-medium text-gray-600 dark:text-gray-400 bg-transparent border border-gray-200 dark:border-gray-700 rounded hover:border-gray-400 dark:hover:border-gray-500 transition-colors">
                deep learning architecture
              </span>
</div>
<div class="text-sm text-gray-500 dark:text-gray-400 flex flex-col items-end gap-y-1 text-right">
<span class="inline-flex items-center justify-end">
<svg class="mr-1.5 h-4 w-4" fill="none" stroke="currentColor" viewbox="0 0 24 24">
<path d="M8 7V5a3 3 0 013-3h2a3 3 0 013 3v2m4 0h-16a2 2 0 00-2 2v9a2 2 0 002 2h16a2 2 0 002-2V9a2 2 0 00-2-2z" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"></path>
</svg>
      
        Created: December 19, 2025
      
    </span>
</div>
</div>
</div>
</header>
<div class="prose prose-base sm:prose-lg dark:prose-invert mx-auto max-w-4xl py-6 sm:py-8">
<h2 id="what-is-a-long-short-term-memory-lstm">What is a Long Short-Term Memory (LSTM)?</h2>
<p>Long Short-Term Memory (LSTM) is a specialized type of recurrent neural network (RNN) architecture designed to overcome the vanishing gradient problem that plagued traditional RNNs when processing long sequences of data. Developed by Sepp Hochreiter and Jürgen Schmidhuber in 1997, LSTM networks have become one of the most influential and widely adopted deep learning architectures for sequential data processing. The fundamental innovation of LSTM lies in its ability to selectively remember and forget information over extended periods, making it particularly effective for tasks involving long-term dependencies in sequential data.</p>
<p>The architecture of LSTM networks incorporates a sophisticated gating mechanism that controls the flow of information through the network. Unlike traditional RNNs that suffer from exponential decay or explosion of gradients during backpropagation through time, LSTM networks maintain a constant error flow through specialized memory cells. These cells are regulated by three types of gates: forget gates, input gates, and output gates. Each gate uses sigmoid activation functions to determine what information should be discarded, what new information should be stored, and what parts of the cell state should be output. This gating mechanism allows LSTM networks to maintain relevant information across many time steps while discarding irrelevant data, effectively solving the long-term dependency problem.</p>
<p>LSTM networks have revolutionized numerous fields including natural language processing, <a data-lb="1" href="/en/glossary/speech-recognition/" title="Speech Recognition glossary entry">speech recognition</a>, machine translation, and time series forecasting. Their ability to capture complex temporal patterns and dependencies has made them indispensable for applications requiring understanding of sequential relationships in data. The success of LSTM networks has led to various improvements and variations, including Gated Recurrent Units (GRUs), bidirectional LSTMs, and attention mechanisms. Today, LSTM networks serve as foundational components in many state-of-the-art deep learning systems, particularly in scenarios where understanding temporal context and maintaining long-term memory are crucial for achieving optimal performance.</p>
<h2 id="core-lstm-components">Core LSTM Components</h2>
<p>Cell State: The cell state acts as the memory highway of the LSTM, carrying information across time steps with minimal linear interactions. It allows information to flow unchanged or be selectively modified through the gating mechanisms.Forget Gate: This gate determines what information should be discarded from the cell state by outputting values between 0 and 1 for each element. A value of 0 means “completely forget” while 1 means “completely retain.“Input Gate: The input gate controls what new information will be stored in the cell state by combining a sigmoid layer that decides which values to update and a tanh layer that creates candidate values.Output Gate: This gate determines what parts of the cell state will be output as the hidden state by applying a sigmoid function to decide which parts to output and multiplying by the tanh of the cell state.Hidden State: The hidden state represents the output of the LSTM cell at each time step, containing information that the network deems relevant for the current prediction or next time step.Candidate Values: These are new candidate values created by the tanh layer that could potentially be added to the cell state, representing new information that might be relevant to remember.Gating Mechanisms: The combination of forget, input, and output gates that work together to regulate information flow, enabling the network to learn what to remember, what to forget, and what to output.</p>
<h2 id="how-long-short-term-memory-lstm-works">How Long Short-Term Memory (LSTM) Works</h2>
<p>The LSTM processing workflow follows a systematic approach through its gating mechanisms:</p>
<ol>
<li>
<p>Forget Gate Processing: The forget gate examines the previous hidden state and current input to determine what information from the cell state should be discarded, outputting values between 0 and 1 for each element in the cell state.</p>
</li>
<li>
<p>Input Gate Activation: The input gate decides which new information will be stored in the cell state by running a sigmoid function over the concatenated previous hidden state and current input.</p>
</li>
<li>
<p>Candidate Value Generation: A tanh layer creates a vector of new candidate values that could be added to the cell state, representing potential new information to be remembered.</p>
</li>
<li>
<p>Cell State Update: The old cell state is updated by multiplying it with the forget gate output (forgetting selected information) and adding the product of input gate and candidate values (adding new information).</p>
</li>
<li>
<p>Output Gate Computation: The output gate determines what parts of the cell state will be output by applying a sigmoid function to the concatenated previous hidden state and current input.</p>
</li>
<li>
<p>Hidden State Generation: The final hidden state is computed by multiplying the output gate values with the tanh of the updated cell state, producing the output for the current time step.Example Workflow: In language modeling, when processing the sentence “The cat sat on the mat,” the LSTM might use the forget gate to discard irrelevant previous context, the input gate to incorporate information about “cat” being the subject, update the cell state to maintain subject-verb agreement information, and use the output gate to produce hidden states that inform predictions about subsequent words while maintaining grammatical context throughout the sequence.</p>
</li>
</ol>
<h2 id="key-benefits">Key Benefits</h2>
<p>Long-Term Dependency Handling: LSTM networks excel at capturing relationships between events separated by many time steps, making them ideal for tasks requiring understanding of long-range dependencies in sequential data.Vanishing Gradient Solution: The gating mechanism and cell state design effectively address the vanishing gradient problem that plagued traditional RNNs, enabling stable training on long sequences.Selective Memory Management: The sophisticated gating system allows LSTMs to selectively remember important information while forgetting irrelevant details, optimizing memory usage for better performance.Flexible Sequence Processing: LSTM networks can handle variable-length sequences and adapt to different input-output configurations, including one-to-many, many-to-one, and many-to-many mappings.Robust Training Stability: The architecture provides more stable gradients during backpropagation, leading to more reliable and consistent training compared to vanilla RNNs.<a data-lb="1" href="/en/glossary/contextual-understanding/" title="contextual understanding glossary entry">Contextual Understanding</a>: LSTMs maintain contextual information across time steps, enabling better understanding of sequential patterns and temporal relationships in data.Versatile Architecture: The modular design allows for easy integration with other neural network components and adaptation to various problem domains and requirements.Proven Performance: Extensive research and real-world applications have demonstrated LSTM effectiveness across diverse domains, providing confidence in their reliability and performance.Scalable Implementation: Modern deep learning frameworks provide optimized LSTM implementations that can efficiently scale to handle large datasets and complex sequential modeling tasks.Transfer Learning Capability: Pre-trained LSTM models can be fine-tuned for specific tasks, reducing training time and improving performance on domain-specific applications.</p>
<h2 id="common-use-cases">Common Use Cases</h2>
<p>Natural Language Processing: LSTMs power language models, text generation systems, and <a data-lb="1" href="/en/glossary/sentiment-analysis/" title="Sentiment Analysis glossary entry">sentiment analysis</a> applications by capturing linguistic patterns and semantic relationships in textual data.Machine Translation: Neural machine translation systems utilize LSTM encoder-decoder architectures to translate text between languages while maintaining semantic meaning and grammatical structure.Speech Recognition: LSTM networks process audio sequences to convert spoken language into text, handling variations in pronunciation, accent, and speaking speed.Time Series Forecasting: Financial markets, weather prediction, and demand forecasting applications leverage LSTMs to identify temporal patterns and predict future values.Video Analysis: LSTM networks analyze video sequences for action recognition, object tracking, and temporal event detection in computer vision applications.Music Generation: Compositional AI systems use LSTMs to generate musical sequences by learning patterns in melody, harmony, and rhythm from training data.Anomaly Detection: Sequential anomaly detection systems employ LSTMs to identify unusual patterns in time-series data for fraud detection and system <a data-lb="1" href="/en/glossary/monitoring/" title="Monitoring glossary entry">monitoring</a>.Chatbots and <a data-lb="1" href="/en/glossary/conversational-ai/" title="Conversational AI glossary entry">Conversational AI</a>: LSTM-based dialogue systems maintain conversation context and generate appropriate responses in natural language interactions.Handwriting Recognition: LSTM networks process sequential pen strokes to recognize handwritten text and convert it into digital format.Protein Sequence Analysis: Bioinformatics applications use LSTMs to analyze protein sequences and predict structural and functional properties.</p>
<h2 id="lstm-vs-traditional-rnn-comparison">LSTM vs Traditional RNN Comparison</h2>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Traditional RNN</th>
<th>LSTM</th>
</tr>
</thead>
<tbody>
<tr>
<td>Memory Mechanism</td>
<td>Simple hidden state</td>
<td>Cell state with gating</td>
</tr>
<tr>
<td>Gradient Flow</td>
<td>Vanishing/exploding gradients</td>
<td>Stable gradient flow</td>
</tr>
<tr>
<td>Long-term Dependencies</td>
<td>Poor performance</td>
<td>Excellent handling</td>
</tr>
<tr>
<td>Training Complexity</td>
<td>Simple but unstable</td>
<td>Complex but stable</td>
</tr>
<tr>
<td>Computational Cost</td>
<td>Lower</td>
<td>Higher due to gates</td>
</tr>
<tr>
<td>Parameter Count</td>
<td>Fewer parameters</td>
<td>4x more parameters</td>
</tr>
</tbody>
</table>
<h2 id="challenges-and-considerations">Challenges and Considerations</h2>
<p>Computational Complexity: LSTM networks require significantly more <a data-lb="1" href="/en/glossary/computational-resources/" title="Computational Resources glossary entry">computational resources</a> than traditional RNNs due to their complex gating mechanisms and increased parameter count.Training Time: The sophisticated architecture and larger parameter space result in longer training times, especially for large datasets and complex sequential modeling tasks.Memory Requirements: LSTM networks consume more memory during training and inference due to the need to maintain cell states and multiple gate computations.Hyperparameter Sensitivity: The performance of LSTM networks can be sensitive to hyperparameter choices, requiring careful tuning of learning rates, hidden dimensions, and regularization parameters.Overfitting Risk: The increased model complexity and parameter count make LSTM networks more susceptible to overfitting, particularly on smaller datasets.Sequential Processing Limitation: Unlike attention mechanisms, LSTMs must process sequences sequentially, limiting parallelization opportunities and potentially creating <a data-lb="1" href="/en/glossary/bottlenecks/" title="Bottlenecks glossary entry">bottlenecks</a>.Gradient Computation Complexity: While LSTMs solve vanishing gradients, the backpropagation through time algorithm still requires careful implementation and can be computationally intensive.Architecture Selection: Choosing appropriate LSTM variants, layer depths, and connectivity patterns requires domain expertise and extensive experimentation.Interpretability Challenges: The complex internal state dynamics make it difficult to interpret what information the network has learned and how decisions are made.Scalability Concerns: Very long sequences can still pose challenges for LSTM networks, requiring techniques like truncated backpropagation or hierarchical approaches.</p>
<h2 id="implementation-best-practices">Implementation Best Practices</h2>
<p>Proper Data Preprocessing: Normalize input sequences, handle variable lengths appropriately, and ensure consistent data formatting to optimize LSTM training and performance.Gradient Clipping: Implement gradient clipping to prevent exploding gradients during training, typically using values between 1.0 and 5.0 for the clipping threshold.Regularization Techniques: Apply dropout, recurrent dropout, and weight regularization to prevent overfitting and improve generalization performance on unseen data.Appropriate Initialization: Use proper weight initialization schemes like Xavier or He initialization to ensure stable training from the beginning of the optimization process.Learning Rate Scheduling: Implement adaptive learning rate schedules or use optimizers like Adam that automatically adjust learning rates during training.Batch Size Optimization: Choose appropriate batch sizes that balance training stability, memory usage, and convergence speed based on available computational resources.Sequence Length Management: Use techniques like truncated backpropagation through time for very long sequences while maintaining important temporal dependencies.Model Architecture Design: Carefully design the number of LSTM layers, hidden units, and connections based on problem complexity and available training data.Validation Strategy: Implement proper time-series cross-validation techniques that respect temporal ordering and avoid data leakage from future to past.Performance Monitoring: Track relevant metrics during training and implement early stopping to prevent overfitting and optimize computational resource usage.</p>
<h2 id="advanced-techniques">Advanced Techniques</h2>
<p>Bidirectional LSTMs: Process sequences in both forward and backward directions to capture dependencies from both past and future contexts, improving performance on tasks where full sequence context is available.Attention Mechanisms: Integrate attention layers with LSTM networks to allow the model to focus on relevant parts of the input sequence, improving performance on long sequences and alignment tasks.Stacked LSTM Architectures: Build deep LSTM networks by stacking multiple LSTM layers to increase model capacity and ability to learn hierarchical temporal representations.Peephole Connections: Enhance standard LSTM architecture by allowing gates to access cell state information directly, providing more precise control over information flow.LSTM Autoencoders: Implement encoder-decoder architectures using LSTMs for sequence-to-sequence learning, dimensionality reduction, and anomaly detection in temporal data.Convolutional LSTM: Combine convolutional operations with LSTM cells to process spatiotemporal data like video sequences, maintaining both spatial and temporal relationships.</p>
<h2 id="future-directions">Future Directions</h2>
<p>Transformer Integration: Hybrid architectures combining LSTM networks with transformer attention mechanisms to leverage the strengths of both sequential processing and parallel attention computation.Neuromorphic Computing: Development of LSTM implementations on neuromorphic hardware platforms for energy-efficient processing of sequential data in edge computing applications.Quantum LSTM Networks: Research into quantum computing implementations of LSTM architectures that could potentially offer exponential speedups for certain sequential processing tasks.Continual Learning: Advanced LSTM architectures capable of learning new tasks without forgetting previously learned information, enabling lifelong learning in dynamic environments.Sparse LSTM Architectures: Development of sparse and pruned LSTM networks that maintain performance while significantly reducing computational requirements and memory usage.Meta-Learning Applications: LSTM networks designed for meta-learning scenarios where the model learns to quickly adapt to new sequential tasks with minimal training data.</p>
<h2 id="references">References</h2>
<ol>
<li>
<p>Hochreiter, S., &amp; Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735-1780.</p>
</li>
<li>
<p>Graves, A. (2013). Generating sequences with recurrent <a data-lb="1" href="/en/glossary/neural-networks/" title="Neural Networks glossary entry">neural networks</a>. arXiv preprint arXiv:1308.0850.</p>
</li>
<li>
<p>Cho, K., et al. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078.</p>
</li>
<li>
<p>Greff, K., et al. (2017). LSTM: A search space odyssey. IEEE transactions on neural networks and learning systems, 28(10), 2222-2232.</p>
</li>
<li>
<p>Olah, C. (2015). Understanding LSTM Networks. Retrieved from <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="nofollow noopener noreferrer" target="_blank">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p>
</li>
<li>
<p>Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). Deep learning. MIT press.</p>
</li>
<li>
<p>Pascanu, R., Mikolov, T., &amp; Bengio, Y. (2013). On the difficulty of training recurrent neural networks. International conference on machine learning.</p>
</li>
<li>
<p>Jozefowicz, R., Zaremba, W., &amp; Sutskever, I. (2015). An empirical exploration of recurrent network architectures. International conference on machine learning.</p>
</li>
</ol>
</div>
<div class="mt-16 sm:mt-20 border-t border-gray-200 dark:border-gray-800 pt-12 sm:pt-16">
<h2 class="text-2xl sm:text-3xl font-bold tracking-tight text-gray-900 dark:text-white mb-8 sm:mb-10">
        
          Related Terms
        
      </h2>
<div class="grid gap-6 sm:gap-8 grid-cols-1 sm:grid-cols-2 lg:grid-cols-3">
<article class="group relative flex flex-col overflow-hidden border border-gray-200 dark:border-gray-700 bg-white dark:bg-gray-900 hover:border-gray-400 dark:hover:border-gray-500 transition-all duration-300">
<div class="flex flex-1 flex-col p-5 sm:p-6">
<h3 class="text-base sm:text-lg font-semibold text-gray-900 dark:text-white line-clamp-2 mb-3">
<a class="hover:text-gray-600 dark:hover:text-gray-300 transition-colors" href="https://main.d1jtfhinlastnr.amplifyapp.com/en/glossary/demand-forecasting/">
                    Demand Forecasting
                  </a>
</h3>
<p class="text-sm text-gray-600 dark:text-gray-400 flex-1 line-clamp-3">
                    A process that uses past sales data and AI to predict what customers will buy in the future, helping...
                  </p>
<div class="mt-4 pt-4 border-t border-gray-100 dark:border-gray-800">
<a class="inline-flex items-center text-sm font-medium text-gray-900 dark:text-gray-100 group-hover:translate-x-1 transition-transform" href="https://main.d1jtfhinlastnr.amplifyapp.com/en/glossary/demand-forecasting/">
                    View details
                    <svg class="ml-1.5 h-4 w-4" fill="none" stroke="currentColor" viewbox="0 0 24 24">
<path d="M9 5l7 7-7 7" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"></path>
</svg>
</a>
</div>
</div>
</article>
</div>
</div>
<div class="mt-12 sm:mt-16 py-8 border-t border-gray-200 dark:border-gray-800">
<a class="inline-flex items-center text-sm font-medium text-gray-600 dark:text-gray-400 hover:text-gray-900 dark:hover:text-gray-100 transition-colors" href="/en/glossary/">
<svg class="mr-2 h-5 w-5" fill="none" stroke="currentColor" viewbox="0 0 24 24">
<path d="M10 19l-7-7m0 0l7-7m-7 7h18" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"></path>
</svg>
      
        Back to Glossary
      
    </a>
</div>
</article>
</main>
<footer style="background-color: #000000;">
<div id="cta-curves-container" style="
    position: relative;
    background: #000000;
    padding: 4rem 2rem;
    overflow: hidden;
  ">
<svg id="cta-curves-svg" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; z-index: 1; opacity: 0.7;" xmlns="http://www.w3.org/2000/svg">
<defs>
<lineargradient id="curveGrad1" x1="0%" x2="100%" y1="0%" y2="0%">
<stop offset="0%" style="stop-color:rgba(99, 102, 241, 0); stop-opacity:0"></stop>
<stop offset="50%" style="stop-color:rgba(99, 102, 241, 0.9); stop-opacity:1"></stop>
<stop offset="100%" style="stop-color:rgba(99, 102, 241, 0); stop-opacity:0"></stop>
</lineargradient>
<lineargradient id="curveGrad2" x1="0%" x2="100%" y1="0%" y2="0%">
<stop offset="0%" style="stop-color:rgba(139, 92, 246, 0); stop-opacity:0"></stop>
<stop offset="50%" style="stop-color:rgba(139, 92, 246, 0.8); stop-opacity:1"></stop>
<stop offset="100%" style="stop-color:rgba(139, 92, 246, 0); stop-opacity:0"></stop>
</lineargradient>
<lineargradient id="curveGrad3" x1="0%" x2="100%" y1="0%" y2="0%">
<stop offset="0%" style="stop-color:rgba(59, 130, 246, 0); stop-opacity:0"></stop>
<stop offset="50%" style="stop-color:rgba(59, 130, 246, 0.7); stop-opacity:1"></stop>
<stop offset="100%" style="stop-color:rgba(59, 130, 246, 0); stop-opacity:0"></stop>
</lineargradient>
</defs>
<path class="curve" data-speed="0.8" fill="none" stroke="url(#curveGrad1)" stroke-width="2"></path>
<path class="curve" data-speed="1.2" fill="none" opacity="0.8" stroke="url(#curveGrad2)" stroke-width="2.5"></path>
<path class="curve" data-speed="0.6" fill="none" opacity="0.6" stroke="url(#curveGrad3)" stroke-width="1.5"></path>
<path class="curve" data-speed="1.5" fill="none" opacity="0.7" stroke="url(#curveGrad1)" stroke-width="2"></path>
<path class="curve" data-speed="0.9" fill="none" opacity="0.5" stroke="url(#curveGrad2)" stroke-width="3"></path>
<path class="curve" data-speed="1.3" fill="none" opacity="0.9" stroke="url(#curveGrad3)" stroke-width="1.8"></path>
<path class="curve" data-speed="0.7" fill="none" opacity="0.6" stroke="url(#curveGrad1)" stroke-width="2.2"></path>
<path class="curve" data-speed="1.1" fill="none" opacity="0.8" stroke="url(#curveGrad2)" stroke-width="2"></path>
<path class="curve" data-speed="1.4" fill="none" opacity="0.5" stroke="url(#curveGrad3)" stroke-width="2.5"></path>
<path class="curve" data-speed="0.85" fill="none" opacity="0.7" stroke="url(#curveGrad1)" stroke-width="1.5"></path>
<path class="curve" data-speed="1.0" fill="none" opacity="0.6" stroke="url(#curveGrad2)" stroke-width="2.8"></path>
<path class="curve" data-speed="1.25" fill="none" opacity="0.8" stroke="url(#curveGrad3)" stroke-width="2"></path>
<path class="curve" data-speed="0.95" fill="none" opacity="0.5" stroke="url(#curveGrad1)" stroke-width="2.3"></path>
<path class="curve" data-speed="1.35" fill="none" opacity="0.9" stroke="url(#curveGrad2)" stroke-width="1.7"></path>
<path class="curve" data-speed="0.75" fill="none" opacity="0.7" stroke="url(#curveGrad3)" stroke-width="2.5"></path>
</svg>
<div style="position: absolute; top: 0; left: 0; width: 50px; height: 50px; border-left: 2px solid rgba(99, 102, 241, 0.3); border-top: 2px solid rgba(99, 102, 241, 0.3);"></div>
<div style="position: absolute; top: 0; right: 0; width: 50px; height: 50px; border-right: 2px solid rgba(99, 102, 241, 0.3); border-top: 2px solid rgba(99, 102, 241, 0.3);"></div>
<div style="position: absolute; bottom: 0; left: 0; width: 50px; height: 50px; border-left: 2px solid rgba(99, 102, 241, 0.3); border-bottom: 2px solid rgba(99, 102, 241, 0.3);"></div>
<div style="position: absolute; bottom: 0; right: 0; width: 50px; height: 50px; border-right: 2px solid rgba(99, 102, 241, 0.3); border-bottom: 2px solid rgba(99, 102, 241, 0.3);"></div>
<div class="mx-auto max-w-2xl text-center" style="position: relative; z-index: 10;">
<h2 class="text-base/7 font-semibold text-indigo-400">Ready to get started?</h2>
<p class="mt-2 text-4xl font-semibold tracking-tight text-balance text-white sm:text-5xl">Start using our services today</p>
<p class="mx-auto mt-6 max-w-xl text-lg/8 text-pretty text-gray-400">Join thousands of satisfied customers who have transformed their business with our solutions.</p>
<div class="mt-8 flex justify-center">
<a class="rounded-md bg-indigo-500 px-3.5 py-2.5 text-sm font-semibold text-white shadow-xs hover:bg-indigo-400 focus-visible:outline-2 focus-visible:outline-offset-2 focus-visible:outline-indigo-400" href="/en/blog/">Get started</a>
</div>
</div>
</div>
<script>
  (function() {
    'use strict';
    
    const container = document.getElementById('cta-curves-container');
    const svg = document.getElementById('cta-curves-svg');
    if (!container || !svg) return;
    
    const curves = svg.querySelectorAll('.curve');
    let mouseX = 0;
    let mouseY = 0;
    let targetMouseX = 0;
    let targetMouseY = 0;
    const mouseInfluence = 150;
    const mouseStrength = 80;
    
    const curveData = Array.from(curves).map((curve, i) => {
      const speed = parseFloat(curve.getAttribute('data-speed')) || 1;
      return {
        element: curve,
        baseY: 50 + (i * 25),
        offset: Math.random() * Math.PI * 2,
        speed: speed,
        amplitude: 30 + Math.random() * 40
      };
    });
    
    container.addEventListener('mousemove', (e) => {
      const rect = container.getBoundingClientRect();
      targetMouseX = e.clientX - rect.left;
      targetMouseY = e.clientY - rect.top;
    });
    
    container.addEventListener('mouseleave', () => {
      targetMouseX = -1000;
      targetMouseY = -1000;
    });
    
    let time = 0;
    
    function animate() {
      time += 0.01;
      
      mouseX += (targetMouseX - mouseX) * 0.15;
      mouseY += (targetMouseY - mouseY) * 0.15;
      
      const width = svg.clientWidth;
      const height = svg.clientHeight;
      
      curveData.forEach((data, index) => {
        const { baseY, offset, speed, amplitude } = data;
        const phase = time * speed + offset;
        
        let path = `M -200,${baseY}`;
        
        for (let x = -200; x <= width + 200; x += 50) {
          const normalY = baseY + Math.sin((x * 0.005) + phase) * amplitude;
          
          const dx = x - mouseX;
          const dy = normalY - mouseY;
          const distance = Math.sqrt(dx * dx + dy * dy);
          
          let y = normalY;
          if (distance < mouseInfluence) {
            const influence = (1 - distance / mouseInfluence);
            const pushY = (normalY - mouseY) * influence * mouseStrength * 0.01;
            y = normalY + pushY;
          }
          
          const nextX = x + 50;
          const controlX = x + 25;
          const controlY = y;
          path += ` Q ${controlX},${controlY} ${nextX},${y}`;
        }
        
        data.element.setAttribute('d', path);
      });
      
      requestAnimationFrame(animate);
    }
    
    animate();
  })();
  </script>
<div class="mx-auto max-w-7xl px-6 py-16 sm:py-24 lg:px-8 lg:py-32">
<div class="mt-24 border-t border-white/10 pt-12 xl:grid xl:grid-cols-3 xl:gap-8">
<picture class="lazy-picture" data-maxwidth="200">
<source data-original-src="/images/interwork-logo-white-1.webp" data-srcset="/images/interwork-logo-white-1.webp 568w" sizes="200px" type="image/webp">
<img alt="Interwork" class="lazy-image h-9" data-original-src="/images/interwork-logo-white-1.webp" data-src="/images/interwork-logo-white-1.webp" decoding="async" loading="lazy"/>
</source></picture>
<div class="mt-16 grid grid-cols-2 gap-8 xl:col-span-2 xl:mt-0">
<div class="md:grid md:grid-cols-2 md:gap-8">
<div>
<h3 class="text-sm/6 font-semibold text-white">Services</h3>
<ul class="mt-6 space-y-4" role="list">
<li>
<a class="text-sm/6 text-gray-400 hover:text-white" href="#">AI Solutions</a>
</li>
<li>
<a class="text-sm/6 text-gray-400 hover:text-white" href="#">Web Development</a>
</li>
<li>
<a class="text-sm/6 text-gray-400 hover:text-white" href="#">System Development</a>
</li>
<li>
<a class="text-sm/6 text-gray-400 hover:text-white" href="#">Consulting</a>
</li>
</ul>
</div>
<div class="mt-10 md:mt-0">
<h3 class="text-sm/6 font-semibold text-white">Support</h3>
<ul class="mt-6 space-y-4" role="list">
<li>
<a class="text-sm/6 text-gray-400 hover:text-white" href="https://support.smartweb.jp/">Support Portal</a>
</li>
<li>
<a class="text-sm/6 text-gray-400 hover:text-white" href="#">Documentation</a>
</li>
</ul>
</div>
</div>
<div class="md:grid md:grid-cols-2 md:gap-8">
<div>
<h3 class="text-sm/6 font-semibold text-white">Company</h3>
<ul class="mt-6 space-y-4" role="list">
<li>
<a class="text-sm/6 text-gray-400 hover:text-white" href="https://www.intwk.co.jp/about/">About</a>
</li>
<li>
<a class="text-sm/6 text-gray-400 hover:text-white" href="/en/blog/">Blog</a>
</li>
<li>
<a class="text-sm/6 text-gray-400 hover:text-white" href="#">Careers</a>
</li>
<li>
<a class="text-sm/6 text-gray-400 hover:text-white" href="#">News</a>
</li>
</ul>
</div>
<div class="mt-10 md:mt-0">
<h3 class="text-sm/6 font-semibold text-white">Legal</h3>
<ul class="mt-6 space-y-4" role="list">
<li>
<a class="text-sm/6 text-gray-400 hover:text-white" href="/en/privacy-policy/">Privacy Policy</a>
</li>
<li>
<a class="text-sm/6 text-gray-400 hover:text-white" href="/en/ai-chatbot-terms-of-use/">AI Chatbot Terms of Use</a>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="language-selector mt-12 border-t border-white/10 pt-8 md:flex md:items-center md:justify-between">
<div>
<p class="text-sm/6 font-semibold text-white mb-4">Available Languages</p>
<div class="language-selector">
<div class="flex flex-wrap items-center justify-center gap-3">
<a aria-label="日本語" class="inline-flex items-center text-sm hover:opacity-75 transition-opacity" href="/ja/glossary/long-short-term-memory--lstm-/" hreflang="ja" title="日本語">
<img alt="日本語" class="rounded" height="18" src="/flags/jp.png" width="24"/>
</a>
<span class="inline-flex items-center text-sm opacity-50" title="English">
<img alt="English" class="rounded" height="18" src="/flags/gb.png" width="24"/>
</span>
</div>
</div>
</div>
</div>
<div class="mt-12 border-t border-white/10 pt-8 md:flex md:items-center md:justify-between">
<div class="flex gap-x-6 md:order-2">
<a class="text-gray-400 hover:text-gray-300" href="https://github.com">
<span class="sr-only">GitHub</span>
</a>
<a class="text-gray-400 hover:text-gray-300" href="https://x.com">
<span class="sr-only">X</span>
</a>
<a class="text-gray-400 hover:text-gray-300" href="https://youtube.com">
<span class="sr-only">YouTube</span>
</a>
</div>
<p class="mt-8 text-sm/6 text-gray-400 md:mt-0" style="text-align: center; animation: copyrightGlow 3s ease-in-out infinite;">© 2026 Interwork Corporation All rights reserved.</p>
</div>
</div>
<style>
    @keyframes copyrightGlow {
      0%, 100% {
        opacity: 0.6;
        text-shadow: 0 0 10px rgba(99, 102, 241, 0);
      }
      50% {
        opacity: 1;
        text-shadow: 0 0 20px rgba(99, 102, 241, 0.5), 0 0 30px rgba(99, 102, 241, 0.3);
      }
    }
  </style>
</footer>
<div class="pointer-events-none fixed inset-x-0 bottom-0 px-6 pb-6 z-50 dark" data-cookie-consent-banner="" id="cookie-consent-banner">
<div class="pointer-events-auto max-w-xl rounded-xl section-bg-light dark:section-bg-dark p-6 ring-1 shadow-lg ring-gray-900/10">
<p class="text-secondary text-sm/6"><strong class="text-heading text-md mb-4 font-semibold">Cookie Consent</strong><br/> We use cookies to enhance your browsing experience and analyze our traffic. See our <a class="font-semibold text-primary hover:text-primary-500" href="/en/privacy-policy/">privacy policy</a>.</p>
<div class="mt-4 flex items-center gap-x-3 flex-wrap">
<a aria-label="Accept All" class="btn-primary dark:btn-primary-dark px-3 py-2 text-sm not-prose group" data-cookie-consent="accept-all" href="#" target="_self">
      Accept All
      
      
    </a>
<a aria-label="Reject All" class="btn-secondary dark:btn-secondary-dark px-3 py-2 text-sm not-prose group" data-cookie-consent="accept-necessary" href="#" target="_self">
      Reject All
      
      
    </a>
<a aria-label="Cookie Settings" class="btn-text dark:btn-text-dark px-3 py-2 text-sm not-prose group" data-cookie-consent="settings" href="#" target="_self">
      Cookie Settings
      
      
    </a>
</div>
</div>
</div>
<div class="fixed inset-0 z-50 hidden dark" id="cookie-settings-modal">
<div class="absolute inset-0 bg-black bg-opacity-50" data-cookie-settings-close=""></div>
<div class="relative mx-auto max-w-xl p-4 sm:p-6 section-bg-light dark:section-bg-dark rounded-xl shadow-xl mt-20">
<div class="flex justify-between items-center mb-4">
<h2 class="text-heading text-xl font-bold">Cookie Settings</h2>
<button class="text-gray-400 hover:text-gray-500 dark:text-gray-300 dark:hover:text-white" data-cookie-settings-close="" type="button">
<span class="sr-only">Close</span>
<svg class="h-6 w-6" fill="none" stroke="currentColor" viewbox="0 0 24 24">
<path d="M6 18L18 6M6 6l12 12" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"></path>
</svg>
</button>
</div>
<div class="space-y-4">
<div class="border-gray-200 dark:border-gray-700 border-b pb-4">
<div class="flex items-center justify-between">
<div>
<h3 class="text-heading text-lg font-medium">Necessary Cookies</h3>
<p class="text-tertiary text-sm">These cookies are required for the website to function and cannot be disabled.</p>
</div>
<div class="ml-3 flex h-5 items-center">
<input checked="" class="h-4 w-4 rounded-xl border-gray-300 text-primary focus:ring-primary dark:border-gray-600 dark:bg-gray-700" disabled="" id="necessary-cookies" name="necessary-cookies" type="checkbox"/>
</div>
</div>
</div>
<div class="border-gray-200 dark:border-gray-700 border-b pb-4">
<div class="flex items-center justify-between">
<div>
<h3 class="text-heading text-lg font-medium">Analytics Cookies</h3>
<p class="text-tertiary text-sm">These cookies help us understand how visitors interact with our website.</p>
</div>
<div class="ml-3 flex h-5 items-center">
<input class="h-4 w-4 rounded-xl border-gray-300 text-primary focus:ring-primary dark:border-gray-600 dark:bg-gray-700" id="analytics-cookies" name="analytics-cookies" type="checkbox"/>
</div>
</div>
</div>
</div>
<div class="mt-6 flex justify-end gap-x-3">
<a aria-label="Cancel" class="btn-secondary dark:btn-secondary-dark px-3 py-2 not-prose group" data-cookie-settings-close="" href="#" target="_self">
      Cancel
      
      
    </a>
<a aria-label="Save Preferences" class="btn-primary dark:btn-primary-dark px-3 py-2 not-prose group" data-cookie-settings-save="" href="#" target="_self">
      Save Preferences
      
      
    </a>
</div>
</div>
</div>
<button aria-label="Back to Top" class="fixed bottom-8 right-8 z-[100] p-3 rounded-full bg-indigo-600 text-white shadow-lg transition-all duration-300 transform translate-y-12 opacity-0 invisible hover:bg-indigo-700 hover:shadow-xl focus:outline-none focus:ring-2 focus:ring-indigo-500 focus:ring-offset-2 dark:bg-indigo-500 dark:hover:bg-indigo-400" id="back-to-top-btn" onclick="window.scrollTo({top: 0, behavior: 'smooth'});">
<svg class="h-6 w-6" fill="none" stroke="currentColor" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
<path d="M5 10l7-7m0 0l7 7m-7-7v18" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"></path>
</svg>
</button>
<script>
document.addEventListener('DOMContentLoaded', () => {
  const backToTopBtn = document.getElementById('back-to-top-btn');
  if (!backToTopBtn) return;

  const scrollThreshold = 300;

  const toggleVisibility = () => {
    if (window.scrollY > scrollThreshold) {
      backToTopBtn.classList.remove('translate-y-12', 'opacity-0', 'invisible');
      backToTopBtn.classList.add('translate-y-0', 'opacity-100', 'visible');
    } else {
      backToTopBtn.classList.remove('translate-y-0', 'opacity-100', 'visible');
      backToTopBtn.classList.add('translate-y-12', 'opacity-0', 'invisible');
    }
  };

  let ticking = false;
  window.addEventListener('scroll', () => {
    if (!ticking) {
      window.requestAnimationFrame(() => {
        toggleVisibility();
        ticking = false;
      });
      ticking = true;
    }
  });
});
</script>
<script src="/js/app.js?v=20260111165525"></script>
</body>
</html>