# Hugo-Boilerplate ã‚¹ã‚¯ãƒªãƒ—ãƒˆè©³ç´°ä»•æ§˜æ›¸

## ğŸ“‹ ç›®æ¬¡

1. [æ¦‚è¦](#1-æ¦‚è¦)
2. [ãƒ“ãƒ«ãƒ‰ãƒ—ãƒ­ã‚»ã‚¹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ](#2-ãƒ“ãƒ«ãƒ‰ãƒ—ãƒ­ã‚»ã‚¹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ)
3. [ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ](#3-ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ)
4. [ç¿»è¨³é–¢é€£ã‚¹ã‚¯ãƒªãƒ—ãƒˆ](#4-ç¿»è¨³é–¢é€£ã‚¹ã‚¯ãƒªãƒ—ãƒˆ)
5. [ãƒªãƒ³ã‚¯ãƒ“ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ](#5-ãƒªãƒ³ã‚¯ãƒ“ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ)
6. [ç”»åƒå‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ](#6-ç”»åƒå‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ)
7. [ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¹ã‚¯ãƒªãƒ—ãƒˆ](#7-ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¹ã‚¯ãƒªãƒ—ãƒˆ)

---

## 1. æ¦‚è¦

### ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 

```
scripts/
â”œâ”€â”€ build_content.sh              # ãƒ¡ã‚¤ãƒ³ãƒ“ãƒ«ãƒ‰ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”œâ”€â”€ convert_tooltip_syntax.py     # ãƒ„ãƒ¼ãƒ«ãƒãƒƒãƒ—æ§‹æ–‡å¤‰æ›
â”œâ”€â”€ extract_automatic_links.py    # è‡ªå‹•ãƒªãƒ³ã‚¯æŠ½å‡º
â”œâ”€â”€ find_duplicate_images.py      # é‡è¤‡ç”»åƒæ¤œå‡º
â”œâ”€â”€ generate_amplify_redirects_file.py # Amplifyãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆç”Ÿæˆ
â”œâ”€â”€ generate_content.py           # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ
â”œâ”€â”€ generate_linkbuilding_keywords.py # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç”Ÿæˆ
â”œâ”€â”€ generate_related_content.py   # é–¢é€£ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ
â”œâ”€â”€ linkbuilding.py              # ãƒªãƒ³ã‚¯ãƒ“ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆåŸºæœ¬ï¼‰
â”œâ”€â”€ linkbuilding_parallel.py     # ãƒªãƒ³ã‚¯ãƒ“ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆä¸¦åˆ—ï¼‰
â”œâ”€â”€ linkbuilding_config.json     # ãƒªãƒ³ã‚¯ãƒ“ãƒ«ãƒ‡ã‚£ãƒ³ã‚°è¨­å®š
â”œâ”€â”€ offload_replicate_images.py  # ç”»åƒã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰
â”œâ”€â”€ precompute_linkbuilding.py   # ãƒªãƒ³ã‚¯ãƒ“ãƒ«ãƒ‡ã‚£ãƒ³ã‚°æœ€é©åŒ–
â”œâ”€â”€ preprocess-images.sh         # ç”»åƒå‰å‡¦ç†
â”œâ”€â”€ requirements.txt             # Pythonä¾å­˜é–¢ä¿‚
â”œâ”€â”€ sync_content_attributes.py   # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å±æ€§åŒæœŸ
â”œâ”€â”€ sync_translation_urls.py     # ç¿»è¨³URLåŒæœŸ
â”œâ”€â”€ sync_translations.py         # ç¿»è¨³åŒæœŸ
â”œâ”€â”€ translate_with_flowhunt.py   # FlowHuntç¿»è¨³
â”œâ”€â”€ translation-urls.py          # ç¿»è¨³URLãƒãƒƒãƒ”ãƒ³ã‚°
â””â”€â”€ validate_tables.py          # ãƒ†ãƒ¼ãƒ–ãƒ«æ¤œè¨¼
```

### Pythonä¾å­˜é–¢ä¿‚ (requirements.txt)

```
beautifulsoup4==4.12.2
flowhunt==0.1.0
lxml==4.9.3
Pillow==10.0.0
python-dotenv==1.0.0
pyyaml==6.0.1
requests==2.31.0
tqdm==4.66.1
```

---

## 2. ãƒ“ãƒ«ãƒ‰ãƒ—ãƒ­ã‚»ã‚¹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

### 2.1 build_content.sh

**ç›®çš„**: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ“ãƒ«ãƒ‰ãƒ—ãƒ­ã‚»ã‚¹å…¨ä½“ã‚’ç®¡ç†ã™ã‚‹ãƒã‚¹ã‚¿ãƒ¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

**ä¸»è¦æ©Ÿèƒ½:**
```bash
#!/bin/bash
set -e  # ã‚¨ãƒ©ãƒ¼æ™‚ã«å³åº§ã«çµ‚äº†

# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š
SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
THEME_DIR="$(dirname "$SCRIPT_DIR")"
HUGO_ROOT="$(dirname "$(dirname "$THEME_DIR")")"

# Pythonä»®æƒ³ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
VENV_DIR="${SCRIPT_DIR}/.venv"
if [ ! -d "$VENV_DIR" ]; then
    python3 -m venv "$VENV_DIR"
    NEED_INSTALL=true
fi
source "${VENV_DIR}/bin/activate"
```

**å®Ÿè¡Œå¯èƒ½ã‚¹ãƒ†ãƒƒãƒ—:**

| ã‚¹ãƒ†ãƒƒãƒ—å | èª¬æ˜ | ä¾å­˜é–¢ä¿‚ |
|---------|------|---------|
| sync_translations | ç¿»è¨³ã‚­ãƒ¼åŒæœŸ | ãªã— |
| build_hugo | Hugoãƒ“ãƒ«ãƒ‰ | sync_translations |
| offload_images | ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ | ãªã— |
| find_duplicate_images | é‡è¤‡ç”»åƒæ¤œå‡º | offload_images |
| translate | FlowHuntç¿»è¨³ | ãªã— |
| sync_content_attributes | å±æ€§åŒæœŸ | translate |
| sync_translation_urls | URLåŒæœŸ | sync_content_attributes |
| generate_amplify_redirects | ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆç”Ÿæˆ | sync_translation_urls |
| generate_linkbuilding_keywords | ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç”Ÿæˆ | build_hugo |
| generate_related_content | é–¢é€£ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ | build_hugo |
| extract_automatic_links | ãƒªãƒ³ã‚¯æŠ½å‡º | ãªã— |
| precompute_linkbuilding | ãƒªãƒ³ã‚¯æœ€é©åŒ– | build_hugo |
| apply_linkbuilding | ãƒªãƒ³ã‚¯é©ç”¨ | precompute_linkbuilding |
| preprocess_images | ç”»åƒæœ€é©åŒ– | ãªã— |

**å®Ÿè¡Œä¾‹:**
```bash
# ãƒ•ãƒ«ãƒ“ãƒ«ãƒ‰
./scripts/build_content.sh

# ç‰¹å®šã‚¹ãƒ†ãƒƒãƒ—ã®ã¿
./scripts/build_content.sh --step translate
./scripts/build_content.sh --step build_hugo,apply_linkbuilding

# ä¸¦åˆ—å®Ÿè¡Œï¼ˆè¨€èªã”ã¨ï¼‰
./scripts/build_content.sh --step generate_linkbuilding_keywords
```

---

## 3. ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

### 3.1 convert_tooltip_syntax.py

**ç›®çš„**: æ—§å½¢å¼ã®ãƒ„ãƒ¼ãƒ«ãƒãƒƒãƒ—æ§‹æ–‡ã‚’æ–°ã—ã„Hugoã‚·ãƒ§ãƒ¼ãƒˆã‚³ãƒ¼ãƒ‰å½¢å¼ã«å¤‰æ›

**ã‚¯ãƒ©ã‚¹æ§‹é€ :**
```python
class TooltipConverter:
    def __init__(self, create_backup=True, verbose=True):
        self.create_backup = create_backup
        self.verbose = verbose
        self.stats = {
            'files_processed': 0,
            'files_modified': 0,
            'tooltips_converted': 0,
            'errors': 0
        }
    
    def convert_text(self, text):
        """ãƒ†ã‚­ã‚¹ãƒˆå†…ã®ãƒ„ãƒ¼ãƒ«ãƒãƒƒãƒ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ã‚·ãƒ§ãƒ¼ãƒˆã‚³ãƒ¼ãƒ‰ã«å¤‰æ›"""
        # ãƒ‘ã‚¿ãƒ¼ãƒ³: **ç”¨èª**ï¼ˆèª¬æ˜ï¼šè©³ç´°ï¼‰
        PATTERN_BOLD = r'\*\*([^*]+)\*\*[ï¼ˆ(]èª¬æ˜[ï¼š:]\s*([^)ï¼‰]+)[)ï¼‰]'
        # ãƒ‘ã‚¿ãƒ¼ãƒ³: ç”¨èªï¼ˆèª¬æ˜ï¼šè©³ç´°ï¼‰
        PATTERN_NORMAL = r'([^\sï¼ˆ(]+)[ï¼ˆ(]èª¬æ˜[ï¼š:]\s*([^)ï¼‰]+)[)ï¼‰]'
        
        # ã‚·ãƒ§ãƒ¼ãƒˆã‚³ãƒ¼ãƒ‰ã«å¤‰æ›
        # {{< tooltip text="è©³ç´°" >}}ç”¨èª{{< /tooltip >}}
```

**å‡¦ç†ãƒ•ãƒ­ãƒ¼:**
1. ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ï¼ˆUTF-8ï¼‰
2. ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ï¼ˆæ­£è¦è¡¨ç¾ï¼‰
3. ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—å‡¦ç†ï¼ˆ`"` â†’ `&quot;`ï¼‰
4. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆï¼ˆ.bakï¼‰
5. å¤‰æ›å¾Œãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜

### 3.2 validate_tables.py

**ç›®çš„**: Markdownãƒ†ãƒ¼ãƒ–ãƒ«ã®æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã‚’æ¤œå‡ºãƒ»ä¿®æ­£

**æ¤œå‡ºã™ã‚‹å•é¡Œ:**
```python
def check_table_headers(content):
    """ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ˜ãƒƒãƒ€ãƒ¼ã®å•é¡Œã‚’æ¤œå‡º"""
    problems = []
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã®å¾Œã«åŒºåˆ‡ã‚Šç·šãŒãªã„
    # | ãƒ˜ãƒƒãƒ€ãƒ¼1 | ãƒ˜ãƒƒãƒ€ãƒ¼2 |
    # ï¼ˆåŒºåˆ‡ã‚Šç·šãªã—ï¼‰
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ã‚»ãƒ«å†…ã®æ”¹è¡Œ
    # | è¤‡æ•°è¡Œ\nãƒ†ã‚­ã‚¹ãƒˆ |
    
    return problems

def fix_table_headers(content):
    """ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ä¿®æ­£"""
    # åŒºåˆ‡ã‚Šç·šã®è‡ªå‹•ç”Ÿæˆ
    # | col1 | col2 | col3 |
    # |------|------|------|  â† è‡ªå‹•ç”Ÿæˆ
```

### 3.3 generate_related_content.py

**ç›®çš„**: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®é–¢é€£æ€§ã‚’åˆ†æã—ã€é–¢é€£è¨˜äº‹ã®ãƒªãƒ³ã‚¯ã‚’ç”Ÿæˆ

**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ :**
```python
def calculate_similarity(content1, content2):
    """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—"""
    # 1. TF-IDF ãƒ™ã‚¯ãƒˆãƒ«åŒ–
    # 2. ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
    # 3. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
    # 4. ã‚«ãƒ†ã‚´ãƒªãƒ»ã‚¿ã‚°é‡ã¿ä»˜ã‘
    
def find_related_content(target_file, all_files, max_related=5):
    """é–¢é€£ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ¤œç´¢"""
    similarities = []
    for file in all_files:
        if file != target_file:
            score = calculate_similarity(target_file, file)
            similarities.append((file, score))
    
    # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½ã‚’è¿”ã™
    return sorted(similarities, key=lambda x: x[1], reverse=True)[:max_related]
```

---

## 4. ç¿»è¨³é–¢é€£ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

### 4.1 translate_with_flowhunt.py

**ç›®çš„**: FlowHunt APIã‚’ä½¿ç”¨ã—ãŸè‡ªå‹•å¤šè¨€èªç¿»è¨³

**APIçµ±åˆ:**
```python
import flowhunt
from dotenv import load_dotenv

class FlowHuntTranslator:
    def __init__(self):
        load_dotenv()
        self.api_key = os.getenv('FLOWHUNT_API_KEY')
        self.flow_id = "flowhunt-flow-translate"
        self.client = flowhunt.Client(api_key=self.api_key)
    
    def translate_content(self, content, source_lang='en', target_lang='ja'):
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç¿»è¨³"""
        # ãƒ•ãƒ­ãƒ³ãƒˆãƒã‚¿ãƒ¼ã¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’åˆ†é›¢
        frontmatter, body = self.split_content(content)
        
        # ãƒ•ãƒ­ãƒ³ãƒˆãƒã‚¿ãƒ¼ã®é¸æŠçš„ç¿»è¨³
        translated_frontmatter = self.translate_frontmatter(
            frontmatter, target_lang
        )
        
        # æœ¬æ–‡ã®ç¿»è¨³ï¼ˆã‚·ãƒ§ãƒ¼ãƒˆã‚³ãƒ¼ãƒ‰ä¿è­·ï¼‰
        translated_body = self.translate_body_safe(body, target_lang)
        
        return self.merge_content(translated_frontmatter, translated_body)
```

**ã‚µãƒãƒ¼ãƒˆè¨€èª:**
```python
SUPPORTED_LANGUAGES = {
    'ja': 'Japanese',
    'zh': 'Chinese',
    'ko': 'Korean',
    'es': 'Spanish',
    'fr': 'French',
    'de': 'German',
    'it': 'Italian',
    'pt': 'Portuguese',
    'ru': 'Russian',
    'ar': 'Arabic',
    # ... 20+ languages
}
```

### 4.2 sync_translations.py

**ç›®çš„**: ç¿»è¨³ãƒ•ã‚¡ã‚¤ãƒ«é–“ã§ã‚­ãƒ¼ã®ä¸€è²«æ€§ã‚’ä¿è¨¼

```python
def sync_translation_keys():
    """ã™ã¹ã¦ã®è¨€èªãƒ•ã‚¡ã‚¤ãƒ«ã§ç¿»è¨³ã‚­ãƒ¼ã‚’åŒæœŸ"""
    languages = ['ja', 'en', 'zh', 'ko', ...]
    
    # ãƒã‚¹ã‚¿ãƒ¼ã‚­ãƒ¼ã‚»ãƒƒãƒˆã‚’ä½œæˆ
    all_keys = set()
    for lang in languages:
        keys = extract_keys(f'i18n/{lang}.yaml')
        all_keys.update(keys)
    
    # å„è¨€èªãƒ•ã‚¡ã‚¤ãƒ«ã«æ¬ è½ã‚­ãƒ¼ã‚’è¿½åŠ 
    for lang in languages:
        add_missing_keys(f'i18n/{lang}.yaml', all_keys)
```

### 4.3 sync_translation_urls.py

**ç›®çš„**: å¤šè¨€èªã‚µã‚¤ãƒˆã®URLæ§‹é€ ã‚’åŒæœŸ

```python
def generate_translation_urls(hugo_root):
    """ç¿»è¨³URL ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ç”Ÿæˆ"""
    url_mapping = {}
    
    for content_file in find_content_files(hugo_root):
        translation_key = extract_translation_key(content_file)
        if translation_key:
            lang = extract_language(content_file)
            url = generate_url(content_file, lang)
            
            if translation_key not in url_mapping:
                url_mapping[translation_key] = {}
            url_mapping[translation_key][lang] = url
    
    return url_mapping
```

---

## 5. ãƒªãƒ³ã‚¯ãƒ“ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

### 5.1 linkbuilding.py

**ç›®çš„**: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å†…ã«è‡ªå‹•çš„ã«å†…éƒ¨ãƒªãƒ³ã‚¯ã‚’æŒ¿å…¥

**ãƒ‡ãƒ¼ã‚¿æ§‹é€ :**
```python
@dataclass
class Keyword:
    """ãƒªãƒ³ã‚¯åŒ–ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"""
    keyword: str
    url: str
    title: str = ""
    priority: int = 0
    exact_match: bool = False

@dataclass
class LinkConfig:
    """ãƒªãƒ³ã‚¯ãƒ“ãƒ«ãƒ‡ã‚£ãƒ³ã‚°è¨­å®š"""
    max_replacements_per_keyword: int = 2
    max_replacements_per_url: int = 2
    max_links_on_page: int = 50
    max_replacements_per_page: int = 30
    min_chars_between_links: int = 1
    skip_existing_links: bool = True
```

**ãƒªãƒ³ã‚¯æŒ¿å…¥ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ :**
```python
def process_html_content(html, keywords, config):
    """HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«ãƒªãƒ³ã‚¯ã‚’æŒ¿å…¥"""
    soup = BeautifulSoup(html, 'html.parser')
    
    # ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ¼ãƒ‰ã‚’å‡¦ç†
    for text_node in find_text_nodes(soup):
        if should_process_node(text_node):
            modified_text = apply_keywords(
                text_node.string, 
                keywords, 
                config
            )
            if modified_text != text_node.string:
                replace_node(text_node, modified_text)
    
    return str(soup)
```

### 5.2 linkbuilding_parallel.py

**ç›®çš„**: ä¸¦åˆ—å‡¦ç†ã«ã‚ˆã‚‹é«˜é€Ÿãƒªãƒ³ã‚¯ãƒ“ãƒ«ãƒ‡ã‚£ãƒ³ã‚°

```python
from concurrent.futures import ProcessPoolExecutor
import multiprocessing

def process_language(lang_code, config):
    """å˜ä¸€è¨€èªã‚’å‡¦ç†"""
    html_files = find_html_files(f'public/{lang_code}')
    keywords = load_keywords(f'data/linkbuilding/{lang_code}.yaml')
    
    for html_file in html_files:
        process_file(html_file, keywords, config)

def main():
    """ä¸¦åˆ—å‡¦ç†ãƒ¡ã‚¤ãƒ³"""
    max_workers = min(8, multiprocessing.cpu_count())
    languages = detect_languages('public')
    
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        futures = []
        for lang in languages:
            future = executor.submit(process_language, lang, config)
            futures.append(future)
        
        # çµæœã‚’å¾…æ©Ÿ
        for future in futures:
            future.result()
```

### 5.3 precompute_linkbuilding.py

**ç›®çš„**: å®Ÿéš›ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«åŸºã¥ã„ã¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æœ€é©åŒ–

```python
def optimize_keywords(keywords, html_files):
    """ä½¿ç”¨ã•ã‚Œãªã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’é™¤å¤–"""
    optimized = []
    
    for keyword in keywords:
        found = False
        for html_file in html_files:
            if keyword_exists_in_file(keyword, html_file):
                found = True
                break
        
        if found:
            optimized.append(keyword)
    
    return optimized

def generate_optimized_files(linkbuilding_dir, public_dir, output_dir):
    """æœ€é©åŒ–ã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ"""
    languages = detect_languages(linkbuilding_dir)
    
    for lang in languages:
        # ã‚¹ã‚­ãƒƒãƒ—æ¡ä»¶ï¼ˆæ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚§ãƒƒã‚¯ï¼‰
        if should_skip_language(lang, output_dir):
            continue
        
        keywords = load_keywords(f'{linkbuilding_dir}/{lang}.yaml')
        html_files = find_html_files(f'{public_dir}/{lang}')
        
        optimized = optimize_keywords(keywords, html_files)
        save_optimized(optimized, f'{output_dir}/{lang}_optimized.json')
```

---

## 6. ç”»åƒå‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

### 6.1 preprocess-images.sh

**ç›®çš„**: ç”»åƒã®æœ€é©åŒ–ã¨WebPå¤‰æ›

```bash
#!/bin/bash

process_image() {
    local input_file=$1
    local output_dir=$2
    
    # WebPå¤‰æ›
    cwebp -q 85 "$input_file" -o "${output_dir}/$(basename ${input_file%.*}).webp"
    
    # ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–ã‚µã‚¤ã‚ºç”Ÿæˆ
    for size in 320 640 1024 1920; do
        convert "$input_file" -resize "${size}x>" \
            "${output_dir}/$(basename ${input_file%.*})-${size}w.jpg"
    done
}

process_all_images() {
    find static/images -type f \( -name "*.jpg" -o -name "*.png" \) | \
    while read image; do
        process_image "$image" "static/images/optimized"
    done
}
```

### 6.2 find_duplicate_images.py

**ç›®çš„**: é‡è¤‡ç”»åƒã‚’æ¤œå‡ºã—ã¦å ±å‘Š

```python
import hashlib
from PIL import Image

def calculate_image_hash(image_path):
    """ç”»åƒã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—"""
    with Image.open(image_path) as img:
        # ç”»åƒã‚’æ­£è¦åŒ–ï¼ˆã‚µã‚¤ã‚ºçµ±ä¸€ï¼‰
        img_resized = img.resize((128, 128))
        # ãƒãƒƒã‚·ãƒ¥è¨ˆç®—
        return hashlib.md5(img_resized.tobytes()).hexdigest()

def find_duplicates(image_dir):
    """é‡è¤‡ç”»åƒã‚’æ¤œå‡º"""
    hash_to_files = defaultdict(list)
    
    for image_file in find_image_files(image_dir):
        file_hash = calculate_image_hash(image_file)
        hash_to_files[file_hash].append(image_file)
    
    # é‡è¤‡ã‚’æŠ½å‡º
    duplicates = {
        hash: files 
        for hash, files in hash_to_files.items() 
        if len(files) > 1
    }
    
    return duplicates
```

### 6.3 offload_replicate_images.py

**ç›®çš„**: å¤–éƒ¨ã‚µãƒ¼ãƒ“ã‚¹ã‹ã‚‰ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜

```python
import requests
from urllib.parse import urlparse

def download_image(url, save_path):
    """ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    response = requests.get(url, stream=True)
    response.raise_for_status()
    
    with open(save_path, 'wb') as f:
        for chunk in response.iter_content(8192):
            f.write(chunk)

def offload_replicate_images(content_dir, image_dir):
    """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å†…ã®å¤–éƒ¨ç”»åƒã‚’ãƒ­ãƒ¼ã‚«ãƒ«åŒ–"""
    for md_file in find_markdown_files(content_dir):
        content = read_file(md_file)
        
        # å¤–éƒ¨ç”»åƒURLã‚’æ¤œå‡º
        external_images = find_external_images(content)
        
        for image_url in external_images:
            # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã‚’ç”Ÿæˆ
            local_path = generate_local_path(image_url, image_dir)
            
            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            download_image(image_url, local_path)
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å†…ã®URLã‚’æ›´æ–°
            content = content.replace(image_url, local_path)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°
        write_file(md_file, content)
```

---

## 7. ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

### 7.1 extract_automatic_links.py

**ç›®çš„**: ãƒ•ãƒ­ãƒ³ãƒˆãƒã‚¿ãƒ¼ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰è‡ªå‹•ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º

```python
def extract_keywords_from_frontmatter(content):
    """ãƒ•ãƒ­ãƒ³ãƒˆãƒã‚¿ãƒ¼ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
    frontmatter = parse_frontmatter(content)
    
    # æœ€åˆã®2ã¤ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä½¿ç”¨
    keywords = frontmatter.get('keywords', [])[:2]
    
    return keywords

def generate_automatic_links(content_dir):
    """è‡ªå‹•ãƒªãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
    links = []
    
    for md_file in find_markdown_files(content_dir):
        keywords = extract_keywords_from_frontmatter(md_file)
        url = generate_url_from_file(md_file)
        
        for keyword in keywords:
            links.append({
                'keyword': keyword,
                'url': url,
                'priority': 1,
                'exact_match': False
            })
    
    return links
```

### 7.2 generate_amplify_redirects_file.py

**ç›®çš„**: AWS Amplifyç”¨ã®ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆãƒ«ãƒ¼ãƒ«ã‚’ç”Ÿæˆ

```python
def generate_redirects(hugo_root):
    """ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆãƒ«ãƒ¼ãƒ«ã‚’ç”Ÿæˆ"""
    redirects = []
    
    # è¨€èªãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆ
    redirects.append({
        "source": "/ja",
        "target": "/",
        "status": "301"
    })
    
    # å¤ã„URLã‹ã‚‰æ–°ã—ã„URLã¸
    url_mappings = load_url_mappings('data/redirects.yaml')
    for old_url, new_url in url_mappings.items():
        redirects.append({
            "source": old_url,
            "target": new_url,
            "status": "301"
        })
    
    # 404ãƒšãƒ¼ã‚¸
    redirects.append({
        "source": "/<*>",
        "target": "/404.html",
        "status": "404"
    })
    
    return redirects

def save_redirects_json(redirects, output_path):
    """JSONå½¢å¼ã§ä¿å­˜"""
    with open(output_path, 'w') as f:
        json.dump(redirects, f, indent=2)
```

### 7.3 generate_linkbuilding_keywords.py

**ç›®çš„**: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰ãƒªãƒ³ã‚¯ãƒ“ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è‡ªå‹•ç”Ÿæˆ

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from collections import Counter

def extract_keywords_from_content(content_files, top_k=10):
    """TF-IDFã‚’ä½¿ç”¨ã—ã¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
    
    # å…¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’åé›†
    documents = []
    for file in content_files:
        content = read_markdown_content(file)
        documents.append(content)
    
    # TF-IDFè¨ˆç®—
    vectorizer = TfidfVectorizer(
        max_features=100,
        ngram_range=(1, 3),
        stop_words=load_stop_words()
    )
    
    tfidf_matrix = vectorizer.fit_transform(documents)
    feature_names = vectorizer.get_feature_names_out()
    
    # ã‚¹ã‚³ã‚¢ã§ãƒ©ãƒ³ã‚¯ä»˜ã‘
    scores = tfidf_matrix.sum(axis=0).A1
    keyword_scores = list(zip(feature_names, scores))
    keyword_scores.sort(key=lambda x: x[1], reverse=True)
    
    return keyword_scores[:top_k]
```

---

## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### ä¸¦åˆ—å‡¦ç†æˆ¦ç•¥

```python
# CPUæ•°ã«å¿œã˜ã¦ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’èª¿æ•´
max_workers = min(
    8,  # æœ€å¤§8ãƒ¯ãƒ¼ã‚«ãƒ¼
    multiprocessing.cpu_count(),  # CPUæ•°
    len(languages)  # è¨€èªæ•°
)

# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ç›£è¦–
def monitor_memory():
    import psutil
    process = psutil.Process()
    memory_mb = process.memory_info().rss / 1024 / 1024
    if memory_mb > 2048:  # 2GBåˆ¶é™
        logger.warning(f"High memory usage: {memory_mb:.0f}MB")
```

### ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æˆ¦ç•¥

```python
# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥
class FileCache:
    def __init__(self, cache_dir='.cache'):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
    
    def get(self, key):
        cache_file = self.cache_dir / f"{key}.cache"
        if cache_file.exists():
            with open(cache_file, 'rb') as f:
                return pickle.load(f)
        return None
    
    def set(self, key, value):
        cache_file = self.cache_dir / f"{key}.cache"
        with open(cache_file, 'wb') as f:
            pickle.dump(value, f)
```

---

## ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

### å…±é€šã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ãƒ¼

```python
class ScriptError(Exception):
    """ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼"""
    pass

def safe_execute(func):
    """ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã«ã‚ˆã‚‹å®‰å…¨å®Ÿè¡Œ"""
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            logger.error(f"Error in {func.__name__}: {e}")
            if DEBUG:
                raise
            return None
    return wrapper
```

### ãƒ­ã‚®ãƒ³ã‚°è¨­å®š

```python
import logging

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scripts.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)
```

---

## å®Ÿè¡Œç’°å¢ƒè¦ä»¶

### ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶

- **OS**: macOS, Linux, WSL
- **Python**: 3.6ä»¥ä¸Š
- **Node.js**: 14ä»¥ä¸Š
- **Hugo**: 0.80ä»¥ä¸Š
- **ãƒ¡ãƒ¢ãƒª**: 4GBä»¥ä¸Šæ¨å¥¨
- **ãƒ‡ã‚£ã‚¹ã‚¯**: 10GBä»¥ä¸Šã®ç©ºãå®¹é‡

### å¿…é ˆãƒ„ãƒ¼ãƒ«

```bash
# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª
python3 --version
node --version
hugo version
git --version

# ç”»åƒå‡¦ç†ãƒ„ãƒ¼ãƒ«
convert --version  # ImageMagick
cwebp -version     # WebP
```

---

## æ›´æ–°å±¥æ­´

- **2025-11-22**: åˆç‰ˆä½œæˆ
- **2025-11-20**: ä¸¦åˆ—å‡¦ç†æœ€é©åŒ–
- **2025-11-19**: ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æ©Ÿèƒ½è¿½åŠ 

---

## è‘—ä½œæ¨©ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

Â© 2025 SmartWeb/Interwork Corporation
License: GPL v2 or later
