#!/usr/bin/env python3
"""
Batch Article Creation using Claude API (Test Mode Available)
APIè‡ªå‹•åŒ–è¨˜äº‹ç”Ÿæˆï¼ˆãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰å¯¾å¿œï¼‰
"""

import os
import csv
import re
import anthropic
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import argparse
from dotenv import load_dotenv

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

# è¨­å®š
API_KEY = os.environ.get("ANTHROPIC_API_KEY") or os.environ.get("CLAUDE_API_KEY")
MODEL = "claude-sonnet-4-5-20250929" # User specified model
OUTPUT_DIR = Path("/Users/TM-MBP1/Documents/GitHub/hugo-boilerplate/content/en/glossary")

# è¨˜äº‹ç”Ÿæˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
ARTICLE_PROMPT = """Create a comprehensive glossary article for: {keyword}

CRITICAL REQUIREMENTS:
- Language: ENGLISH ONLY (all content must be in English)
- Word count: 2,000-2,500 words
- Format: 30% prose, 70% structured content (bullets, tables)
- Include frontmatter with these fields:
  - title: "{keyword}"
  - date: {date}
  - translationKey: {slug}
  - description: (SEO-optimized, 150-160 chars)
  - keywords: (5-8 keywords, lowercase)
  - category: "Application & Use-Cases"
  - type: glossary
  - draft: false

STRUCTURE (must include):
1. What is {keyword}? (2-3 paragraphs prose)
2. Core Technologies/Concepts (bullets with descriptions)
3. How It Works (workflow explanation)
4. Key Benefits (bullets)
5. Common Use Cases (5-7 examples)
6. Comparison Table (if applicable)
7. Challenges and Considerations
8. Implementation Best Practices
9. References (markdown links with full URLs)

QUALITY STANDARDS:
- Professional, technical tone
- Specific examples and metrics
- No redundancy
- Clear, scannable structure
- All external links in References section only
- Follow the style of AI-chatbot.md and RAG.md

Write the complete article in English."""

def generate_article(keyword: str, slug: str, date: str, output_dir: Path) -> tuple:
    """1è¨˜äº‹ã‚’ç”Ÿæˆ"""
    client = anthropic.Anthropic(api_key=API_KEY)
    
    start_time = time.time()
    
    try:
        message = client.messages.create(
            model=MODEL,
            max_tokens=8000,
            temperature=0.2,
            messages=[{
                "role": "user",
                "content": ARTICLE_PROMPT.format(
                    keyword=keyword,
                    slug=slug,
                    date=date
                )
            }]
        )
        
        content = message.content[0].text
        elapsed = time.time() - start_time
        
        # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã¨ã‚³ã‚¹ãƒˆè¨ˆç®—
        input_tokens = message.usage.input_tokens
        output_tokens = message.usage.output_tokens
        # Sonnet 3.5 pricing (approximate)
        cost = (input_tokens * 3 / 1_000_000) + (output_tokens * 15 / 1_000_000)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        filepath = output_dir / f"{slug}.md"
        filepath.write_text(content, encoding='utf-8')
        
        print(f"âœ… {keyword}: {output_tokens} tokens, ${cost:.4f}, {elapsed:.1f}s")
        
        return (keyword, True, output_tokens, cost, elapsed)
        
    except Exception as e:
        print(f"âŒ {keyword}: Error - {e}")
        return (keyword, False, 0, 0.0, 0.0)

def parse_flowhunt_csv(csv_path: Path) -> list:
    """glossaries_flowhunt.csvå½¢å¼ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‘ãƒ¼ã‚¹ã™ã‚‹"""
    keywords = []
    with open(csv_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            filename = row.get('filename', '').strip()
            flow_input = row.get('flow_input', '')
            
            if not filename or not flow_input:
                continue
                
            # Extract TERM from flow_input
            term_match = re.search(r'TERM:\s*(.+)', flow_input)
            if term_match:
                term = term_match.group(1).strip()
                slug = filename.replace('.md', '')
                keywords.append((term, slug))
    return keywords

def parse_prioritized_csv(csv_path: Path) -> list:
    """prioritized_keywords.csvå½¢å¼ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‘ãƒ¼ã‚¹ã™ã‚‹"""
    keywords = []
    with open(csv_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            term = row.get('keyword', '').strip()
            filename = row.get('filename', '').strip()
            
            if term and filename:
                slug = filename.replace('.md', '')
                keywords.append((term, slug))
    return keywords

def batch_generate(keywords: list, output_dir: Path, max_workers: int = 5, limit: int = 0):
    """è¤‡æ•°è¨˜äº‹ã‚’ä¸¦åˆ—ç”Ÿæˆ"""
    date = "2025-12-21"
    results = []
    total_cost = 0.0
    total_time = 0.0
    total_tokens = 0
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # æ—¢å­˜ãƒã‚§ãƒƒã‚¯ï¼ˆæœ¬ç•ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨å‡ºåŠ›å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä¸¡æ–¹ã‚’ç¢ºèªï¼‰
    production_dir = Path("/Users/TM-MBP1/Documents/GitHub/hugo-boilerplate/content/en/glossary")
    existing_files = {f.stem.lower() for f in output_dir.glob("*.md")}
    if production_dir.exists():
        existing_files.update(f.stem.lower() for f in production_dir.glob("*.md"))
    
    target_keywords = []
    seen_slugs = set()
    
    print(f"Checking {len(keywords)} candidates against {len(existing_files)} existing files")
    
    for term, slug in keywords:
        slug_lower = slug.lower()
        if slug_lower in existing_files:
            print(f"Skipping existing: {term} ({slug})")
            continue
        if slug_lower in seen_slugs:
            print(f"Skipping duplicate in list: {term} ({slug})")
            continue
            
        seen_slugs.add(slug_lower)
        target_keywords.append((term, slug))
    
    if limit > 0:
        target_keywords = target_keywords[:limit]
        
    if not target_keywords:
        print("No new articles to generate.")
        return []

    print(f"\n{'='*70}")
    print(f"ðŸš€ ãƒãƒƒãƒç”Ÿæˆé–‹å§‹")
    print(f"{'='*70}")
    print(f"ç”Ÿæˆäºˆå®šæ•°: {len(target_keywords)}")
    print(f"ä¸¦åˆ—æ•°: {max_workers}")
    print(f"å‡ºåŠ›å…ˆ: {output_dir}")
    print(f"{'='*70}\n")
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {
            executor.submit(generate_article, kw, slug, date, output_dir): (kw, slug)
            for kw, slug in target_keywords
        }
        
        for future in as_completed(futures):
            keyword, slug = futures[future]
            kw, success, tokens, cost, elapsed = future.result()
            
            if success:
                total_cost += cost
                total_time += elapsed
                total_tokens += tokens
                
                results.append({
                    "keyword": kw,
                    "slug": slug,
                    "tokens": tokens,
                    "cost": f"${cost:.4f}",
                    "time": f"{elapsed:.1f}s",
                    "status": "âœ…"
                })
            else:
                results.append({
                    "keyword": kw,
                    "slug": slug,
                    "tokens": 0,
                    "cost": "$0.0000",
                    "time": "0.0s",
                    "status": "âŒ"
                })
    
    # ã‚µãƒžãƒªãƒ¼è¡¨ç¤º
    successful = sum(1 for r in results if r['status'] == 'âœ…')
    
    print(f"\n{'='*70}")
    print(f"ðŸ“Š å®Œäº†ã‚µãƒžãƒªãƒ¼")
    print(f"{'='*70}")
    print(f"æˆåŠŸ:         {successful}/{len(target_keywords)}")
    print(f"åˆè¨ˆãƒˆãƒ¼ã‚¯ãƒ³: {total_tokens:,}")
    print(f"åˆè¨ˆã‚³ã‚¹ãƒˆ:   ${total_cost:.4f}")
    if len(target_keywords) > 0:
        print(f"å¹³å‡æ™‚é–“:     {total_time/len(target_keywords):.1f}ç§’/è¨˜äº‹")
    print(f"ç·æ™‚é–“:       {total_time/60:.1f}åˆ†")
    print(f"{'='*70}\n")
    
    return results

def main():
    parser = argparse.ArgumentParser(description="Claude APIè¨˜äº‹è‡ªå‹•ç”Ÿæˆ")
    parser.add_argument('--test', action='store_true', help='ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼ˆåˆ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«å‡ºåŠ›ï¼‰')
    parser.add_argument('--csv', help='ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('--keywords', nargs='+', help='ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆï¼ˆç›´æŽ¥æŒ‡å®šï¼‰')
    parser.add_argument('--workers', type=int, default=3, help='ä¸¦åˆ—æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3ï¼‰')
    parser.add_argument('--limit', type=int, default=0, help='ç”Ÿæˆã™ã‚‹æœ€å¤§è¨˜äº‹æ•°ï¼ˆ0ã¯ç„¡åˆ¶é™ï¼‰')
    parser.add_argument('--output-dir', help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæŒ‡å®š')
    
    args = parser.parse_args()
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š
    if args.output_dir:
        output_dir = Path(args.output_dir)
    elif args.test:
        output_dir = Path("/Users/TM-MBP1/Documents/GitHub/hugo-boilerplate/content/en/glossary-api-test")
    else:
        output_dir = OUTPUT_DIR
        
    print(f"Output Directory: {output_dir}")
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰èª­ã¿è¾¼ã¿
    keywords = []
    if args.csv:
        csv_path = Path(args.csv)
        if not csv_path.exists():
            print(f"Error: CSV file not found: {csv_path}")
            return
            
        if "flowhunt" in csv_path.name:
            keywords = parse_flowhunt_csv(csv_path)
        else:
            keywords = parse_prioritized_csv(csv_path)
        print(f"Loaded {len(keywords)} keywords from {csv_path.name}")
        
    elif args.keywords:
        keywords = [(kw, kw.replace(' ', '-')) for kw in args.keywords]
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
        keywords = [
            ("Customer Segmentation", "Customer-Segmentation"),
            ("Sentiment Analysis", "Sentiment-Analysis"),
        ]
    
    # å®Ÿè¡Œ
    results = batch_generate(keywords, output_dir, max_workers=args.workers, limit=args.limit)
    
    # çµæžœè©³ç´°
    if results:
        print("\nðŸ“‹ è©³ç´°çµæžœ:")
        for r in results:
            print(f"{r['status']} {r['keyword']:35s} {r['tokens']:>6} tokens  {r['cost']:>10s}  {r['time']:>8s}")

if __name__ == "__main__":
    main()
