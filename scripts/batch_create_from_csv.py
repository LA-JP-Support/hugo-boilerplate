#!/usr/bin/env python3
"""
CSV-based Batch Article Creation (v3)
CSVã‹ã‚‰æœªä½œæˆè¨˜äº‹ã‚’å–å¾—ã—ã¦ä¸€æ‹¬ç”Ÿæˆ
"""

import os
import csv
import anthropic
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import argparse
import re
from dotenv import load_dotenv

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

# è¨­å®š
API_KEY = os.environ.get("ANTHROPIC_API_KEY") or os.environ.get("CLAUDE_API_KEY")
MODEL = "claude-sonnet-4-5-20250929"
OUTPUT_DIR = Path("/Users/TM-MBP1/Documents/GitHub/hugo-boilerplate/content/en/glossary")
DEFAULT_CSV_PATH = Path("/Users/TM-MBP1/Documents/GitHub/hugo-boilerplate/docs/prioritized_keywords.csv")

def needs_article(keyword: str) -> bool:
    """å† è©ãŒå¿…è¦ã‹ã‚’åˆ¤å®š"""
    if keyword.endswith('s') or keyword.endswith('es'):
        return False
    uncountable = ['forecasting', 'analysis', 'learning', 'intelligence', 'processing', 
                   'management', 'optimization', 'automation', 'segmentation', 'modeling',
                   'mining', 'clustering', 'classification', 'recognition', 'detection']
    if any(uc in keyword.lower() for uc in uncountable):
        return False
    return True

def get_article(keyword: str) -> str:
    """é©åˆ‡ãªå† è©ã‚’è¿”ã™"""
    if not needs_article(keyword):
        return ""
    if keyword[0].lower() in 'aeiou':
        return "an "
    return "a "

ARTICLE_PROMPT = """You are an expert technical writer creating a comprehensive glossary article.

TOPIC: {keyword}

CRITICAL REQUIREMENTS:
âœ“ Language: ENGLISH ONLY
âœ“ Word count: 2,700-2,900 words (STRICT - do not exceed 3,000 words)
âœ“ Format: 30% prose in opening sections, 70% structured content (bullets, tables)
âœ“ Write complete, detailed content - do not summarize or abbreviate
âœ“ Use proper English grammar including articles (a/an/the) where appropriate

EXACT FRONTMATTER (copy this format):
---
title: "{keyword}"
date: {date}
translationKey: {slug}
description: "SEO-optimized description (150-160 chars)"
keywords:
- keyword1
- keyword2
- keyword3
- keyword4
- keyword5
category: "Application & Use-Cases"
type: glossary
draft: false
---

EXACT STRUCTURE (use these H2 headings, no H3):

## What is {article}{keyword}?
Write 3 full paragraphs (800-1000 words total)

## Core [Specific Technologies/Approaches/Components]
List 5-7 main concepts with **Bold Names** and 2-3 sentence explanations

## How {keyword} Works
Detailed workflow with 6-10 steps and example workflow

## Key Benefits
List 8-10 benefits with **Bold Headers** and explanations

## Common Use Cases
List 8-10 real-world applications with **Bold Headers**

## [Appropriate Comparison Table]
Create comparison table with 5-6 items

## Challenges and Considerations
List 8-10 challenges with **Bold Headers**

## Implementation Best Practices
List 10 best practices with **Bold Headers**

## Advanced Techniques
List 5-6 advanced concepts with **Bold Headers**

## Future Directions
List 5-6 future trends with **Bold Headers**

## References
Include 6-8 authoritative references

IMPORTANT: Keep total length under 3,000 words. Write the COMPLETE article."""

def generate_article(keyword: str, slug: str, date: str, output_dir: Path) -> tuple:
    """1è¨˜äº‹ã‚’ç”Ÿæˆ"""
    client = anthropic.Anthropic(api_key=API_KEY)
    article = get_article(keyword)
    
    start_time = time.time()
    
    try:
        message = client.messages.create(
            model=MODEL,
            max_tokens=16000,
            temperature=0.1,
            messages=[{
                "role": "user",
                "content": ARTICLE_PROMPT.format(
                    keyword=keyword,
                    slug=slug,
                    date=date,
                    article=article
                )
            }]
        )
        
        content = message.content[0].text
        elapsed = time.time() - start_time
        
        input_tokens = message.usage.input_tokens
        output_tokens = message.usage.output_tokens
        cost = (input_tokens * 3 / 1_000_000) + (output_tokens * 15 / 1_000_000)
        words = len(re.findall(r'\b\w+\b', content))
        
        filepath = output_dir / f"{slug}.md"
        filepath.write_text(content, encoding='utf-8')
        
        print(f"âœ… {keyword}: {words} words, ${cost:.4f}, {elapsed:.1f}s")
        
        return (keyword, True, words, output_tokens, cost, elapsed)
        
    except Exception as e:
        print(f"âŒ {keyword}: Error - {e}")
        return (keyword, False, 0, 0, 0.0, 0.0)

def get_pending_keywords(csv_path: Path, output_dir: Path, limit: int = None) -> list:
    """æœªä½œæˆè¨˜äº‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã‚’å–å¾—"""
    if not csv_path.exists():
        print(f"âŒ CSV not found: {csv_path}")
        return []
    
    with open(csv_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        rows = list(reader)
    
    pending = []
    for row in rows:
        filename = row.get('filename', '')
        keyword = row.get('keyword', '')
        
        if not filename or not keyword:
            continue
        
        filepath = output_dir / filename
        if not filepath.exists():
            slug = filename.replace('.md', '')
            pending.append((keyword, slug))
    
    if limit:
        pending = pending[:limit]
    
    return pending

def batch_generate(keywords: list, output_dir: Path, max_workers: int = 10):
    """è¤‡æ•°è¨˜äº‹ã‚’ä¸¦åˆ—ç”Ÿæˆ"""
    date = "2025-12-21"
    results = []
    total_cost = 0.0
    total_time = 0.0
    total_tokens = 0
    total_words = 0
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\n{'='*70}")
    print(f"ğŸš€ æœ¬ç•ªãƒãƒƒãƒç”Ÿæˆé–‹å§‹ (v3)")
    print(f"{'='*70}")
    print(f"è¨˜äº‹æ•°: {len(keywords)}")
    print(f"ä¸¦åˆ—æ•°: {max_workers}")
    print(f"å‡ºåŠ›å…ˆ: {output_dir}")
    print(f"ç›®æ¨™èªæ•°: 2,700-2,900èª/è¨˜äº‹")
    print(f"äºˆæƒ³æ™‚é–“: {len(keywords) * 100 / max_workers / 60:.1f}åˆ†")
    print(f"äºˆæƒ³ã‚³ã‚¹ãƒˆ: ${len(keywords) * 0.088:.2f}")
    print(f"{'='*70}\n")
    
    start_time = time.time()
    completed = 0
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {
            executor.submit(generate_article, kw, slug, date, output_dir): (kw, slug)
            for kw, slug in keywords
        }
        
        for future in as_completed(futures):
            keyword, slug = futures[future]
            kw, success, words, tokens, cost, elapsed = future.result()
            
            completed += 1
            
            if success:
                total_cost += cost
                total_time += elapsed
                total_tokens += tokens
                total_words += words
                
                results.append({
                    "keyword": kw,
                    "words": words,
                    "tokens": tokens,
                    "cost": cost,
                    "status": "âœ…"
                })
            else:
                results.append({
                    "keyword": kw,
                    "words": 0,
                    "tokens": 0,
                    "cost": 0.0,
                    "status": "âŒ"
                })
            
            # é€²æ—è¡¨ç¤º
            if completed % 10 == 0:
                elapsed_total = time.time() - start_time
                remaining = len(keywords) - completed
                eta = (elapsed_total / completed) * remaining / 60
                print(f"\nğŸ“Š é€²æ—: {completed}/{len(keywords)} ({completed/len(keywords)*100:.1f}%)")
                print(f"   çµŒé: {elapsed_total/60:.1f}åˆ†, æ®‹ã‚Š: {eta:.1f}åˆ†ï¼ˆäºˆæ¸¬ï¼‰")
                print(f"   ã‚³ã‚¹ãƒˆ: ${total_cost:.2f}\n")
    
    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    successful = sum(1 for r in results if r['status'] == 'âœ…')
    
    print(f"\n{'='*70}")
    print(f"ğŸ“Š å®Œäº†ã‚µãƒãƒªãƒ¼")
    print(f"{'='*70}")
    print(f"æˆåŠŸ:         {successful}/{len(keywords)} ({successful/len(keywords)*100:.1f}%)")
    print(f"å¹³å‡èªæ•°:     {total_words/successful:.0f}èª/è¨˜äº‹" if successful > 0 else "")
    print(f"åˆè¨ˆãƒˆãƒ¼ã‚¯ãƒ³: {total_tokens:,}")
    print(f"åˆè¨ˆã‚³ã‚¹ãƒˆ:   ${total_cost:.2f}")
    print(f"ç·æ™‚é–“:       {(time.time()-start_time)/60:.1f}åˆ†")
    print(f"{'='*70}\n")
    
    return results

def main():
    parser = argparse.ArgumentParser(description="CSV-based batch article generation (v3)")
    parser.add_argument('--limit', type=int, help='ç”Ÿæˆã™ã‚‹è¨˜äº‹æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: å…¨æœªä½œæˆè¨˜äº‹ï¼‰')
    parser.add_argument('--workers', type=int, default=10, help='ä¸¦åˆ—æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10ï¼‰')
    parser.add_argument('--output-dir', help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: content/en/glossaryï¼‰')
    
    args = parser.parse_args()
    
    output_dir = Path(args.output_dir) if args.output_dir else OUTPUT_DIR
    
    # æœªä½œæˆè¨˜äº‹ã‚’å–å¾—
    print("ğŸ“‹ æœªä½œæˆè¨˜äº‹ã‚’å–å¾—ä¸­...")
    keywords = get_pending_keywords(CSV_PATH, output_dir, limit=args.limit)
    
    if not keywords:
        print("âœ… å…¨ã¦ã®è¨˜äº‹ãŒä½œæˆæ¸ˆã¿ã§ã™")
        return
    
    print(f"ğŸ“ æœªä½œæˆè¨˜äº‹: {len(keywords)}ä»¶")
    if args.limit:
        print(f"   ä»Šå›ç”Ÿæˆ: {min(args.limit, len(keywords))}ä»¶")
    
    # ç¢ºèª
    print(f"\nâš ï¸  ã“ã‚Œã‹ã‚‰{len(keywords)}ä»¶ã®è¨˜äº‹ã‚’ç”Ÿæˆã—ã¾ã™")
    print(f"   äºˆæƒ³æ™‚é–“: {len(keywords) * 100 / args.workers / 60:.1f}åˆ†")
    print(f"   äºˆæƒ³ã‚³ã‚¹ãƒˆ: ${len(keywords) * 0.088:.2f}")
    
    response = input("\nç¶šè¡Œã—ã¾ã™ã‹ï¼Ÿ (yes/no): ")
    if response.lower() not in ['yes', 'y']:
        print("âŒ ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ")
        return
    
    # å®Ÿè¡Œ
    results = batch_generate(keywords, output_dir, max_workers=args.workers)
    
    # å¤±æ•—ã—ãŸè¨˜äº‹ã‚’ãƒªã‚¹ãƒˆ
    failed = [r for r in results if r['status'] == 'âŒ']
    if failed:
        print("\nâš ï¸  å¤±æ•—ã—ãŸè¨˜äº‹:")
        for r in failed:
            print(f"   - {r['keyword']}")

if __name__ == "__main__":
    main()
