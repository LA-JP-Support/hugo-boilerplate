#!/usr/bin/env python3
"""
å†…éƒ¨ãƒªãƒ³ã‚¯ãƒã‚§ãƒƒã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

Hugoç”Ÿæˆå¾Œã®HTMLãƒ•ã‚¡ã‚¤ãƒ«ã«å†…éƒ¨ãƒªãƒ³ã‚¯ï¼ˆdata-lb="1"ï¼‰ãŒæ­£ã—ãè¿½åŠ ã•ã‚Œã¦ã„ã‚‹ã‹ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¾ã™ã€‚

ä½¿ç”¨æ–¹æ³•:
    python3 scripts/check_internal_links.py --public-dir public
    python3 scripts/check_internal_links.py --public-dir public --language ja
    python3 scripts/check_internal_links.py --public-dir public --min-links 5
"""

import argparse
from pathlib import Path
from bs4 import BeautifulSoup
import sys


def check_internal_links(public_dir, language=None, min_links=0, verbose=False):
    """
    HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®å†…éƒ¨ãƒªãƒ³ã‚¯ã‚’ãƒã‚§ãƒƒã‚¯
    
    Args:
        public_dir: publicãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
        language: ãƒã‚§ãƒƒã‚¯ã™ã‚‹è¨€èªï¼ˆNone=å…¨è¨€èªï¼‰
        min_links: æœ€å°ãƒªãƒ³ã‚¯æ•°ã®é–¾å€¤
        verbose: è©³ç´°å‡ºåŠ›
    
    Returns:
        dict: ãƒã‚§ãƒƒã‚¯çµæœ
    """
    public_path = Path(public_dir)
    
    if not public_path.exists():
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {public_dir} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return None
    
    # è¨€èªã”ã¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ±ºå®š
    if language:
        search_dirs = [public_path / language]
    else:
        # ä¸»è¦è¨€èªã®ã¿ãƒã‚§ãƒƒã‚¯ï¼ˆen, jaï¼‰
        search_dirs = [public_path / 'en', public_path / 'ja', public_path]
    
    results = {
        'total_files': 0,
        'files_with_links': 0,
        'files_without_links': [],
        'total_links': 0,
        'files_below_threshold': []
    }
    
    for search_dir in search_dirs:
        if not search_dir.exists():
            continue
        
        # HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        html_files = list(search_dir.rglob('*.html'))
        
        for html_file in html_files:
            # index.htmlã®ã¿ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆãƒšãƒ¼ã‚¸ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ï¼‰
            if html_file.name != 'index.html':
                continue
            
            results['total_files'] += 1
            
            try:
                with open(html_file, 'r', encoding='utf-8') as f:
                    soup = BeautifulSoup(f.read(), 'html.parser')
                
                # data-lb="1"å±æ€§ã‚’æŒã¤ãƒªãƒ³ã‚¯ã‚’æ¤œç´¢
                internal_links = soup.find_all('a', {'data-lb': '1'})
                link_count = len(internal_links)
                
                results['total_links'] += link_count
                
                if link_count > 0:
                    results['files_with_links'] += 1
                else:
                    rel_path = html_file.relative_to(public_path)
                    results['files_without_links'].append(str(rel_path))
                
                # é–¾å€¤ãƒã‚§ãƒƒã‚¯
                if 0 < link_count < min_links:
                    rel_path = html_file.relative_to(public_path)
                    results['files_below_threshold'].append({
                        'path': str(rel_path),
                        'count': link_count
                    })
                
                if verbose and link_count > 0:
                    rel_path = html_file.relative_to(public_path)
                    print(f"âœ“ {rel_path}: {link_count}ä»¶")
                
            except Exception as e:
                print(f"âš ï¸  ã‚¨ãƒ©ãƒ¼: {html_file} - {e}")
    
    return results


def print_report(results, min_links=0):
    """ãƒã‚§ãƒƒã‚¯çµæœã®ãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›"""
    print("\n" + "=" * 60)
    print("ğŸ“Š å†…éƒ¨ãƒªãƒ³ã‚¯ãƒã‚§ãƒƒã‚¯çµæœ")
    print("=" * 60)
    
    print(f"\nâœ… çµ±è¨ˆ:")
    print(f"  ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {results['total_files']}ä»¶")
    print(f"  å†…éƒ¨ãƒªãƒ³ã‚¯ã‚ã‚Š: {results['files_with_links']}ä»¶")
    print(f"  å†…éƒ¨ãƒªãƒ³ã‚¯ãªã—: {len(results['files_without_links'])}ä»¶")
    print(f"  ç·ãƒªãƒ³ã‚¯æ•°: {results['total_links']:,}ä»¶")
    
    if results['total_files'] > 0:
        avg_links = results['total_links'] / results['total_files']
        print(f"  å¹³å‡ãƒªãƒ³ã‚¯æ•°: {avg_links:.1f}ä»¶/ãƒ•ã‚¡ã‚¤ãƒ«")
    
    # å†…éƒ¨ãƒªãƒ³ã‚¯ãŒãªã„ãƒ•ã‚¡ã‚¤ãƒ«
    if results['files_without_links']:
        print(f"\nâŒ å†…éƒ¨ãƒªãƒ³ã‚¯ãŒ0ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ« ({len(results['files_without_links'])}ä»¶):")
        for path in results['files_without_links'][:20]:
            print(f"  - {path}")
        if len(results['files_without_links']) > 20:
            print(f"  ... ä»–{len(results['files_without_links']) - 20}ä»¶")
    
    # é–¾å€¤æœªæº€ã®ãƒ•ã‚¡ã‚¤ãƒ«
    if min_links > 0 and results['files_below_threshold']:
        print(f"\nâš ï¸  å†…éƒ¨ãƒªãƒ³ã‚¯ãŒ{min_links}ä»¶æœªæº€ã®ãƒ•ã‚¡ã‚¤ãƒ« ({len(results['files_below_threshold'])}ä»¶):")
        for item in results['files_below_threshold'][:20]:
            print(f"  - {item['path']}: {item['count']}ä»¶")
        if len(results['files_below_threshold']) > 20:
            print(f"  ... ä»–{len(results['files_below_threshold']) - 20}ä»¶")
    
    print("\n" + "=" * 60)
    
    # çµ‚äº†ã‚³ãƒ¼ãƒ‰
    if results['files_without_links']:
        return 1
    return 0


def main():
    parser = argparse.ArgumentParser(
        description='Hugoç”Ÿæˆå¾Œã®HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®å†…éƒ¨ãƒªãƒ³ã‚¯ã‚’ãƒã‚§ãƒƒã‚¯'
    )
    parser.add_argument(
        '--public-dir',
        default='public',
        help='publicãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: publicï¼‰'
    )
    parser.add_argument(
        '--language',
        choices=['en', 'ja'],
        help='ãƒã‚§ãƒƒã‚¯ã™ã‚‹è¨€èªï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯å…¨è¨€èªï¼‰'
    )
    parser.add_argument(
        '--min-links',
        type=int,
        default=0,
        help='æœ€å°ãƒªãƒ³ã‚¯æ•°ã®é–¾å€¤ï¼ˆã“ã®æ•°æœªæº€ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è­¦å‘Šï¼‰'
    )
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='è©³ç´°å‡ºåŠ›'
    )
    
    args = parser.parse_args()
    
    results = check_internal_links(
        args.public_dir,
        language=args.language,
        min_links=args.min_links,
        verbose=args.verbose
    )
    
    if results is None:
        sys.exit(1)
    
    exit_code = print_report(results, min_links=args.min_links)
    sys.exit(exit_code)


if __name__ == '__main__':
    main()
