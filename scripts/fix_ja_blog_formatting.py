#!/usr/bin/env python3
"""
æ—¥æœ¬èªãƒ–ãƒ­ã‚°è¨˜äº‹ã®ä¿®æ­£ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
1. å¤ªå­—ã®åŠ©è©å•é¡Œã‚’ä¿®æ­£ï¼ˆ**å˜èªã®** â†’ **å˜èª**ã®ï¼‰
2. è‹±èªç‰ˆã‹ã‚‰å†…éƒ¨ãƒªãƒ³ã‚¯ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆã—ã¦æ—¥æœ¬èªç‰ˆã«é©ç”¨
"""

import re
import os
from pathlib import Path
from typing import Dict, List, Tuple

# æ—¥æœ¬èªã®åŠ©è©ãƒªã‚¹ãƒˆ
PARTICLES = ['ã®', 'ãŒ', 'ã‚’', 'ã«', 'ã¸', 'ã¨', 'ã‹ã‚‰', 'ã¾ã§', 'ã‚ˆã‚Š', 'ã§', 'ã‚„', 'ã¯']

def fix_bold_particles(content: str) -> str:
    """
    å¤ªå­—ã®ä¸­ã«åŠ©è©ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã€åŠ©è©ã‚’å¤ªå­—ã®å¤–ã«å‡ºã™
    ä¾‹: **Googleã®** â†’ **Google**ã®
    """
    fixed_content = content
    
    for particle in PARTICLES:
        # **å˜èªåŠ©è©** ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ **å˜èª**åŠ©è© ã«ä¿®æ­£
        pattern = r'\*\*([^\*]+?)(' + re.escape(particle) + r')\*\*'
        replacement = r'**\1**\2'
        fixed_content = re.sub(pattern, replacement, fixed_content)
    
    return fixed_content

def extract_internal_links_from_en(en_file: Path) -> Dict[str, str]:
    """
    è‹±èªç‰ˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å†…éƒ¨ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
    è¿”ã‚Šå€¤: {è¡¨ç¤ºãƒ†ã‚­ã‚¹ãƒˆ: ãƒªãƒ³ã‚¯URL}
    """
    if not en_file.exists():
        return {}
    
    content = en_file.read_text(encoding='utf-8')
    links = {}
    
    # [text](/en/glossary/term/) å½¢å¼ã®ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
    pattern = r'\[([^\]]+?)\]\((/en/glossary/[^\)]+?/)\)'
    matches = re.findall(pattern, content)
    
    for text, url in matches:
        # è‹±èªã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚­ãƒ¼ã¨ã—ã¦ä¿å­˜
        links[text] = url
    
    return links

def create_term_mapping() -> Dict[str, Tuple[str, str]]:
    """
    è‹±èªç”¨èªã‹ã‚‰æ—¥æœ¬èªç”¨èªã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
    è¿”ã‚Šå€¤: {è‹±èªç”¨èª: (æ—¥æœ¬èªç”¨èª, glossary_url)}
    """
    mapping = {
        'ChatGPT': ('ChatGPT', '/en/glossary/ChatGPT/'),
        'neural networks': ('ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯', '/en/glossary/neural-networks/'),
        'token': ('ãƒˆãƒ¼ã‚¯ãƒ³', '/en/glossary/Token/'),
        'Token': ('ãƒˆãƒ¼ã‚¯ãƒ³', '/en/glossary/Token/'),
        'pre-training': ('äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°', '/en/glossary/Pre-Training/'),
        'Pre-Training': ('äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°', '/en/glossary/Pre-Training/'),
        'computational resources': ('è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹', '/en/glossary/computational-resources/'),
        'GPT': ('GPT', '/en/glossary/GPT/'),
        'knowledge base': ('ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹', '/en/glossary/knowledge-base/'),
        'Google': ('Google', '/en/glossary/Google/'),
        'Gemini': ('Gemini', '/en/glossary/Gemini/'),
        'Microsoft': ('Microsoft', '/en/glossary/Microsoft/'),
        'Meta': ('Meta', '/en/glossary/Meta/'),
        'Anthropic': ('Anthropic', '/en/glossary/Anthropic/'),
        'OpenAI': ('OpenAI', '/en/glossary/OpenAI/'),
        'Claude': ('Claude', '/en/glossary/Claude/'),
        'Chatbot': ('ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ', '/en/glossary/Chatbot/'),
        'Tokenization': ('ãƒˆãƒ¼ã‚¯ãƒ³åŒ–', '/en/glossary/Tokenization/'),
        'tokenization': ('ãƒˆãƒ¼ã‚¯ãƒ³åŒ–', '/en/glossary/Tokenization/'),
        'Conversation-History': ('ä¼šè©±å±¥æ­´', '/en/glossary/Conversation-History/'),
        'conversation history': ('ä¼šè©±å±¥æ­´', '/en/glossary/Conversation-History/'),
        'Text-Generation': ('ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ', '/en/glossary/Text-Generation/'),
        'text generation': ('ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ', '/en/glossary/Text-Generation/'),
        'hallucination': ('å¹»è¦š', '/en/glossary/hallucination/'),
        'prompts': ('ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ', '/en/glossary/prompts/'),
    }
    
    return mapping

def add_internal_links_to_ja(content: str, term_mapping: Dict[str, Tuple[str, str]]) -> str:
    """
    æ—¥æœ¬èªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«å†…éƒ¨ãƒªãƒ³ã‚¯ã‚’è¿½åŠ 
    æ—¢å­˜ã®ãƒªãƒ³ã‚¯ã€HTML ã‚¿ã‚°ã€ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å†…ã¯é™¤å¤–
    """
    # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã¨ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ã‚³ãƒ¼ãƒ‰ã‚’ä¸€æ™‚çš„ã«ä¿è­·
    code_blocks = []
    inline_codes = []
    html_tags = []
    
    def save_code_block(match):
        code_blocks.append(match.group(0))
        return f"___CODE_BLOCK_{len(code_blocks)-1}___"
    
    def save_inline_code(match):
        inline_codes.append(match.group(0))
        return f"___INLINE_CODE_{len(inline_codes)-1}___"
    
    def save_html_tag(match):
        html_tags.append(match.group(0))
        return f"___HTML_TAG_{len(html_tags)-1}___"
    
    # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’ä¿è­·
    content = re.sub(r'```[\s\S]*?```', save_code_block, content)
    # ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ã‚³ãƒ¼ãƒ‰ã‚’ä¿è­·
    content = re.sub(r'`[^`]+`', save_inline_code, content)
    # HTMLã‚¿ã‚°ã‚’ä¿è­·ï¼ˆYouTubeã®iframeãªã©ï¼‰
    content = re.sub(r'<[^>]+>', save_html_tag, content)
    content = re.sub(r'\{\{<.*?>\}\}', save_html_tag, content)
    
    # å„ç”¨èªã«ã¤ã„ã¦ãƒªãƒ³ã‚¯ã‚’è¿½åŠ 
    for en_term, (ja_term, url) in term_mapping.items():
        # å¤ªå­—ã®ä¸­ã®ç”¨èªã‚’å‡¦ç† **ç”¨èª** â†’ **[ç”¨èª](url)**
        pattern_bold = r'\*\*(' + re.escape(ja_term) + r')\*\*'
        
        def replace_bold(match):
            return f'**[{match.group(1)}]({url})**'
        
        content = re.sub(pattern_bold, replace_bold, content)
        
        # é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆä¸­ã®ç”¨èªã‚’å‡¦ç†ï¼ˆæœ€åˆã®å‡ºç¾ã®ã¿ï¼‰
        # æ—¢ã«ãƒªãƒ³ã‚¯ã«ãªã£ã¦ã„ãªã„ã€ã‹ã¤HTMLã‚¿ã‚°å†…ã§ãªã„ç®‡æ‰€
        if f'[{ja_term}]' not in content:
            # æœ€åˆã®å‡ºç¾ã‚’æ¢ã—ã¦ãƒªãƒ³ã‚¯åŒ–
            pattern_plain = r'(?<!\[)(?<!\*)(?<!\*\*)(' + re.escape(ja_term) + r')(?!\]|</|>|\*)'
            content = re.sub(pattern_plain, f'[{ja_term}]({url})', content, count=1)
    
    # ä¿è­·ã—ãŸè¦ç´ ã‚’å¾©å…ƒ
    for i, code in enumerate(code_blocks):
        content = content.replace(f"___CODE_BLOCK_{i}___", code)
    for i, code in enumerate(inline_codes):
        content = content.replace(f"___INLINE_CODE_{i}___", code)
    for i, tag in enumerate(html_tags):
        content = content.replace(f"___HTML_TAG_{i}___", tag)
    
    return content

def process_ja_blog_file(ja_file: Path, en_file: Path, term_mapping: Dict[str, Tuple[str, str]]) -> bool:
    """
    æ—¥æœ¬èªãƒ–ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
    """
    if not ja_file.exists():
        print(f"âš ï¸  ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {ja_file}")
        return False
    
    print(f"ğŸ“ å‡¦ç†ä¸­: {ja_file.name}")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    content = ja_file.read_text(encoding='utf-8')
    original_content = content
    
    # 1. å¤ªå­—ã®åŠ©è©å•é¡Œã‚’ä¿®æ­£
    content = fix_bold_particles(content)
    
    # 2. å†…éƒ¨ãƒªãƒ³ã‚¯ã‚’è¿½åŠ 
    content = add_internal_links_to_ja(content, term_mapping)
    
    # å¤‰æ›´ãŒã‚ã£ãŸå ´åˆã®ã¿æ›¸ãè¾¼ã¿
    if content != original_content:
        ja_file.write_text(content, encoding='utf-8')
        print(f"âœ… ä¿®æ­£å®Œäº†: {ja_file.name}")
        return True
    else:
        print(f"â„¹ï¸  å¤‰æ›´ãªã—: {ja_file.name}")
        return False

def main():
    # ãƒ‘ã‚¹ã®è¨­å®š
    base_dir = Path(__file__).parent.parent
    ja_blog_dir = base_dir / 'content' / 'ja' / 'blog'
    en_blog_dir = base_dir / 'content' / 'en' / 'blog'
    
    if not ja_blog_dir.exists():
        print(f"âŒ æ—¥æœ¬èªãƒ–ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {ja_blog_dir}")
        return
    
    # ç”¨èªãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
    term_mapping = create_term_mapping()
    
    # æ—¥æœ¬èªãƒ–ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
    ja_files = sorted(ja_blog_dir.glob('*.md'))
    total_files = len(ja_files)
    modified_files = 0
    
    print(f"\nğŸš€ æ—¥æœ¬èªãƒ–ãƒ­ã‚°è¨˜äº‹ã®ä¿®æ­£ã‚’é–‹å§‹ã—ã¾ã™")
    print(f"ğŸ“Š å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {total_files}\n")
    
    for ja_file in ja_files:
        # å¯¾å¿œã™ã‚‹è‹±èªãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
        en_file = en_blog_dir / ja_file.name
        
        if process_ja_blog_file(ja_file, en_file, term_mapping):
            modified_files += 1
    
    print(f"\nâœ¨ å®Œäº†!")
    print(f"ğŸ“Š ä¿®æ­£ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«: {modified_files}/{total_files}")

if __name__ == '__main__':
    main()
