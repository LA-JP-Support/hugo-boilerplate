#!/usr/bin/env python3
"""
Improved Batch Article Creation using Claude API
æ”¹å–„ç‰ˆAPIè‡ªå‹•åŒ–è¨˜äº‹ç”Ÿæˆ
"""

import os
import anthropic
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import argparse
from dotenv import load_dotenv

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

# è¨­å®š
API_KEY = os.environ.get("ANTHROPIC_API_KEY") or os.environ.get("CLAUDE_API_KEY")
MODEL = "claude-sonnet-4-20250514"
OUTPUT_DIR = Path("/Users/TM-MBP1/Documents/GitHub/hugo-boilerplate/content/en/glossary")

# æ”¹å–„ç‰ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
ARTICLE_PROMPT = """You are an expert technical writer creating a comprehensive glossary article.

TOPIC: {keyword}

CRITICAL REQUIREMENTS:
âœ“ Language: ENGLISH ONLY
âœ“ Word count: 2,700-2,900 words (STRICT - do not stop early)
âœ“ Format: 30% prose in opening sections, 70% structured content (bullets, tables)
âœ“ Write complete, detailed content - do not summarize or abbreviate

EXACT FRONTMATTER (copy this format):
---
title: "{keyword}"
date: {date}
translationKey: {slug}
description: "SEO-optimized description (150-160 chars)"
keywords:
- keyword1
- keyword2
- keyword3
- keyword4
- keyword5
category: "Application & Use-Cases"
type: glossary
draft: false
---

EXACT STRUCTURE (use these H2 headings, no H3 unless within lists):

## What is {keyword}?
Write 3 full paragraphs (800-1000 words total):
- Paragraph 1: Comprehensive definition and core concept
- Paragraph 2: How it differs from traditional approaches, transformation it enables
- Paragraph 3: Business impact, measurable outcomes, real-world significance

## Core [Specific Technologies/Approaches/Components for {keyword}]
Use topic-specific subheading. List 5-7 main concepts with **Bold Names**
Each with 2-3 sentence explanations. Examples:
**Technology Name** - Detailed explanation of what it is, how it works, why it matters.

## How [Topic] Works
Detailed workflow explanation:
1. Step-by-step process (6-10 steps)
2. Each step: 2-3 sentences
3. Include "Example Workflow:" section at end (150+ words)

## Key Benefits
List 8-10 benefits with **Bold Headers**
Format: **Benefit Name** - 2-3 sentence explanation with specific metrics/outcomes

## Common Use Cases
List 8-10 real-world applications with **Bold Headers**
Format: **Use Case Name** - 2-3 sentence explanation with industry examples

## [Appropriate Comparison Table]
Create comparison table relevant to topic. Include 5-6 items compared across 4-6 dimensions.

## Challenges and Considerations
List 8-10 challenges with **Bold Headers**
Format: **Challenge Name** - 2-3 sentence explanation with mitigation strategies

## Implementation Best Practices
List 10 best practices with **Bold Headers**
Format: **Practice Name** - 2-3 sentence detailed guidance

## [Advanced/Future Section]
Choose appropriate: "Advanced Techniques", "Future Directions", or similar
List 5-6 advanced concepts with **Bold Headers** and explanations

## References
Include 6-8 authoritative references:
- [Title - Source](https://full-url.com)

QUALITY CHECKLIST:
âœ“ 2,700-2,900 words (count carefully)
âœ“ 3 paragraphs for "What is" section (800-1000 words)
âœ“ All sections present with appropriate depth
âœ“ Comparison table included
âœ“ Professional tone, specific examples
âœ“ No redundancy, clear structure
âœ“ All external links in References only

DO NOT:
âœ— Use H3 headings (use **bold** for sub-items instead)
âœ— Stop writing before 2,700 words
âœ— Summarize or abbreviate content
âœ— Include inline external links outside References

IMPORTANT: Write the COMPLETE article to 2,700-2,900 words. Do not stop early."""

def generate_article(keyword: str, slug: str, date: str, output_dir: Path) -> tuple:
    """1è¨˜äº‹ã‚’ç”Ÿæˆ"""
    client = anthropic.Anthropic(api_key=API_KEY)
    
    start_time = time.time()
    
    try:
        message = client.messages.create(
            model=MODEL,
            max_tokens=16000,  # å¢—åŠ 
            temperature=0.1,   # ä½ä¸‹
            messages=[{
                "role": "user",
                "content": ARTICLE_PROMPT.format(
                    keyword=keyword,
                    slug=slug,
                    date=date
                )
            }]
        )
        
        content = message.content[0].text
        elapsed = time.time() - start_time
        
        # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã¨ã‚³ã‚¹ãƒˆè¨ˆç®—
        input_tokens = message.usage.input_tokens
        output_tokens = message.usage.output_tokens
        cost = (input_tokens * 3 / 1_000_000) + (output_tokens * 15 / 1_000_000)
        
        # èªæ•°ã‚«ã‚¦ãƒ³ãƒˆ
        import re
        words = len(re.findall(r'\b\w+\b', content))
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        filepath = output_dir / f"{slug}.md"
        filepath.write_text(content, encoding='utf-8')
        
        print(f"âœ… {keyword}: {words} words, {output_tokens} tokens, ${cost:.4f}, {elapsed:.1f}s")
        
        return (keyword, True, words, output_tokens, cost, elapsed)
        
    except Exception as e:
        print(f"âŒ {keyword}: Error - {e}")
        return (keyword, False, 0, 0, 0.0, 0.0)

def batch_generate(keywords: list, output_dir: Path, max_workers: int = 3):
    """è¤‡æ•°è¨˜äº‹ã‚’ä¸¦åˆ—ç”Ÿæˆ"""
    date = "2025-12-19"
    results = []
    total_cost = 0.0
    total_time = 0.0
    total_tokens = 0
    total_words = 0
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\n{'='*70}")
    print(f"ğŸš€ æ”¹å–„ç‰ˆãƒãƒƒãƒç”Ÿæˆé–‹å§‹")
    print(f"{'='*70}")
    print(f"è¨˜äº‹æ•°: {len(keywords)}")
    print(f"ä¸¦åˆ—æ•°: {max_workers}")
    print(f"å‡ºåŠ›å…ˆ: {output_dir}")
    print(f"ç›®æ¨™èªæ•°: 2,700-2,900èª/è¨˜äº‹")
    print(f"{'='*70}\n")
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {
            executor.submit(generate_article, kw, slug, date, output_dir): (kw, slug)
            for kw, slug in keywords
        }
        
        for future in as_completed(futures):
            keyword, slug = futures[future]
            kw, success, words, tokens, cost, elapsed = future.result()
            
            if success:
                total_cost += cost
                total_time += elapsed
                total_tokens += tokens
                total_words += words
                
                # èªæ•°ãƒã‚§ãƒƒã‚¯
                if 2700 <= words <= 2900:
                    word_status = "âœ…"
                elif 2500 <= words < 2700:
                    word_status = "âš ï¸"
                else:
                    word_status = "âŒ"
                
                results.append({
                    "keyword": kw,
                    "slug": slug,
                    "words": words,
                    "word_status": word_status,
                    "tokens": tokens,
                    "cost": f"${cost:.4f}",
                    "time": f"{elapsed:.1f}s",
                    "status": "âœ…"
                })
            else:
                results.append({
                    "keyword": kw,
                    "slug": slug,
                    "words": 0,
                    "word_status": "âŒ",
                    "tokens": 0,
                    "cost": "$0.0000",
                    "time": "0.0s",
                    "status": "âŒ"
                })
    
    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    successful = sum(1 for r in results if r['status'] == 'âœ…')
    word_compliant = sum(1 for r in results if r['word_status'] == 'âœ…')
    
    print(f"\n{'='*70}")
    print(f"ğŸ“Š å®Œäº†ã‚µãƒãƒªãƒ¼")
    print(f"{'='*70}")
    print(f"æˆåŠŸ:         {successful}/{len(keywords)}")
    print(f"èªæ•°é”æˆ:     {word_compliant}/{len(keywords)} (2,700-2,900èª)")
    print(f"å¹³å‡èªæ•°:     {total_words/len(keywords):.0f}èª/è¨˜äº‹")
    print(f"åˆè¨ˆãƒˆãƒ¼ã‚¯ãƒ³: {total_tokens:,}")
    print(f"åˆè¨ˆã‚³ã‚¹ãƒˆ:   ${total_cost:.4f}")
    print(f"å¹³å‡æ™‚é–“:     {total_time/len(keywords):.1f}ç§’/è¨˜äº‹")
    print(f"ç·æ™‚é–“:       {total_time/60:.1f}åˆ†")
    print(f"{'='*70}\n")
    
    return results

def main():
    parser = argparse.ArgumentParser(description="æ”¹å–„ç‰ˆClaude APIè¨˜äº‹è‡ªå‹•ç”Ÿæˆ")
    parser.add_argument('--test', action='store_true', help='ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼ˆåˆ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«å‡ºåŠ›ï¼‰')
    parser.add_argument('--keywords', nargs='+', help='ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ')
    parser.add_argument('--workers', type=int, default=3, help='ä¸¦åˆ—æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3ï¼‰')
    parser.add_argument('--output-dir', help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    
    args = parser.parse_args()
    
    # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã®å ´åˆ
    if args.test:
        if args.output_dir:
            output_dir = Path(args.output_dir)
        else:
            output_dir = Path("/Users/TM-MBP1/Documents/GitHub/hugo-boilerplate/content/en/glossary-api-test-v2")
        print(f"ğŸ§ª ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼ˆæ”¹å–„ç‰ˆï¼‰: {output_dir}")
    else:
        output_dir = OUTPUT_DIR
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¨­å®š
    if args.keywords:
        keywords = [(kw, kw.replace(' ', '-')) for kw in args.keywords]
    else:
        keywords = [
            ("Customer Segmentation", "Customer-Segmentation"),
            ("Sentiment Analysis", "Sentiment-Analysis"),
            ("Recommendation Systems", "Recommendation-Systems"),
        ]
    
    # å®Ÿè¡Œ
    results = batch_generate(keywords, output_dir, max_workers=args.workers)
    
    # çµæœè©³ç´°
    print("\nğŸ“‹ è©³ç´°çµæœ:")
    print(f"{'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹':<6} {'ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰':<35} {'èªæ•°':<12} {'ãƒˆãƒ¼ã‚¯ãƒ³':<10} {'ã‚³ã‚¹ãƒˆ':<10} {'æ™‚é–“':<8}")
    print("-" * 90)
    for r in results:
        print(f"{r['status']:<6} {r['keyword']:<35} {r['words']:>4}èª {r['word_status']:<6} {r['tokens']:>6}  {r['cost']:<10} {r['time']:<8}")

if __name__ == "__main__":
    main()
