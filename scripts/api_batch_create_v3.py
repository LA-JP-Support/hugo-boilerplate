#!/usr/bin/env python3
"""
Grammar-Fixed Batch Article Creation (v3)
æ–‡æ³•ä¿®æ­£ç‰ˆAPIè‡ªå‹•åŒ–è¨˜äº‹ç”Ÿæˆ
"""

import os
import anthropic
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import argparse
import re
from dotenv import load_dotenv

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

# è¨­å®š
API_KEY = os.environ.get("ANTHROPIC_API_KEY") or os.environ.get("CLAUDE_API_KEY")
MODEL = "claude-sonnet-4-20250514"
OUTPUT_DIR = Path("/Users/TM-MBP1/Documents/GitHub/hugo-boilerplate/content/en/glossary")

def needs_article(keyword: str) -> bool:
    """å† è©ãŒå¿…è¦ã‹ã‚’åˆ¤å®š"""
    # è¤‡æ•°å½¢ã§çµ‚ã‚ã‚‹å ´åˆã¯ä¸è¦
    if keyword.endswith('s') or keyword.endswith('es'):
        return False
    # ä¸å¯ç®—åè©ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰
    uncountable = ['forecasting', 'analysis', 'learning', 'intelligence', 'processing', 
                   'management', 'optimization', 'automation', 'segmentation']
    if any(uc in keyword.lower() for uc in uncountable):
        return False
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§å† è©å¿…è¦
    return True

def get_article(keyword: str) -> str:
    """é©åˆ‡ãªå† è©ã‚’è¿”ã™"""
    if not needs_article(keyword):
        return ""
    # æ¯éŸ³ã§å§‹ã¾ã‚‹å ´åˆã¯an
    if keyword[0].lower() in 'aeiou':
        return "an "
    return "a "

# æ–‡æ³•ä¿®æ­£ç‰ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
ARTICLE_PROMPT = """You are an expert technical writer creating a comprehensive glossary article.

TOPIC: {keyword}

CRITICAL REQUIREMENTS:
âœ“ Language: ENGLISH ONLY
âœ“ Word count: 2,700-2,900 words (STRICT - do not stop early)
âœ“ Format: 30% prose in opening sections, 70% structured content (bullets, tables)
âœ“ Write complete, detailed content - do not summarize or abbreviate
âœ“ Use proper English grammar including articles (a/an/the) where appropriate

EXACT FRONTMATTER (copy this format):
---
title: "{keyword}"
date: {date}
translationKey: {slug}
description: "SEO-optimized description (150-160 chars)"
keywords:
- keyword1
- keyword2
- keyword3
- keyword4
- keyword5
category: "Application & Use-Cases"
type: glossary
draft: false
---

EXACT STRUCTURE (use these H2 headings, no H3 unless within lists):

## What is {article}{keyword}?
Write 3 full paragraphs (800-1000 words total):
- Paragraph 1: Comprehensive definition and core concept
- Paragraph 2: How it differs from traditional approaches, transformation it enables
- Paragraph 3: Business impact, measurable outcomes, real-world significance

IMPORTANT: Use proper grammar - "{article}{keyword}" is correct (include "a" or "an" if it's a singular countable noun)

## Core [Specific Technologies/Approaches/Components]
Use topic-specific subheading appropriate for {keyword}. List 5-7 main concepts with **Bold Names**
Each with 2-3 sentence explanations. Examples:
**Technology Name** - Detailed explanation of what it is, how it works, why it matters.

## How {keyword} Works
Detailed workflow explanation:
1. Step-by-step process (6-10 steps)
2. Each step: 2-3 sentences
3. Include "Example Workflow:" section at end (150+ words)

## Key Benefits
List 8-10 benefits with **Bold Headers**
Format: **Benefit Name** - 2-3 sentence explanation with specific metrics/outcomes

## Common Use Cases
List 8-10 real-world applications with **Bold Headers**
Format: **Use Case Name** - 2-3 sentence explanation with industry examples

## [Appropriate Comparison Table]
Create comparison table relevant to topic. Include 5-6 items compared across 4-6 dimensions.
Title should be specific to the topic (e.g., "Recommendation Algorithms Comparison", "Forecasting Approach Comparison")

## Challenges and Considerations
List 8-10 challenges with **Bold Headers**
Format: **Challenge Name** - 2-3 sentence explanation with mitigation strategies

## Implementation Best Practices
List 10 best practices with **Bold Headers**
Format: **Practice Name** - 2-3 sentence detailed guidance

## Advanced Techniques
List 5-6 advanced concepts with **Bold Headers** and explanations (each 2-3 sentences)

## Future Directions
List 5-6 future trends with **Bold Headers** and explanations (each 2-3 sentences)

## References
Include 6-8 authoritative references in standard bibliographic format.
- For articles/reports/books: Author/Organization. (Year). Title. Source Name. (NO LINKS/URLs)
- For services/tools: Name of Service. Description. URL: https://... (Explicit URL)

QUALITY CHECKLIST:
âœ“ 2,700-2,900 words (count carefully)
âœ“ 3 paragraphs for "What is" section (800-1000 words)
âœ“ All sections present with appropriate depth
âœ“ Comparison table included
âœ“ Professional tone, specific examples
âœ“ No redundancy, clear structure
âœ“ References formatted correctly (Text-only for articles, URL text for services)
âœ“ Proper English grammar throughout

DO NOT:
âœ— Use H3 headings (use **bold** for sub-items instead)
âœ— Stop writing before 2,700 words
âœ— Summarize or abbreviate content
âœ— Include inline external links outside References
âœ— Include Markdown links in References (use text citations or explicit URL text)
âœ— Forget articles (a/an/the) where grammatically required

IMPORTANT: Write the COMPLETE article to 2,700-2,900 words. Do not stop early."""

def generate_article(keyword: str, slug: str, date: str, output_dir: Path) -> tuple:
    """1è¨˜äº‹ã‚’ç”Ÿæˆ"""
    client = anthropic.Anthropic(api_key=API_KEY)
    
    start_time = time.time()
    
    # é©åˆ‡ãªå† è©ã‚’æ±ºå®š
    article = get_article(keyword)
    
    try:
        message = client.messages.create(
            model=MODEL,
            max_tokens=16000,
            temperature=0.1,
            messages=[{
                "role": "user",
                "content": ARTICLE_PROMPT.format(
                    keyword=keyword,
                    slug=slug,
                    date=date,
                    article=article
                )
            }]
        )
        
        content = message.content[0].text
        elapsed = time.time() - start_time
        
        # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã¨ã‚³ã‚¹ãƒˆè¨ˆç®—
        input_tokens = message.usage.input_tokens
        output_tokens = message.usage.output_tokens
        cost = (input_tokens * 3 / 1_000_000) + (output_tokens * 15 / 1_000_000)
        
        # èªæ•°ã‚«ã‚¦ãƒ³ãƒˆ
        words = len(re.findall(r'\b\w+\b', content))
        
        # æ–‡æ³•ãƒã‚§ãƒƒã‚¯: "What is [keyword]?" ã®ç¢ºèª
        grammar_ok = True
        if needs_article(keyword):
            expected = f"What is {article}{keyword}?"
            if expected not in content:
                grammar_ok = False
                print(f"  âš ï¸  æ–‡æ³•è­¦å‘Š: '{expected}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        filepath = output_dir / f"{slug}.md"
        filepath.write_text(content, encoding='utf-8')
        
        status = "âœ…" if grammar_ok else "âš ï¸"
        print(f"{status} {keyword}: {words} words, {output_tokens} tokens, ${cost:.4f}, {elapsed:.1f}s")
        
        return (keyword, True, words, output_tokens, cost, elapsed, grammar_ok)
        
    except Exception as e:
        print(f"âŒ {keyword}: Error - {e}")
        return (keyword, False, 0, 0, 0.0, 0.0, False)

def batch_generate(keywords: list, output_dir: Path, max_workers: int = 3):
    """è¤‡æ•°è¨˜äº‹ã‚’ä¸¦åˆ—ç”Ÿæˆ"""
    date = "2025-12-19"
    results = []
    total_cost = 0.0
    total_time = 0.0
    total_tokens = 0
    total_words = 0
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\n{'='*70}")
    print(f"ğŸš€ æ–‡æ³•ä¿®æ­£ç‰ˆãƒãƒƒãƒç”Ÿæˆé–‹å§‹ (v3)")
    print(f"{'='*70}")
    print(f"è¨˜äº‹æ•°: {len(keywords)}")
    print(f"ä¸¦åˆ—æ•°: {max_workers}")
    print(f"å‡ºåŠ›å…ˆ: {output_dir}")
    print(f"ç›®æ¨™èªæ•°: 2,700-2,900èª/è¨˜äº‹")
    print(f"{'='*70}\n")
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {
            executor.submit(generate_article, kw, slug, date, output_dir): (kw, slug)
            for kw, slug in keywords
        }
        
        for future in as_completed(futures):
            keyword, slug = futures[future]
            kw, success, words, tokens, cost, elapsed, grammar_ok = future.result()
            
            if success:
                total_cost += cost
                total_time += elapsed
                total_tokens += tokens
                total_words += words
                
                # èªæ•°ãƒã‚§ãƒƒã‚¯
                if 2700 <= words <= 2900:
                    word_status = "âœ…"
                elif 2500 <= words < 2700:
                    word_status = "âš ï¸"
                else:
                    word_status = "âŒ"
                
                grammar_status = "âœ…" if grammar_ok else "âš ï¸"
                
                results.append({
                    "keyword": kw,
                    "slug": slug,
                    "words": words,
                    "word_status": word_status,
                    "grammar_status": grammar_status,
                    "tokens": tokens,
                    "cost": f"${cost:.4f}",
                    "time": f"{elapsed:.1f}s",
                    "status": "âœ…"
                })
            else:
                results.append({
                    "keyword": kw,
                    "slug": slug,
                    "words": 0,
                    "word_status": "âŒ",
                    "grammar_status": "âŒ",
                    "tokens": 0,
                    "cost": "$0.0000",
                    "time": "0.0s",
                    "status": "âŒ"
                })
    
    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    successful = sum(1 for r in results if r['status'] == 'âœ…')
    word_compliant = sum(1 for r in results if r['word_status'] == 'âœ…')
    grammar_ok = sum(1 for r in results if r['grammar_status'] == 'âœ…')
    
    print(f"\n{'='*70}")
    print(f"ğŸ“Š å®Œäº†ã‚µãƒãƒªãƒ¼")
    print(f"{'='*70}")
    print(f"æˆåŠŸ:         {successful}/{len(keywords)}")
    print(f"èªæ•°é”æˆ:     {word_compliant}/{len(keywords)} (2,700-2,900èª)")
    print(f"æ–‡æ³•OK:       {grammar_ok}/{len(keywords)}")
    print(f"å¹³å‡èªæ•°:     {total_words/len(keywords):.0f}èª/è¨˜äº‹")
    print(f"åˆè¨ˆãƒˆãƒ¼ã‚¯ãƒ³: {total_tokens:,}")
    print(f"åˆè¨ˆã‚³ã‚¹ãƒˆ:   ${total_cost:.4f}")
    print(f"å¹³å‡æ™‚é–“:     {total_time/len(keywords):.1f}ç§’/è¨˜äº‹")
    print(f"ç·æ™‚é–“:       {total_time/60:.1f}åˆ†")
    print(f"{'='*70}\n")
    
    return results

def main():
    parser = argparse.ArgumentParser(description="æ–‡æ³•ä¿®æ­£ç‰ˆClaude APIè¨˜äº‹è‡ªå‹•ç”Ÿæˆ (v3)")
    parser.add_argument('--test', action='store_true', help='ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼ˆåˆ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«å‡ºåŠ›ï¼‰')
    parser.add_argument('--keywords', nargs='+', help='ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ')
    parser.add_argument('--workers', type=int, default=3, help='ä¸¦åˆ—æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3ï¼‰')
    parser.add_argument('--output-dir', help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    
    args = parser.parse_args()
    
    # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã®å ´åˆ
    if args.test:
        if args.output_dir:
            output_dir = Path(args.output_dir)
        else:
            output_dir = Path("/Users/TM-MBP1/Documents/GitHub/hugo-boilerplate/content/en/glossary-api-test-v3")
        print(f"ğŸ§ª ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼ˆæ–‡æ³•ä¿®æ­£ç‰ˆv3ï¼‰: {output_dir}")
    else:
        output_dir = OUTPUT_DIR
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¨­å®š
    if args.keywords:
        keywords = [(kw, kw.replace(' ', '-')) for kw in args.keywords]
    else:
        keywords = [
            ("Customer Segmentation", "Customer-Segmentation"),
            ("Sentiment Analysis", "Sentiment-Analysis"),
            ("Recommendation Systems", "Recommendation-Systems"),
        ]
    
    # å®Ÿè¡Œ
    results = batch_generate(keywords, output_dir, max_workers=args.workers)
    
    # çµæœè©³ç´°
    print("\nğŸ“‹ è©³ç´°çµæœ:")
    print(f"{'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹':<6} {'ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰':<35} {'èªæ•°':<12} {'æ–‡æ³•':<6} {'ãƒˆãƒ¼ã‚¯ãƒ³':<10} {'ã‚³ã‚¹ãƒˆ':<10} {'æ™‚é–“':<8}")
    print("-" * 95)
    for r in results:
        print(f"{r['status']:<6} {r['keyword']:<35} {r['words']:>4}èª {r['word_status']:<6} {r['grammar_status']:<6} {r['tokens']:>6}  {r['cost']:<10} {r['time']:<8}")

if __name__ == "__main__":
    main()
